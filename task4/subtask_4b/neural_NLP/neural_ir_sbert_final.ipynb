{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1e02b9",
   "metadata": {},
   "source": [
    "# CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval) - NLP Approach\n",
    "\n",
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd553803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl' \n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "PATH_QUERY_TRAIN_DATA = '../subtask4b_query_tweets_train.tsv' \n",
    "PATH_QUERY_DEV_DATA = '../subtask4b_query_tweets_dev.tsv' \n",
    "PATH_QUERY_TEST_DATA = '../subtask4b_query_tweets_test.tsv' \n",
    "\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_TEST_DATA, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b8b41",
   "metadata": {},
   "source": [
    "## Code for uploading  Model to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b87724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model on hugging face (in a new repo)\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "def create_repo_on_huggingface(repo_id_str):\n",
    "    try:\n",
    "        repo_url = create_repo(repo_id=repo_id_str, exist_ok=True, private=True)\n",
    "        print(f\"Created or found repository on Hugging Face Hub: {repo_url}\")\n",
    "        # create_repo returns the URL of the repository, not the repo_id string.\n",
    "        # Let's keep the repo_id string for upload_folder\n",
    "        repo_id = repo_id_str\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        print(\"It seems your huggingface_hub library version is incompatible.\")\n",
    "        print(\"Please update it: pip install -U huggingface_hub\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating the repository: {e}\")\n",
    "    return repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0171b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already done\n",
    "# repo_id = create_repo_on_huggingface('LukasXperiaZ/all-mpnet-base-v2-neural-ir-2-epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ff791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads the model to hugging face\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "def upload_model_to_huggingface(local_folder_path, repo_id):\n",
    "    # Path to your local directory containing the trained model files\n",
    "\n",
    "    print(f\"Uploading files from {local_folder_path} to {repo_id}...\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=local_folder_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type='model', # Specify the type of repository\n",
    "        commit_message='Upload final model from checkpoint',\n",
    "    )\n",
    "\n",
    "    print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also done previously\n",
    "# local_folder_path = '/output/all-mpnet-base-v2-neural-ir-2-epochs'\n",
    "# upload_model_to_huggingface(local_folder_path, repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a214",
   "metadata": {},
   "source": [
    "# 2. Running with SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01edbb",
   "metadata": {},
   "source": [
    "## 2.1. Running fine tuned model No. 1 \n",
    "\n",
    "This one was fine-tuned on the training data with 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model_name = 'LukasXperiaZ/all-mpnet-base-v2-neural-ir-2-epochs'\n",
    "\n",
    "model_1 = SentenceTransformer(model_name)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing abstracts with empty string\n",
    "df_collection['abstract'] = df_collection['abstract'].fillna('')\n",
    "\n",
    "# Combine title, abstract, authors and journal into a single string\n",
    "papers = (\n",
    "    df_collection['title'].fillna('') + '. ' +\n",
    "    df_collection['abstract'].fillna('') + ' ' +\n",
    "    df_collection['authors'].fillna('') + ' ' +\n",
    "    df_collection['journal'].fillna('')\n",
    ").tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd572d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all papers once\n",
    "paper_embeddings = model_1.encode(papers, convert_to_tensor=True)\n",
    "uid_to_index = {uid: idx for idx, uid in enumerate(cord_uids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd919614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "def get_sbert_topk(df_query, model, k=10):\n",
    "    queries = df_query['tweet_text'].tolist()\n",
    "    query_embeddings = model.encode(queries, convert_to_tensor=True)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "\n",
    "    topk_results = []\n",
    "    for query_emb in query_embeddings:\n",
    "\n",
    "        scores = util.cos_sim(query_emb, paper_embeddings)[0]\n",
    "        top_k = torch.topk(scores, k=k)\n",
    "        topk_uids = [cord_uids[idx] for idx in top_k.indices]\n",
    "        topk_results.append(topk_uids)\n",
    "    return topk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d86e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top-k predictions for train/dev/test\n",
    "#df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_1, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_1, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_1, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f35b9",
   "metadata": {},
   "source": [
    "### 2.1.1 Evaluation\n",
    "\n",
    "Evaluate the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04672367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(\n",
    "            lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                       if x[col_gold] in [i for i in x[col_pred][:k]] else 0),\n",
    "            axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc75c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fad293",
   "metadata": {},
   "source": [
    "### 2.1.2 Exporting results to prepare the submission on Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d76a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['sbert_topk'].apply(lambda x: x[:5])\n",
    "\n",
    "df_query_test[['post_id', 'preds']].to_csv('/output/neural_ir/02_epochs/test_results.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06991711",
   "metadata": {},
   "source": [
    "## 2.2 Model No. 2\n",
    "\n",
    "Fine-Tune another model with more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e238a8",
   "metadata": {},
   "source": [
    "### 2.2.1 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_paper_text(row):\n",
    "    parts = [row.get('title', ''), row.get('abstract', ''), row.get('journal', ''), row.get('authors', '')]\n",
    "    return ' '.join([p.strip() for p in parts if isinstance(p, str) and p.strip()])\n",
    "\n",
    "model_2 = SentenceTransformer('all-mpnet-base-v2') # or use 'pritamdeka/S-PubMedBert-MS-MARCO'\n",
    "\n",
    "cord_uid_to_text = {\n",
    "    row['cord_uid']: build_paper_text(row)\n",
    "    for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[row['tweet_text'], cord_uid_to_text[row['cord_uid']]])\n",
    "    for _, row in df_query_train.iterrows()\n",
    "    if row['cord_uid'] in cord_uid_to_text\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)  # 32 works, 40 might work\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model_2)\n",
    "\n",
    "model_2.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    "    output_path='./output/all-mpnet-base-v2-neural-ir-10-epochs'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save(\"./output/all-mpnet-base-v2-neural-ir-10-epochs\")\n",
    "\n",
    "# save model on huggingface\n",
    "repo_id_2 = create_repo_on_huggingface('LukasXperiaZ/all-mpnet-base-v2-neural-ir-10-epochs')\n",
    "\n",
    "local_folder_path = './output/all-mpnet-base-v2-neural-ir-10-epochs'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce527d",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47511fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "# embed all papers on model\n",
    "paper_embeddings = model_2.encode(papers, convert_to_tensor=True)\n",
    "# ensure the shape for the cosine similarity\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "#df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_2, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_2, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebced5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99070103",
   "metadata": {},
   "source": [
    "## 2.3 Model No. 3\n",
    "\n",
    "Fine-Tune a model pretrained with a medical environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b2812",
   "metadata": {},
   "source": [
    "### 2.3.1 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb32056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_paper_text(row):\n",
    "    parts = [row.get('title', ''), row.get('abstract', ''), row.get('journal', ''), row.get('authors', '')]\n",
    "    return ' '.join([p.strip() for p in parts if isinstance(p, str) and p.strip()])\n",
    "\n",
    "model_3 = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO') #\n",
    "\n",
    "cord_uid_to_text = {\n",
    "    row['cord_uid']: build_paper_text(row)\n",
    "    for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[row['tweet_text'], cord_uid_to_text[row['cord_uid']]])\n",
    "    for _, row in df_query_train.iterrows()\n",
    "    if row['cord_uid'] in cord_uid_to_text\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)  # 32 works, 40 might work\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model_3)\n",
    "\n",
    "model_3.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    "    output_path='./output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save(\"./output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deba9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model on huggingface\n",
    "repo_id_3 = create_repo_on_huggingface('LukasXperiaZ/S-PubMedBert-MS-MARCO-neural-ir-10-epochs')\n",
    "\n",
    "local_folder_path = './output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e7c3b",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "# embed all papers on model\n",
    "paper_embeddings = model_3.encode(papers, convert_to_tensor=True)\n",
    "# ensure the shape for the cosine similarity\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_3, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_3, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_3, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
