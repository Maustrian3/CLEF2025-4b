{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1e02b9",
   "metadata": {},
   "source": [
    "# CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval) - NLP Approach\n",
    "\n",
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd553803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl' \n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "PATH_QUERY_TRAIN_DATA = '../subtask4b_query_tweets_train.tsv' \n",
    "PATH_QUERY_DEV_DATA = '../subtask4b_query_tweets_dev.tsv' \n",
    "PATH_QUERY_TEST_DATA = '../subtask4b_query_tweets_test.tsv' \n",
    "\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "df_query_test = pd.read_csv(PATH_QUERY_TEST_DATA, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b8b41",
   "metadata": {},
   "source": [
    "## Code for uploading  Model to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b87724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model on hugging face (in a new repo)\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "def create_repo_on_huggingface(repo_id_str):\n",
    "    try:\n",
    "        repo_url = create_repo(repo_id=repo_id_str, exist_ok=True, private=True)\n",
    "        print(f\"Created or found repository on Hugging Face Hub: {repo_url}\")\n",
    "        # create_repo returns the URL of the repository, not the repo_id string.\n",
    "        # Let's keep the repo_id string for upload_folder\n",
    "        repo_id = repo_id_str\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        print(\"It seems your huggingface_hub library version is incompatible.\")\n",
    "        print(\"Please update it: pip install -U huggingface_hub\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating the repository: {e}\")\n",
    "    return repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0171b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already done\n",
    "# repo_id = create_repo_on_huggingface('LukasXperiaZ/all-mpnet-base-v2-neural-ir-2-epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5ff791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads the model to hugging face\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "def upload_model_to_huggingface(local_folder_path, repo_id):\n",
    "    # Path to your local directory containing the trained model files\n",
    "\n",
    "    print(f\"Uploading files from {local_folder_path} to {repo_id}...\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=local_folder_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type='model', # Specify the type of repository\n",
    "        commit_message='Upload final model from checkpoint',\n",
    "    )\n",
    "\n",
    "    print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also done previously\n",
    "# local_folder_path = '/output/all-mpnet-base-v2-neural-ir-2-epochs'\n",
    "# upload_model_to_huggingface(local_folder_path, repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a214",
   "metadata": {},
   "source": [
    "# 2. Running with SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01edbb",
   "metadata": {},
   "source": [
    "## 2.1. Running fine tuned model No. 1 \n",
    "\n",
    "This one was fine-tuned on the training data with 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594b3de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub: LukasXperiaZ/all-mpnet-base-v2-neural-ir-2-epochs\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "model_name = 'LukasXperiaZ/all-mpnet-base-v2-neural-ir-2-epochs'\n",
    "\n",
    "model_1 = SentenceTransformer(model_name)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2cc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing abstracts with empty string\n",
    "df_collection['abstract'] = df_collection['abstract'].fillna('')\n",
    "\n",
    "# Combine title, abstract, authors and journal into a single string\n",
    "papers = (\n",
    "    df_collection['title'].fillna('') + '. ' +\n",
    "    df_collection['abstract'].fillna('') + ' ' +\n",
    "    df_collection['authors'].fillna('') + ' ' +\n",
    "    df_collection['journal'].fillna('')\n",
    ").tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd572d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all papers once\n",
    "paper_embeddings = model_1.encode(papers, convert_to_tensor=True)\n",
    "uid_to_index = {uid: idx for idx, uid in enumerate(cord_uids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd919614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "def get_sbert_topk(df_query, model, k=10):\n",
    "    queries = df_query['tweet_text'].tolist()\n",
    "    query_embeddings = model.encode(queries, convert_to_tensor=True)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "\n",
    "    topk_results = []\n",
    "    for query_emb in query_embeddings:\n",
    "\n",
    "        scores = util.cos_sim(query_emb, paper_embeddings)[0]\n",
    "        top_k = torch.topk(scores, k=k)\n",
    "        topk_uids = [cord_uids[idx] for idx in top_k.indices]\n",
    "        topk_results.append(topk_uids)\n",
    "    return topk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d86e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top-k predictions for train/dev/test\n",
    "df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_1, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_1, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_1, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f35b9",
   "metadata": {},
   "source": [
    "### 2.1.1 Evaluation\n",
    "\n",
    "Evaluate the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04672367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(\n",
    "            lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                       if x[col_gold] in [i for i in x[col_pred][:k]] else 0),\n",
    "            axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc75c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Results on the train set: {1: np.float64(0.6206333151793355), 5: np.float64(0.7006470603490754), 10: np.float64(0.7081681566529462)}\n",
      "SBERT Results on the dev set: {1: np.float64(0.6028571428571429), 5: np.float64(0.6658095238095239), 10: np.float64(0.672548185941043)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fad293",
   "metadata": {},
   "source": [
    "### 2.1.2 Exporting results to prepare the submission on Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d76a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['sbert_topk'].apply(lambda x: x[:5])\n",
    "\n",
    "df_query_test[['post_id', 'preds']].to_csv('.test_results_02_epochs.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06991711",
   "metadata": {},
   "source": [
    "## 2.2 Model No. 2\n",
    "\n",
    "Fine-Tune another model with more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e238a8",
   "metadata": {},
   "source": [
    "### 2.2.1 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_paper_text(row):\n",
    "    parts = [row.get('title', ''), row.get('abstract', ''), row.get('journal', ''), row.get('authors', '')]\n",
    "    return ' '.join([p.strip() for p in parts if isinstance(p, str) and p.strip()])\n",
    "\n",
    "model_2 = SentenceTransformer('all-mpnet-base-v2') # or use 'pritamdeka/S-PubMedBert-MS-MARCO'\n",
    "\n",
    "cord_uid_to_text = {\n",
    "    row['cord_uid']: build_paper_text(row)\n",
    "    for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[row['tweet_text'], cord_uid_to_text[row['cord_uid']]])\n",
    "    for _, row in df_query_train.iterrows()\n",
    "    if row['cord_uid'] in cord_uid_to_text\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)  # 32 works, 40 might work\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model_2)\n",
    "\n",
    "model_2.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    "    output_path='./output/all-mpnet-base-v2-neural-ir-10-epochs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save(\"./output/all-mpnet-base-v2-neural-ir-10-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7556ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or found repository on Hugging Face Hub: https://huggingface.co/LukasXperiaZ/all-mpnet-base-v2-neural-ir-10-epochs\n"
     ]
    }
   ],
   "source": [
    "# save model on huggingface\n",
    "repo_id_2 = create_repo_on_huggingface('LukasXperiaZ/all-mpnet-base-v2-neural-ir-10-epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder_path = './output/all-mpnet-base-v2-neural-ir-10-epochs'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce527d",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875dfe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32497569687a4a8d9a3a59b78767fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2745cd29ad19456eae22b8053bec9c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb05d56b2d104b598283a7d369511564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/53.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83080cf1db674d378df050016ca93b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6355aeb4c1415581a5fd57f22f35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f72e44cfe84c7ab1bca10abc799d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943e2cf92ce249d9ba3a67a978d096f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af664248d234847b6dd4312d8a1deef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6446e142f946ea994ee09534c86b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba7846bed7a4958acb9394aed8cb83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd8554983c9486b99f9c308c253367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub: LukasXperiaZ/all-mpnet-base-v2-neural-ir-10-epochs\n"
     ]
    }
   ],
   "source": [
    "model_2 = SentenceTransformer(repo_id_2)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {repo_id_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47511fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "# embed all papers on model\n",
    "paper_embeddings = model_2.encode(papers, convert_to_tensor=True)\n",
    "# ensure the shape for the cosine similarity\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_2, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_2, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_2, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebced5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Results on the train set: {1: np.float64(0.7943670738349023), 5: np.float64(0.869449415181929), 10: np.float64(0.8721370713279217)}\n",
      "SBERT Results on the dev set: {1: np.float64(0.6092857142857143), 5: np.float64(0.675702380952381), 10: np.float64(0.6835195578231292)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f743fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['sbert_topk'].apply(lambda x: x[:5])\n",
    "\n",
    "df_query_test[['post_id', 'preds']].to_csv('./test_results_10_epochs.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99070103",
   "metadata": {},
   "source": [
    "## 2.3 Model No. 3\n",
    "\n",
    "Fine-Tune a model from the medical environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b2812",
   "metadata": {},
   "source": [
    "### 2.3.1 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb32056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830df4de29dd41dc9d4d9a14850b5ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2010' max='2010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2010/2010 17:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.064100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_paper_text(row):\n",
    "    parts = [row.get('title', ''), row.get('abstract', ''), row.get('journal', ''), row.get('authors', '')]\n",
    "    return ' '.join([p.strip() for p in parts if isinstance(p, str) and p.strip()])\n",
    "\n",
    "model_3 = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO') #\n",
    "\n",
    "cord_uid_to_text = {\n",
    "    row['cord_uid']: build_paper_text(row)\n",
    "    for _, row in df_collection.iterrows()\n",
    "}\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[row['tweet_text'], cord_uid_to_text[row['cord_uid']]])\n",
    "    for _, row in df_query_train.iterrows()\n",
    "    if row['cord_uid'] in cord_uid_to_text\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model_3)\n",
    "\n",
    "model_3.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    "    output_path='./output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2addc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save(\"./output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3deba9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or found repository on Hugging Face Hub: https://huggingface.co/LukasXperiaZ/S-PubMedBert-MS-MARCO-neural-ir-10-epochs\n",
      "Uploading files from ./output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs to LukasXperiaZ/S-PubMedBert-MS-MARCO-neural-ir-10-epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99777bb5e88497184239c1fc66e023f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# save model on huggingface\n",
    "repo_id_3 = create_repo_on_huggingface('LukasXperiaZ/S-PubMedBert-MS-MARCO-neural-ir-10-epochs')\n",
    "\n",
    "local_folder_path = './output/S-PubMedBert-MS-MARCO-neural-ir-10-epochs'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e7c3b",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ffac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d8d0a5fef343d190fd811526a97fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da466b450484f0c8d7bf1787dc9f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb31417aad4c1393a4eafed6260cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/59.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b970923f7404eb0f9a0348644272b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136340f457af4102bbfa376d4a3a9e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d94b33a537843629751ac708fe20ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e3eb7c4c1547c4bf1f7c11f7a4d970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f14b379e404f45b592545339c6a721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e823c7b6f604945b28555923f414f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/706k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e382578befb4ccf98d248a6a95e36b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd799ab04704e2dab52e04a2a4eeeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub: LukasXperiaZ/S-PubMedBert-MS-MARCO-neural-ir-10-epochs\n"
     ]
    }
   ],
   "source": [
    "model_3 = SentenceTransformer(repo_id_3)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {repo_id_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebbeb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "# embed all papers on model\n",
    "paper_embeddings = model_3.encode(papers, convert_to_tensor=True)\n",
    "# ensure the shape for the cosine similarity\n",
    "paper_embeddings = F.normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "df_query_train['sbert_topk'] = get_sbert_topk(df_query_train, model_3, k=10)\n",
    "df_query_dev['sbert_topk'] = get_sbert_topk(df_query_dev, model_3, k=10)\n",
    "df_query_test['sbert_topk'] = get_sbert_topk(df_query_test, model_3, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c31e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Results on the train set: {1: np.float64(0.7567104956041392), 5: np.float64(0.8431948442646334), 10: np.float64(0.846685876560225)}\n",
      "SBERT Results on the dev set: {1: np.float64(0.5771428571428572), 5: np.float64(0.6482976190476191), 10: np.float64(0.6549143990929704)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MRR\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'sbert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'sbert_topk')\n",
    "#results_test = get_performance_mrr(df_query_test, 'cord_uid', 'sbert_topk')\n",
    "\n",
    "print(f\"SBERT Results on the train set: {results_train}\")\n",
    "print(f\"SBERT Results on the dev set: {results_dev}\")\n",
    "#print(f\"SBERT Results on the test set: {results_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a84637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_test['preds'] = df_query_test['sbert_topk'].apply(lambda x: x[:5])\n",
    "\n",
    "df_query_test[['post_id', 'preds']].to_csv('./test_results_PubMedBert_10_epochs.tsv', index=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
