{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1742975967136,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "rQPqDKP_QHFM"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8N7h9BhQI5m"
   },
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742975971100,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "2GQI4HcKR6hS"
   },
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl' #MODIFY PATH"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742975975524,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "SYBB3UYbMwTA"
   },
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742975976305,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "4v3lygNOQQSn",
    "outputId": "ee5b9abd-f889-4a4e-ce11-32d2691433cb"
   },
   "source": [
    "df_collection.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1742975978238,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "9veNFFGDZRx7",
    "outputId": "5eec7f85-7d20-44d7-8986-a85cb00533d8"
   },
   "source": [
    "df_collection.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAUiDU0xXLBt"
   },
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742975982410,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "v8gwkZDSXPsd"
   },
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_QUERY_TRAIN_DATA = '../subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = '../subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742976006985,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "VqxjYq2tYDmE"
   },
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "szMEK3OkYLvX"
   },
   "source": [
    "df_query_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aslmTTJQyL2X"
   },
   "source": [
    "df_query_train.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_query_dev.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "B5X8FwLhLY3u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976030778,
     "user_tz": -60,
     "elapsed": 28,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "outputId": "36e21737-8257-4568-8346-0d3e0980ee53"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_query_dev.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6gDlBZnLcdH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976032804,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "outputId": "11cd57d2-a4b7-4b06-a9af-9ba5e29c191b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr_BDzufPmmP"
   },
   "source": "# 2) Running TF-IDF with preprocessing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1742976045832,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "BHfJ7ItxO8u8",
    "outputId": "8a4d8f08-31c0-4a9d-814e-b921b80fbe35"
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jXCC7K_ZPQL2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976047296,
     "user_tz": -60,
     "elapsed": 1414,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    }
   },
   "source": [
    "def standard_tokenizer(text):\n",
    "    \"\"\"Standard tokenizer with stemming and stop word removal.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "def name_tokenizer(text):\n",
    "    \"\"\"Tokenizer for extracting potential names (capitalized n-grams).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return [\"unknown\"] # return unknown\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    name_ngrams = []\n",
    "    for n in [2, 3]:  # Bigrams and trigrams\n",
    "        for i in range(len(pos_tags) - n + 1):\n",
    "            ngram_pos = pos_tags[i:i + n]\n",
    "            ngram_words = [word for word, tag in ngram_pos]\n",
    "            ngram_tags = [tag for word, tag in ngram_pos]\n",
    "            if all(tag == 'NNP' for tag in ngram_tags) or \\\n",
    "               (n == 2 and ngram_tags[0] == 'NNP' and ngram_tags[1] in ['NN', 'NNP']):\n",
    "                if all(word[0].isupper() for word in ngram_words):\n",
    "                    name_ngrams.append(\" \".join(ngram_words).lower())\n",
    "    if not name_ngrams:\n",
    "        return [\"unknown\"] #return unkown if no name ngrams are found.\n",
    "    return name_ngrams"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Metadata Vectorization\n",
    "title_abstract_vectorizer = TfidfVectorizer(tokenizer=standard_tokenizer)\n",
    "title_abstract_matrix = title_abstract_vectorizer.fit_transform(df_collection['title'] + \" \" + df_collection['abstract'])\n",
    "\n",
    "authors_vectorizer = TfidfVectorizer(tokenizer=name_tokenizer)\n",
    "authors_matrix = authors_vectorizer.fit_transform(df_collection['authors'].fillna(''))\n",
    "\n",
    "journal_vectorizer = TfidfVectorizer(tokenizer=standard_tokenizer)\n",
    "journal_matrix = journal_vectorizer.fit_transform(df_collection['journal'].fillna(''))\n",
    "\n",
    "def extract_year(text):\n",
    "    \"\"\"Extracts a year (4 digits) from text.\"\"\"\n",
    "    match = re.search(r'\\b\\d{4}\\b', text)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None\n",
    "\n",
    "def get_weighted_similarity_topk(query_text):\n",
    "    title_abstract_vector = title_abstract_vectorizer.transform([query_text])\n",
    "    title_abstract_similarity = cosine_similarity(title_abstract_vector, title_abstract_matrix).flatten()\n",
    "\n",
    "    potential_authors = \" \".join(name_tokenizer(query_text))\n",
    "    potential_journal = query_text # TODO extract something\n",
    "    potential_year = extract_year(query_text)\n",
    "\n",
    "    authors_vector = authors_vectorizer.transform([potential_authors])\n",
    "    journal_vector = journal_vectorizer.transform([potential_journal])\n",
    "\n",
    "    authors_similarity = cosine_similarity(authors_vector, authors_matrix).flatten()\n",
    "    journal_similarity = cosine_similarity(journal_vector, journal_matrix).flatten()\n",
    "\n",
    "    publish_time_similarity = np.zeros_like(title_abstract_similarity)\n",
    "    if potential_year is not None:\n",
    "        publish_time_similarity = 1 - np.abs(df_collection['timet'] - potential_year) / (df_collection['timet'].max() - df_collection['timet'].min())\n",
    "\n",
    "    weighted_similarity = (\n",
    "        0.6 * title_abstract_similarity +\n",
    "        0.2 * authors_similarity +\n",
    "        0.1 * journal_similarity +\n",
    "        0.1 * publish_time_similarity\n",
    "    )\n",
    "\n",
    "    top_indices = np.argsort(weighted_similarity)[::-1][:5]\n",
    "    return df_collection['cord_uid'].iloc[top_indices].tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve topk candidates using TF-IDF\n",
    "df_query_train['tfidf_topk'] = df_query_train['tweet_text'].apply(get_weighted_similarity_topk)\n",
    "df_query_dev['tfidf_topk'] = df_query_dev['tweet_text'].apply(get_weighted_similarity_topk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKBlTCZUMSc"
   },
   "source": "# 3) Evaluating TF-IDF with preprocessing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(\n",
    "            lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                      if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742976555898,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "c-vdGWXXTgjZ"
   },
   "source": [
    "# Evaluate TF-IDF results\n",
    "results_train_tfidf = get_performance_mrr(df_query_train, 'cord_uid', 'tfidf_topk')\n",
    "results_dev_tfidf = get_performance_mrr(df_query_dev, 'cord_uid', 'tfidf_topk')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1742976568622,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "xLX9SMg5USkH",
    "outputId": "7c414679-6486-4e08-dffe-23d8cffbddf0"
   },
   "source": [
    "# Print TF-IDF MRR@k results\n",
    "print(f\"TF-IDF Results on the train set: {results_train_tfidf}\")\n",
    "print(f\"TF-IDF Results on the dev set: {results_dev_tfidf}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Exporting results to prepare the submission on Codalab"
   ],
   "metadata": {
    "id": "RazcRTV84KQC"
   }
  },
  {
   "cell_type": "code",
   "source": "#df_query_dev['preds'] = df_query_dev['bm25_topk'].apply(lambda x: x[:5])",
   "metadata": {
    "id": "DFng4ocDw3Hk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976603546,
     "user_tz": -60,
     "elapsed": 39,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#df_query_dev[['post_id', 'preds']].to_csv('predictions.tsv', index=None, sep='\\t')",
   "metadata": {
    "id": "nAVBQYh_xP8O",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976608184,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "opGI1H1h4Og5"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
