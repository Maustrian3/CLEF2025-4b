{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1742975967136,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "rQPqDKP_QHFM",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.587877Z",
     "start_time": "2025-04-25T13:06:10.582411Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8N7h9BhQI5m"
   },
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742975971100,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "2GQI4HcKR6hS",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.685492Z",
     "start_time": "2025-04-25T13:06:10.671907Z"
    }
   },
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl' #MODIFY PATH"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742975975524,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "SYBB3UYbMwTA",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.769760Z",
     "start_time": "2025-04-25T13:06:10.712252Z"
    }
   },
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742975976305,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "4v3lygNOQQSn",
    "outputId": "ee5b9abd-f889-4a4e-ce11-32d2691433cb",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.795853Z",
     "start_time": "2025-04-25T13:06:10.772272Z"
    }
   },
   "source": [
    "df_collection.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7718 entries, 162 to 1056448\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cord_uid          7718 non-null   object        \n",
      " 1   source_x          7718 non-null   object        \n",
      " 2   title             7718 non-null   object        \n",
      " 3   doi               7677 non-null   object        \n",
      " 4   pmcid             4959 non-null   object        \n",
      " 5   pubmed_id         6233 non-null   object        \n",
      " 6   license           7718 non-null   object        \n",
      " 7   abstract          7718 non-null   object        \n",
      " 8   publish_time      7715 non-null   object        \n",
      " 9   authors           7674 non-null   object        \n",
      " 10  journal           6668 non-null   object        \n",
      " 11  mag_id            0 non-null      float64       \n",
      " 12  who_covidence_id  528 non-null    object        \n",
      " 13  arxiv_id          20 non-null     object        \n",
      " 14  label             7718 non-null   object        \n",
      " 15  time              7715 non-null   datetime64[ns]\n",
      " 16  timet             7718 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1742975978238,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "9veNFFGDZRx7",
    "outputId": "5eec7f85-7d20-44d7-8986-a85cb00533d8",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.813939Z",
     "start_time": "2025-04-25T13:06:10.799438Z"
    }
   },
   "source": [
    "df_collection.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAUiDU0xXLBt"
   },
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742975982410,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "v8gwkZDSXPsd",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.819333Z",
     "start_time": "2025-04-25T13:06:10.815453Z"
    }
   },
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_QUERY_TRAIN_DATA = '../subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = '../subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742976006985,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "VqxjYq2tYDmE",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.940116Z",
     "start_time": "2025-04-25T13:06:10.847273Z"
    }
   },
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "szMEK3OkYLvX",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:10.952665Z",
     "start_time": "2025-04-25T13:06:10.942134Z"
    }
   },
   "source": [
    "df_query_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3        3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4        4  Obtaining a diagnosis of autism in lower incom...  tiqksd69"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aslmTTJQyL2X",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:11.008229Z",
     "start_time": "2025-04-25T13:06:10.997836Z"
    }
   },
   "source": [
    "df_query_train.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12853 entries, 0 to 12852\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     12853 non-null  int64 \n",
      " 1   tweet_text  12853 non-null  object\n",
      " 2   cord_uid    12853 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 301.4+ KB\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "source": [
    "df_query_dev.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "B5X8FwLhLY3u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976030778,
     "user_tz": -60,
     "elapsed": 28,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "outputId": "36e21737-8257-4568-8346-0d3e0980ee53",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:11.079305Z",
     "start_time": "2025-04-25T13:06:11.069261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2       73  I recall early on reading that researchers who...  sts48u9i\n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "source": [
    "df_query_dev.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6gDlBZnLcdH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976032804,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "outputId": "11cd57d2-a4b7-4b06-a9af-9ba5e29c191b",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:11.218541Z",
     "start_time": "2025-04-25T13:06:11.207101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     1400 non-null   int64 \n",
      " 1   tweet_text  1400 non-null   object\n",
      " 2   cord_uid    1400 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr_BDzufPmmP"
   },
   "source": "# 2) Running BM25 with preprocessing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3156,
     "status": "ok",
     "timestamp": 1742976045832,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "BHfJ7ItxO8u8",
    "outputId": "8a4d8f08-31c0-4a9d-814e-b921b80fbe35",
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:11.267576Z",
     "start_time": "2025-04-25T13:06:11.259279Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jXCC7K_ZPQL2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976047296,
     "user_tz": -60,
     "elapsed": 1414,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:11.369518Z",
     "start_time": "2025-04-25T13:06:11.359254Z"
    }
   },
   "source": [
    "def standard_tokenizer(text):\n",
    "    \"\"\"Standard tokenizer with stemming and stop word removal.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "def name_tokenizer(text):\n",
    "    \"\"\"Tokenizer for extracting potential names (capitalized n-grams).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return [\"unknown\"] # return unknown\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    name_ngrams = []\n",
    "    for n in [2, 3]:  # Bigrams and trigrams\n",
    "        for i in range(len(pos_tags) - n + 1):\n",
    "            ngram_pos = pos_tags[i:i + n]\n",
    "            ngram_words = [word for word, tag in ngram_pos]\n",
    "            ngram_tags = [tag for word, tag in ngram_pos]\n",
    "            # Check if the current n-gram potentially represents a name or organization\n",
    "            # Condition 1: All words in the n-gram are proper nouns (NNP)\n",
    "            #   - This is common for multi-word names like \"Albert Einstein\" or \"World Health Organization\"\n",
    "            # Condition 2: (For bigrams only) The first word is a proper noun (NNP)\n",
    "            #   and the second word is either a common noun (NN) or another proper noun (NNP)\n",
    "            #   - This can capture patterns like \"Dr. Smith\" or \"Apple Inc.\"\n",
    "            if all(tag == 'NNP' for tag in ngram_tags) or \\\n",
    "               (n == 2 and ngram_tags[0] == 'NNP' and ngram_tags[1] in ['NN', 'NNP']):\n",
    "                    name_ngrams.append(\" \".join(ngram_words).lower())\n",
    "    if not name_ngrams:\n",
    "        return [\"unknown\"] #return unkown if no name ngrams are found.\n",
    "    return name_ngrams\n",
    "\n",
    "def extract_year(text):\n",
    "    \"\"\"Extracts a year (4 digits) from text.\"\"\"\n",
    "    match = re.search(r'\\b\\d{4}\\b', text)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:37.994039Z",
     "start_time": "2025-04-25T13:06:11.411996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize BM25 models for different metadata attributes\n",
    "corpus_title = df_collection['title'].fillna('')\n",
    "tokenized_corpus_title = [standard_tokenizer(doc) for doc in corpus_title]\n",
    "bm25_title = BM25Okapi(tokenized_corpus_title)\n",
    "\n",
    "corpus_abstract = df_collection['title'].fillna('')\n",
    "tokenized_corpus_abstract = [standard_tokenizer(doc) for doc in corpus_abstract]\n",
    "bm25_abstract = BM25Okapi(tokenized_corpus_abstract)\n",
    "\n",
    "corpus_authors = df_collection['authors'].fillna('')\n",
    "tokenized_corpus_authors = [name_tokenizer(doc) for doc in corpus_authors]\n",
    "bm25_authors = BM25Okapi(tokenized_corpus_authors)\n",
    "\n",
    "corpus_journal = df_collection['journal'].fillna('')\n",
    "tokenized_corpus_journal = [name_tokenizer(doc) for doc in corpus_journal]\n",
    "bm25_journal = BM25Okapi(tokenized_corpus_journal)\n",
    "\n",
    "cord_uids = df_collection['cord_uid'].tolist()"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:06:38.002175Z",
     "start_time": "2025-04-25T13:06:37.994039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_individual_scores(query_text):\n",
    "    \"\"\"\n",
    "    Calculates individual BM25 scores and publication time similarity\n",
    "    for a query against all papers.\n",
    "    \"\"\"\n",
    "    potential_year = extract_year(query_text)\n",
    "\n",
    "    # BM25 scores for different metadata\n",
    "    tokenized_query_standard = standard_tokenizer(query_text)\n",
    "    tokenized_query_names = name_tokenizer(query_text)\n",
    "\n",
    "    bm25_scores_title = bm25_title.get_scores(tokenized_query_standard)\n",
    "    bm25_scores_abstract = bm25_abstract.get_scores(tokenized_query_standard)\n",
    "    bm25_scores_authors = bm25_authors.get_scores(tokenized_query_names)\n",
    "    bm25_scores_journal = bm25_journal.get_scores(tokenized_query_names)\n",
    "\n",
    "    # Publication time similarity\n",
    "    publish_time_similarity = np.zeros(len(df_collection))\n",
    "    if potential_year is not None:\n",
    "        publish_time_similarity = 1 - np.abs(df_collection['timet'] - potential_year) / (\n",
    "            df_collection['timet'].max() - df_collection['timet'].min())\n",
    "    else:\n",
    "         # If no year is found, set similarity to a baseline (0)\n",
    "         publish_time_similarity = np.zeros(len(df_collection))\n",
    "\n",
    "\n",
    "    # Return individual scores\n",
    "    return {\n",
    "        'title_bm25': bm25_scores_title,\n",
    "        'abstract_bm25': bm25_scores_abstract,\n",
    "        'authors_bm25': bm25_scores_authors,\n",
    "        'journal_bm25': bm25_scores_journal,\n",
    "        'publish_time_sim': publish_time_similarity\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:07:57.639056Z",
     "start_time": "2025-04-25T13:06:38.003362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate and store individual scores for each tweet in the dev set\n",
    "df_query_dev['individual_scores'] = df_query_dev['tweet_text'].apply(get_individual_scores)"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:07:57.647831Z",
     "start_time": "2025-04-25T13:07:57.639056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rank_papers_with_weights_normalized(individual_scores, weights, k=5):\n",
    "    \"\"\"\n",
    "    Calculates weighted similarity after normalizing individual scores\n",
    "    and returns top-k cord_uids.\n",
    "    individual_scores: dictionary of score arrays from get_individual_scores\n",
    "    weights: dictionary of weights corresponding to the keys in individual_scores\n",
    "    k: number of top results to return\n",
    "    \"\"\"\n",
    "    final_similarity = np.zeros(len(df_collection))\n",
    "    scaler = MinMaxScaler() # Initialize a scaler\n",
    "\n",
    "    normalized_scores = {}\n",
    "    for key, score_array in individual_scores.items():\n",
    "        # Reshape for the scaler and fit_transform\n",
    "        if len(score_array) > 0: # Avoid scaling empty arrays if any score is missing for all docs\n",
    "             # Handle potential NaN or infinite values if they occur in raw scores before scaling\n",
    "             score_array = np.nan_to_num(score_array, nan=0.0, posinf=1e10, neginf=-1e10) # Replace inf/NaN with large/small numbers or 0\n",
    "             normalized_scores[key] = scaler.fit_transform(score_array.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "             normalized_scores[key] = score_array # Keep empty if it was empty\n",
    "\n",
    "    for key, norm_score_array in normalized_scores.items():\n",
    "        if key in weights:\n",
    "             final_similarity += weights[key] * norm_score_array\n",
    "\n",
    "\n",
    "    # Ensure final_similarity does not contain NaNs or Infs before sorting\n",
    "    final_similarity = np.nan_to_num(final_similarity, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "\n",
    "    top_indices = np.argsort(-final_similarity)[:k]\n",
    "    return [cord_uids[i] for i in top_indices]"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:07:57.655913Z",
     "start_time": "2025-04-25T13:07:57.647831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(\n",
    "            lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                      if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:07:57.662986Z",
     "start_time": "2025-04-25T13:07:57.655913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the range or specific values to explore for each weight\n",
    "weight_options = {\n",
    "    'title_bm25': [0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "    'abstract_bm25': [0.1, 0.3, 0.5, 0.7],\n",
    "    'authors_bm25': [0.1, 0.3, 0.5, 0.7],\n",
    "    'journal_bm25': [0.1, 0.3, 0.5],\n",
    "    'publish_time_sim': [0.1, 0.3, 0.5],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T13:55:55.200889Z",
     "start_time": "2025-04-25T13:14:53.960559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "best_mrr = -1\n",
    "best_weights = None\n",
    "\n",
    "# Generate all combinations of weights\n",
    "weight_combinations = list(itertools.product(*weight_options.values()))\n",
    "\n",
    "print(f\"Exploring {len(weight_combinations)} weight combinations with Grid Search...\")\n",
    "\n",
    "for combo in weight_combinations:\n",
    "    current_weights = dict(zip(weight_options.keys(), combo))\n",
    "\n",
    "    # Apply the ranking function with current weights\n",
    "    df_query_dev['preds_grid_search'] = df_query_dev['individual_scores'].apply(\n",
    "        lambda x: rank_papers_with_weights_normalized(x, current_weights, k=5)\n",
    "    )\n",
    "\n",
    "    # Evaluate the results (with MRR@5 as the objective)\n",
    "    mrr_at_5 = get_performance_mrr(df_query_dev, 'cord_uid', 'preds_grid_search', list_k=[5])[5]\n",
    "\n",
    "    if mrr_at_5 > best_mrr:\n",
    "        best_mrr = mrr_at_5\n",
    "        best_weights = current_weights\n",
    "        print(f\"New best weights: {current_weights}, MRR@5: {mrr_at_5}\")\n",
    "\n",
    "print(\"\\nGrid Search Finished.\")\n",
    "print(f\"Best MRR@5: {best_mrr}\")\n",
    "print(f\"Best Weights: {best_weights}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring 720 weight combinations with Grid Search...\n",
      "New best weights: {'title_bm25': 0.1, 'abstract_bm25': 0.1, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.26119047619047614\n",
      "New best weights: {'title_bm25': 0.1, 'abstract_bm25': 0.3, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.3786071428571428\n",
      "New best weights: {'title_bm25': 0.1, 'abstract_bm25': 0.3, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.5}, MRR@5: 0.3787261904761905\n",
      "New best weights: {'title_bm25': 0.1, 'abstract_bm25': 0.5, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.40667857142857144\n",
      "New best weights: {'title_bm25': 0.1, 'abstract_bm25': 0.7, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.41725000000000007\n",
      "New best weights: {'title_bm25': 0.3, 'abstract_bm25': 0.7, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.4218928571428571\n",
      "New best weights: {'title_bm25': 0.5, 'abstract_bm25': 0.7, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.4248571428571428\n",
      "New best weights: {'title_bm25': 0.7, 'abstract_bm25': 0.7, 'authors_bm25': 0.1, 'journal_bm25': 0.1, 'publish_time_sim': 0.1}, MRR@5: 0.4251666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[107]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     12\u001B[39m current_weights = \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(weight_options.keys(), combo))\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Apply the ranking function with current weights\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m df_query_dev[\u001B[33m'\u001B[39m\u001B[33mpreds_grid_search\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf_query_dev\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mindividual_scores\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrank_papers_with_weights_normalized\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Evaluate the results (with MRR@5 as the objective)\u001B[39;00m\n\u001B[32m     20\u001B[39m mrr_at_5 = get_performance_mrr(df_query_dev, \u001B[33m'\u001B[39m\u001B[33mcord_uid\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mpreds_grid_search\u001B[39m\u001B[33m'\u001B[39m, list_k=[\u001B[32m5\u001B[39m])[\u001B[32m5\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4789\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4790\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4791\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4796\u001B[39m     **kwargs,\n\u001B[32m   4797\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4798\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4799\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4800\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4915\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4916\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4924\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1426\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1427\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1501\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1502\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1503\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1504\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1505\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1506\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1507\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1512\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1513\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[107]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     12\u001B[39m current_weights = \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(weight_options.keys(), combo))\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Apply the ranking function with current weights\u001B[39;00m\n\u001B[32m     15\u001B[39m df_query_dev[\u001B[33m'\u001B[39m\u001B[33mpreds_grid_search\u001B[39m\u001B[33m'\u001B[39m] = df_query_dev[\u001B[33m'\u001B[39m\u001B[33mindividual_scores\u001B[39m\u001B[33m'\u001B[39m].apply(\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mrank_papers_with_weights_normalized\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m )\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Evaluate the results (with MRR@5 as the objective)\u001B[39;00m\n\u001B[32m     20\u001B[39m mrr_at_5 = get_performance_mrr(df_query_dev, \u001B[33m'\u001B[39m\u001B[33mcord_uid\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mpreds_grid_search\u001B[39m\u001B[33m'\u001B[39m, list_k=[\u001B[32m5\u001B[39m])[\u001B[32m5\u001B[39m]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[99]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mrank_papers_with_weights_normalized\u001B[39m\u001B[34m(individual_scores, weights, k)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(score_array) > \u001B[32m0\u001B[39m: \u001B[38;5;66;03m# Avoid scaling empty arrays if any score is missing for all docs\u001B[39;00m\n\u001B[32m     16\u001B[39m      \u001B[38;5;66;03m# Handle potential NaN or infinite values if they occur in raw scores before scaling\u001B[39;00m\n\u001B[32m     17\u001B[39m      score_array = np.nan_to_num(score_array, nan=\u001B[32m0.0\u001B[39m, posinf=\u001B[32m1e10\u001B[39m, neginf=-\u001B[32m1e10\u001B[39m) \u001B[38;5;66;03m# Replace inf/NaN with large/small numbers or 0\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m      normalized_scores[key] = \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscore_array\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m.flatten()\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     20\u001B[39m      normalized_scores[key] = score_array \u001B[38;5;66;03m# Keep empty if it was empty\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    322\u001B[39m         return_tuple = (\n\u001B[32m    323\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    324\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    325\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\base.py:918\u001B[39m, in \u001B[36mTransformerMixin.fit_transform\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m    903\u001B[39m         warnings.warn(\n\u001B[32m    904\u001B[39m             (\n\u001B[32m    905\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has a `transform`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    914\u001B[39m         )\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    917\u001B[39m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m918\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    920\u001B[39m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    322\u001B[39m         return_tuple = (\n\u001B[32m    323\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    324\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    325\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:532\u001B[39m, in \u001B[36mMinMaxScaler.transform\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    528\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    530\u001B[39m xp, _ = get_namespace(X)\n\u001B[32m--> \u001B[39m\u001B[32m532\u001B[39m X = \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    534\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    536\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_array_api\u001B[49m\u001B[43m.\u001B[49m\u001B[43msupported_float_dtypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    539\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    540\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    542\u001B[39m X *= \u001B[38;5;28mself\u001B[39m.scale_\n\u001B[32m    543\u001B[39m X += \u001B[38;5;28mself\u001B[39m.min_\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2942\u001B[39m         out = X, y\n\u001B[32m   2943\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m-> \u001B[39m\u001B[32m2944\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2945\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m   2946\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\utils\\validation.py:1118\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1115\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[32m   1116\u001B[39m     \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n\u001B[32m   1117\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m np.may_share_memory(array, array_orig):\n\u001B[32m-> \u001B[39m\u001B[32m1118\u001B[39m         array = \u001B[43m_asarray_with_order\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1119\u001B[39m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxp\u001B[49m\n\u001B[32m   1120\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1121\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1122\u001B[39m     \u001B[38;5;66;03m# always make a copy for non-numpy arrays\u001B[39;00m\n\u001B[32m   1123\u001B[39m     array = _asarray_with_order(\n\u001B[32m   1124\u001B[39m         array, dtype=dtype, order=order, copy=\u001B[38;5;28;01mTrue\u001B[39;00m, xp=xp\n\u001B[32m   1125\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\clef2025-checkthat-lab-SqgNKs6A-py3.11\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:837\u001B[39m, in \u001B[36m_asarray_with_order\u001B[39m\u001B[34m(array, dtype, order, copy, xp, device)\u001B[39m\n\u001B[32m    834\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[32m    835\u001B[39m     \u001B[38;5;66;03m# Use NumPy API to support order\u001B[39;00m\n\u001B[32m    836\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m837\u001B[39m         array = numpy.array(array, order=order, dtype=dtype)\n\u001B[32m    838\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    839\u001B[39m         array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TRAIN SET EVAL"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:16:56.894814Z",
     "start_time": "2025-04-25T10:16:56.888934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_weighted_similarity_topk(query_text,\n",
    "                        weight_title=1.0,\n",
    "                        weight_abstract=0.7,\n",
    "                        weight_authors=0.3,\n",
    "                        weight_journal=0.3,\n",
    "                        weight_publish_time=0.1):\n",
    "    \"\"\"\n",
    "    Calculates a weighted similarity score using BM25 on different metadata\n",
    "    and publication time.\n",
    "    \"\"\"\n",
    "    potential_year = extract_year(query_text)\n",
    "\n",
    "    # BM25 scores for different metadata\n",
    "    tokenized_query_standard = standard_tokenizer(query_text)\n",
    "    tokenized_query_names = name_tokenizer(query_text)\n",
    "\n",
    "    bm25_scores_title = bm25_title.get_scores(tokenized_query_standard)\n",
    "    bm25_scores_abstract = bm25_abstract.get_scores(tokenized_query_standard)\n",
    "    bm25_scores_authors = bm25_authors.get_scores(tokenized_query_names)\n",
    "    bm25_scores_journal = bm25_journal.get_scores(tokenized_query_names)\n",
    "\n",
    "    # Publication time similarity\n",
    "    publish_time_similarity = np.zeros(len(df_collection))\n",
    "    if potential_year is not None:\n",
    "        publish_time_similarity = 1 - np.abs(df_collection['timet'] - potential_year) / (\n",
    "            df_collection['timet'].max() - df_collection['timet'].min())\n",
    "\n",
    "    # Combine weighted BM25 scores and publish time similarity\n",
    "    final_similarity = (\n",
    "        weight_title * bm25_scores_title +\n",
    "        weight_abstract * bm25_scores_abstract +\n",
    "        weight_authors * bm25_scores_authors +\n",
    "        weight_journal * bm25_scores_journal +\n",
    "        weight_publish_time * publish_time_similarity # FIXME different scales between bm25 scores and publish time similarity\n",
    "    )\n",
    "\n",
    "    top_indices = np.argsort(-final_similarity)[:5]\n",
    "    return df_collection['cord_uid'].iloc[top_indices].tolist()"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:33:32.619003Z",
     "start_time": "2025-04-25T10:16:59.471900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve topk candidates using BM25\n",
    "df_query_train['weighted_bm25_topk'] = df_query_train['tweet_text'].apply(get_weighted_similarity_topk)"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKBlTCZUMSc"
   },
   "source": "# 3) Evaluating BM25 with preprocessing"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:40:10.614846Z",
     "start_time": "2025-04-25T10:40:09.381199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate BM25 results with best weights\n",
    "best_weights = {'title_bm25': 1.0, 'abstract_bm25': 0.7, 'authors_bm25': 0.3, 'journal_bm25': 0.3, 'publish_time_sim': 0.1}\n",
    "\n",
    "df_query_dev['weighted_bm25_topk'] = df_query_dev['individual_scores'].apply(\n",
    "    lambda x: rank_papers_with_weights(x, best_weights, k=5)\n",
    ")\n",
    "\n",
    "results_train_weighted_bm25 = get_performance_mrr(df_query_train, 'cord_uid', 'weighted_bm25_topk')\n",
    "results_dev_weighted_bm25 = get_performance_mrr(df_query_dev, 'cord_uid', 'weighted_bm25_topk')"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1742976568622,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "xLX9SMg5USkH",
    "outputId": "7c414679-6486-4e08-dffe-23d8cffbddf0",
    "ExecuteTime": {
     "end_time": "2025-04-25T10:40:17.997494Z",
     "start_time": "2025-04-25T10:40:17.977661Z"
    }
   },
   "source": [
    "# Print BM25 MRR@k results\n",
    "# print(f\"BM25 Results on the train set: {results_train_weighted_bm25}\")\n",
    "print(f\"BM25 Results on the dev set: {results_dev_weighted_bm25}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Results on the train set: {1: np.float64(0.38053372753442777), 5: np.float64(0.42342254726522993), 10: np.float64(0.42342254726522993)}\n",
      "BM25 Results on the dev set: {1: np.float64(0.39), 5: np.float64(0.4312261904761905), 10: np.float64(0.4312261904761905)}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Exporting results to prepare the submission on Codalab"
   ],
   "metadata": {
    "id": "RazcRTV84KQC"
   }
  },
  {
   "cell_type": "code",
   "source": "df_query_dev['preds'] = df_query_dev['weighted_bm25_topk'].apply(lambda x: x[:5])",
   "metadata": {
    "id": "DFng4ocDw3Hk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976603546,
     "user_tz": -60,
     "elapsed": 39,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-04-25T09:26:17.006077Z",
     "start_time": "2025-04-25T09:26:16.996710Z"
    }
   },
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": "df_query_dev[['post_id', 'preds']].to_csv('predictions_weighted_bm25.tsv', index=None, sep='\\t')",
   "metadata": {
    "id": "nAVBQYh_xP8O",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1742976608184,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-04-25T09:26:17.022544Z",
     "start_time": "2025-04-25T09:26:17.006077Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "opGI1H1h4Og5",
    "ExecuteTime": {
     "end_time": "2025-04-25T09:26:17.026980Z",
     "start_time": "2025-04-25T09:26:17.022544Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
