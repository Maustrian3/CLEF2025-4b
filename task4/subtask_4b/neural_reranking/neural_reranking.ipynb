{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc808609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read from the JSON file\n",
    "def load_series_from_json(filename):\n",
    "    loaded_series = pd.read_json(filename)\n",
    "    return loaded_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ranked_train'\n",
    "ranked_train = load_series_from_json(filename)\n",
    "\n",
    "filename = 'ranked_dev'\n",
    "ranked_dev = load_series_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e38809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['htlvpvz5', '32z7b3fp', 'dbgtslc8', '52cub1om', 'h7hj64q5', 'trmwm9qq', '65gedo6u', 'rwgqkow3', 'am11yqbf', 'tpovr13h']\n"
     ]
    }
   ],
   "source": [
    "print(ranked_train['tfidf_topk'][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ee3266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       post_id                                         tweet_text  cord_uid  \\\n",
      "0            0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5   \n",
      "1            1  this study isn't receiving sufficient attentio...  4kfl29ul   \n",
      "2            2  thanks, xi jinping. a reminder that this study...  jtwb17u8   \n",
      "3            3  Taiwan - a population of 23 million has had ju...  0w9k8iy1   \n",
      "4            4  Obtaining a diagnosis of autism in lower incom...  tiqksd69   \n",
      "...        ...                                                ...       ...   \n",
      "12848    14248  \"evidence on covid-19 reveals a growing body o...  9169o29b   \n",
      "12849    14249  Outdoor lighting has detrimental impacts on lo...  s2bpha8l   \n",
      "12850    14250  26/ and influenza virus (and other pathogens, ...  atloc9th   \n",
      "12851    14251  does it?'sars-cov-2-naÃ¯ve vaccinees had a 13.0...  t4y1ylb3   \n",
      "12852    14252  when \"the airway immune cells of children are ...  nlsv8bin   \n",
      "\n",
      "                                              tfidf_topk  \n",
      "0      [htlvpvz5, 32z7b3fp, dbgtslc8, 52cub1om, h7hj6...  \n",
      "1      [apqzyln2, asdcpvhx, 33znyrn8, ljcdfmbu, 296il...  \n",
      "2      [jtwb17u8, a0q61mpi, veeavho5, 8hkxbxz9, hnsv4...  \n",
      "3      [lsgm7y5t, 9g0lo2e9, l5ogbl5p, x14iywtr, b97ac...  \n",
      "4      [tiqksd69, k7smwz6w, b0dzhsrh, aqbhxv1f, 0u330...  \n",
      "...                                                  ...  \n",
      "12848  [1y1ik2u9, 1zuhilmu, dgng39yd, 9169o29b, qfren...  \n",
      "12849  [s2bpha8l, 8a3fp7ym, 2u9eenwu, ajzl3y46, jkdyo...  \n",
      "12850  [y7s6qt6j, lavcsqov, g17lp8ch, 7s79900j, 7y6ok...  \n",
      "12851  [t4y1ylb3, 7a543f7v, ro3k6q06, 8ztv34hs, ws3tf...  \n",
      "12852  [nlsv8bin, cwqsbwaj, noxt3ikd, 8gh3q7p3, tiu86...  \n",
      "\n",
      "[12853 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ranked_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5583b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      post_id                                         tweet_text  cord_uid  \\\n",
      "0          16  covid recovery: this study from the usa reveal...  3qvh482o   \n",
      "1          69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu   \n",
      "2          73  I recall early on reading that researchers who...  sts48u9i   \n",
      "3          93  You know you're credible when NIH website has ...  3sr2exq9   \n",
      "4          96  Resistance to antifungal medications is a grow...  ybwwmyqy   \n",
      "...       ...                                                ...       ...   \n",
      "1395    14193  Residents at high risk of covid-19: effectiven...  0gn3b98n   \n",
      "1396    14196  61% of teenagers hospitalized for covid were \"...  25bdifv6   \n",
      "1397    14203  \"fresh evidence backing melatonin against covi...  qn6wawxk   \n",
      "1398    14233  the vaccine doesn't halt the spread, it is pro...  3u3i5myh   \n",
      "1399    14236  \"Great commentary from K. Carvalho,  black pre...  nih4l4ok   \n",
      "\n",
      "                                             tfidf_topk  \n",
      "0     [25aj8rj5, 66g5lpm6, 59up4v56, o4vvlmr4, vmmwt...  \n",
      "1     [r58aohnu, p0kg6dyz, yrowv62k, eay6qfhz, s2vck...  \n",
      "2     [mkwgkkoi, vtcq6jgf, ohyhjt2p, zu8dg7ma, gruir...  \n",
      "3     [3sr2exq9, sv48gjkk, z795y51f, tx8ypqsm, k0f4c...  \n",
      "4     [ybwwmyqy, ouvq2wpq, sxx3yid9, rs3umc1x, ierqf...  \n",
      "...                                                 ...  \n",
      "1395  [0gn3b98n, d8x3b9a3, wotf0lzx, 5chxk43x, 4sqjv...  \n",
      "1396  [s1qnjgev, b7llp5r7, s1gdbsfx, idhbd102, ajido...  \n",
      "1397  [qn6wawxk, dsz66r4u, 059oj76m, b3ui95vx, 7x1aj...  \n",
      "1398  [25aj8rj5, qh6fqna8, x84su3ki, ky5env7t, ivue7...  \n",
      "1399  [nih4l4ok, ndcg3o7a, rm6av6sj, dfhxcqr8, pmvax...  \n",
      "\n",
      "[1400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ranked_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oral care in rehabilitation medicine: oral vulnerability, oral muscle wasting, and hospital-associated oral issues\n"
     ]
    }
   ],
   "source": [
    "tweet_texts = ranked_train['tweet_text']\n",
    "print(tweet_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e9b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of a tweet:  85\n",
      "Average length of a tweet:  29.8\n"
     ]
    }
   ],
   "source": [
    "max_length_tweet = 0\n",
    "sum_length_tweets = 0\n",
    "list_length_tweets = []\n",
    "for tweet in tweet_texts:\n",
    "    len_tweet = len(tweet.split(' '))\n",
    "    sum_length_tweets += len_tweet\n",
    "    list_length_tweets.append(len_tweet)\n",
    "    if len_tweet > max_length_tweet:\n",
    "        max_length_tweet = len_tweet\n",
    "\n",
    "avg_length_tweets = sum_length_tweets/len(tweet_texts)\n",
    "\n",
    "print(\"Maximum length of a tweet: \", max_length_tweet)\n",
    "print(\"Average length of a tweet: \", f'{avg_length_tweets:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ea7447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(list_length_tweets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b150803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# A check if the corresponding paper is in the top 100\n",
    "print(ranked_train['cord_uid'][0] in ranked_train['tfidf_topk'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7718 entries, 162 to 1056448\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cord_uid          7718 non-null   object        \n",
      " 1   source_x          7718 non-null   object        \n",
      " 2   title             7718 non-null   object        \n",
      " 3   doi               7677 non-null   object        \n",
      " 4   pmcid             4959 non-null   object        \n",
      " 5   pubmed_id         6233 non-null   object        \n",
      " 6   license           7718 non-null   object        \n",
      " 7   abstract          7718 non-null   object        \n",
      " 8   publish_time      7715 non-null   object        \n",
      " 9   authors           7674 non-null   object        \n",
      " 10  journal           6668 non-null   object        \n",
      " 11  mag_id            0 non-null      float64       \n",
      " 12  who_covidence_id  528 non-null    object        \n",
      " 13  arxiv_id          20 non-null     object        \n",
      " 14  label             7718 non-null   object        \n",
      " 15  time              7715 non-null   datetime64[ns]\n",
      " 16  timet             7718 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Import references\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "df_collection.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ace1f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44add005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cross-sectional study examines rates of cognitive impairment among patients who survived COVID-19 and whether the care setting was associated with cognitive impairment rates. \n",
      "\n",
      "cord_uid\n",
      "umvrwgaw    BACKGROUND: Governments are preparing for a po...\n",
      "spiud6ok    The basic reproductive ratio, R (0), is one of...\n",
      "aclzp3iy    The pandemic of swine flu (H1N1) influenza spr...\n",
      "ycxyn2a2    The mode of infection transmission has profoun...\n",
      "zxe95qy9    In the new millennium, the centuries-old strat...\n",
      "Name: abstract, dtype: object\n"
     ]
    }
   ],
   "source": [
    "paper_info = df_collection.set_index('cord_uid')['abstract']\n",
    "print(paper_info['3qvh482o'], '\\n')\n",
    "print(paper_info[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c5aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the abstracts:  1234\n",
      "Average length of an abstract:  232.0\n"
     ]
    }
   ],
   "source": [
    "max_length_abstract = 0\n",
    "sum_length_abstracts = 0\n",
    "list_length_abstracts = []\n",
    "paper_info_list = paper_info.values.tolist()\n",
    "for abstract in paper_info_list:\n",
    "    word_count = len(abstract.split(' '))\n",
    "    sum_length_abstracts += word_count\n",
    "    list_length_abstracts.append(word_count)\n",
    "    if word_count > max_length_abstract:\n",
    "        max_length_abstract = word_count\n",
    "\n",
    "avg_length_abstracts = sum_length_abstracts/len(paper_info_list)\n",
    "\n",
    "print(\"Maximum length of the abstracts: \", max_length_abstract)\n",
    "print(\"Average length of an abstract: \", f'{avg_length_abstracts:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd1edb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list_length_abstracts)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_text    Oral care in rehabilitation medicine: oral vul...\n",
      "cord_uid                                               htlvpvz5\n",
      "tfidf_topk    [htlvpvz5, 32z7b3fp, dbgtslc8, 52cub1om, h7hj6...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweet_info_train = ranked_train[[\"tweet_text\", \"cord_uid\", \"tfidf_topk\"]]\n",
    "print(tweet_info_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d385346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_id                                                      16\n",
      "tweet_text    covid recovery: this study from the usa reveal...\n",
      "cord_uid                                               3qvh482o\n",
      "tfidf_topk    [25aj8rj5, 66g5lpm6, 59up4v56, o4vvlmr4, vmmwt...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweet_info_dev = ranked_dev[[\"post_id\", \"tweet_text\", \"cord_uid\", \"tfidf_topk\"]]\n",
    "print(tweet_info_dev.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7267da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get n negative samples (excluding the positive sample)\n",
    "def get_negative_samples(topk, positive_sample, n=10):\n",
    "    # Remove the excluded element if it exists\n",
    "    filtered_array = [elem for elem in topk if elem != positive_sample]\n",
    "    \n",
    "    # Make sure n isn't bigger than the available elements\n",
    "    n = min(n, len(filtered_array))\n",
    "    \n",
    "    return random.sample(filtered_array, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c665726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6hl55c60', 'znpc5jq2', 'nnkul2r2', '4j8efxsw', 'omk29wlq', 'f3j0g82b', 'xmjj1s5s', '1gv3t5t0', '65gedo6u', 'iwd8nkls']\n"
     ]
    }
   ],
   "source": [
    "first_tweet_info = tweet_info_train.iloc[0]\n",
    "positive_sample = first_tweet_info['cord_uid']\n",
    "topk = first_tweet_info['tfidf_topk']\n",
    "negative_samples = get_negative_samples(topk, positive_sample, 10)\n",
    "\n",
    "assert positive_sample not in negative_samples\n",
    "\n",
    "print(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a295a4",
   "metadata": {},
   "source": [
    "Baseline approach\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12853 training examples as dictionaries.\n",
      "Converted to Hugging Face Dataset with 12853 rows and columns: ['query', 'positive', 'label']\n",
      "{'query': 'Oral care in rehabilitation medicine: oral vulnerability, oral muscle wasting, and hospital-associated oral issues', 'positive': 'Oral health is a crucial but often neglected aspect of rehabilitation medicine. Approximately 71% of hospitalized rehabilitation patients and 91% of hospitalized acute care patients have impaired oral health. Poor oral condition in hospitalized patients can be attributed to factors such as age, physical dependency, cognitive decline, malnutrition, low skeletal muscle mass and strength, and multimorbidity. Another major factor is a lack of knowledge and interest in oral problems among health care workers. Recently, new concepts have been proposed, such as oral frailty, oral sarcopenia, and hospital-associated oral problems. Oral frailty, the accumulation of a slightly poor status of oral conditions and function, strongly predicts physical frailty, dysphagia, malnutrition, need for long-term care, and mortality in community-dwelling older adults. Oral sarcopenia refers to sarcopenia associated with oral conditions and function, although its definition has not yet been fully discussed. Hospital-associated oral problems are caused by disease, disease treatment, surgery, endotracheal intubation, poor self-care abilities, lack of care by medical staff, drugs, and iatrogenic factors during hospitalization. Furthermore, oral problems have negative impacts on rehabilitation outcomes, which include functional recovery, length of hospital stay, discharge home, and in-hospital mortality. Oral health management provided by dental hygienists improves not only oral status and function, swallowing function, and nutritional status but also activities of daily living, discharge home, and in-hospital mortality in post-acute rehabilitation. Oral rehabilitation, promotion, education, and medical-dental collaboration can be effective interventions for oral problems and therefore are necessary to improve rehabilitation outcomes.', 'label': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_list_of_dicts = []\n",
    "\n",
    "# Iterate through the rows of the tweet_info_train DataFrame\n",
    "for index, row in tweet_info_train.iterrows():\n",
    "    tweet_text = row[\"tweet_text\"]\n",
    "    correct_cord_uid = row[\"cord_uid\"]\n",
    "\n",
    "    # Get the abstract of the correct paper\n",
    "    if correct_cord_uid in paper_info:\n",
    "        correct_abstract = paper_info[correct_cord_uid]\n",
    "\n",
    "        # Ensure the abstract is a string and not empty\n",
    "        if isinstance(correct_abstract, str) and correct_abstract.strip():\n",
    "            # Create a dictionary for this training example (a positive pair)\n",
    "            example_dict = {\n",
    "                'query': tweet_text,\n",
    "                'positive': correct_abstract,\n",
    "                'label': 1.0 # Label indicating it's a positive pair\n",
    "            }\n",
    "            train_data_list_of_dicts.append(example_dict)\n",
    "\n",
    "        else:\n",
    "            # Handle cases where the abstract is missing, not a string, or empty after stripping whitespace\n",
    "            print(f\"Warning: Abstract {correct_abstract} is not a string or empty, skipping tweet at index {index}.\")\n",
    "\n",
    "    else:\n",
    "        # Handle cases where the correct paper's abstract is not found in your paper_info data\n",
    "        print(f\"Warning: Correct abstract not found for {correct_cord_uid} for tweet at index {index}, skipping tweet.\")\n",
    "\n",
    "print(f\"Created {len(train_data_list_of_dicts)} training examples as dictionaries.\")\n",
    "\n",
    "# Convert the list of dictionaries into a datasets.Dataset\n",
    "train_dataset = Dataset.from_list(train_data_list_of_dicts)\n",
    "\n",
    "print(f\"Converted to Hugging Face Dataset with {len(train_dataset)} rows and columns: {train_dataset.column_names}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_sentence_transformer_model(name):\n",
    "    try:\n",
    "        model = SentenceTransformer(name)\n",
    "        print(f\"Successfully loaded {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the model. Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback if it fails\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_sentence_transformer_model('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bbe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 186 correct cord_uid's are not in top-k for validation tweets, cannot evaluate re-ranking for them.\n",
      "Created 1214 evaluation examples for RerankingEvaluator.\n",
      "{'query': 'covid recovery: this study from the usa reveals that a proportion of cases experience impairment in some cognitive functions for several months after infection. some possible biases &amp; limitations but more research is required on impact of these long term effects.', 'positive': ['This cross-sectional study examines rates of cognitive impairment among patients who survived COVID-19 and whether the care setting was associated with cognitive impairment rates.'], 'negative': ['Since its emergence in Wuhan, China, covid-19 has spread and had a profound effect on the lives and health of people around the globe. As of 4 July 2021, more than 183 million confirmed cases of covid-19 had been recorded worldwide, and 3.97 million deaths. Recent evidence has shown that a range of persistent symptoms can remain long after the acute SARS-CoV-2 infection, and this condition is now coined long covid by recognized research institutes. Studies have shown that long covid can affect the whole spectrum of people with covid-19, from those with very mild acute disease to the most severe forms. Like acute covid-19, long covid can involve multiple organs and can affect many systems including, but not limited to, the respiratory, cardiovascular, neurological, gastrointestinal, and musculoskeletal systems. The symptoms of long covid include fatigue, dyspnea, cardiac abnormalities, cognitive impairment, sleep disturbances, symptoms of post-traumatic stress disorder, muscle pain, concentration problems, and headache. This review summarizes studies of the long term effects of covid-19 in hospitalized and non-hospitalized patients and describes the persistent symptoms they endure. Risk factors for acute covid-19 and long covid and possible therapeutic options are also discussed.', 'BACKGROUND: No effective pharmacological or non-pharmacological interventions exist for patients with long COVID. We aimed to describe recovery 1 year after hospital discharge for COVID-19, identify factors associated with patient-perceived recovery, and identify potential therapeutic targets by describing the underlying inflammatory profiles of the previously described recovery clusters at 5 months after hospital discharge. METHODS: The Post-hospitalisation COVID-19 study (PHOSP-COVID) is a prospective, longitudinal cohort study recruiting adults (aged â¥18 years) discharged from hospital with COVID-19 across the UK. Recovery was assessed using patient-reported outcome measures, physical performance, and organ function at 5 months and 1 year after hospital discharge, and stratified by both patient-perceived recovery and recovery cluster. Hierarchical logistic regression modelling was performed for patient-perceived recovery at 1 year. Cluster analysis was done using the clustering large applications k-medoids approach using clinical outcomes at 5 months. Inflammatory protein profiling was analysed from plasma at the 5-month visit. This study is registered on the ISRCTN Registry, ISRCTN10980107, and recruitment is ongoing. FINDINGS: 2320 participants discharged from hospital between March 7, 2020, and April 18, 2021, were assessed at 5 months after discharge and 807 (32Â·7%) participants completed both the 5-month and 1-year visits. 279 (35Â·6%) of these 807 patients were women and 505 (64Â·4%) were men, with a mean age of 58Â·7 (SD 12Â·5) years, and 224 (27Â·8%) had received invasive mechanical ventilation (WHO class 7â9). The proportion of patients reporting full recovery was unchanged between 5 months (501 [25Â·5%] of 1965) and 1 year (232 [28Â·9%] of 804). Factors associated with being less likely to report full recovery at 1 year were female sex (odds ratio 0Â·68 [95% CI 0Â·46â0Â·99]), obesity (0Â·50 [0Â·34â0Â·74]) and invasive mechanical ventilation (0Â·42 [0Â·23â0Â·76]). Cluster analysis (n=1636) corroborated the previously reported four clusters: very severe, severe, moderate with cognitive impairment, and mild, relating to the severity of physical health, mental health, and cognitive impairment at 5 months. We found increased inflammatory mediators of tissue damage and repair in both the very severe and the moderate with cognitive impairment clusters compared with the mild cluster, including IL-6 concentration, which was increased in both comparisons (n=626 participants). We found a substantial deficit in median EQ-5D-5L utility index from before COVID-19 (retrospective assessment; 0Â·88 [IQR 0Â·74â1Â·00]), at 5 months (0Â·74 [0Â·64â0Â·88]) to 1 year (0Â·75 [0Â·62â0Â·88]), with minimal improvements across all outcome measures at 1 year after discharge in the whole cohort and within each of the four clusters. INTERPRETATION: The sequelae of a hospital admission with COVID-19 were substantial 1 year after discharge across a range of health domains, with the minority in our cohort feeling fully recovered. Patient-perceived health-related quality of life was reduced at 1 year compared with before hospital admission. Systematic inflammation and obesity are potential treatable traits that warrant further investigation in clinical trials. FUNDING: UK Research and Innovation and National Institute for Health Research.', 'Importance COVID-19 is associated with clinically significant symptoms despite resolution of the acute infection (i.e., post-COVID-19 syndrome). Fatigue and cognitive impairment are amongst the most common and debilitating symptoms of post-COVID-19 syndrome. OBJECTIVE: To quantify the proportion of individuals experiencing fatigue and cognitive impairment 12 or more weeks following COVID-19 diagnosis, and to characterize the inflammatory correlates and functional consequences of post-COVID-19 syndrome. DATA SOURCES: Systematic searches were conducted without language restrictions from database inception to June 8, 2021 on PubMed/MEDLINE, The Cochrane Library, PsycInfo, Embase, Web of Science, Google/Google Scholar, and select reference lists. STUDY SELECTION: Primary research articles which evaluated individuals at least 12 weeks after confirmed COVID-19 diagnosis and specifically reported on fatigue, cognitive impairment, inflammatory parameters, and/or functional outcomes were selected. DATA EXTRACTION & SYNTHESIS: Two reviewers independently extracted published summary data and assessed methodological quality and risk of bias. A meta-analysis of proportions was conducted to pool Freeman-Turkey double arcsine transformed proportions using the random-effects restricted maximum-likelihood model. MAIN OUTCOMES & MEASURES: The co-primary outcomes were the proportions of individuals reporting fatigue and cognitive impairment, respectively, 12 or more weeks after COVID-19 infection. The secondary outcomes were inflammatory correlates and functional consequences of post-COVID-19 syndrome. RESULTS: The literature search yielded 10,979 studies, and 81 studies were selected for inclusion. The fatigue meta-analysis comprised 68 studies, the cognitive impairment meta-analysis comprised 43 studies, and 48 studies were included in the narrative synthesis. Meta-analysis revealed that the proportion of individuals experiencing fatigue 12 or more weeks following COVID-19 diagnosis was 0.32 (95% CI, 0.27, 0.37; p < 0.001; n = 25,268; I(2)=99.1%). The proportion of individuals exhibiting cognitive impairment was 0.22 (95% CI, 0.17, 0.28; p < 0.001; n = 13,232; I(2)=98.0). Moreover, narrative synthesis revealed elevations in proinflammatory markers and considerable functional impairment in a subset of individuals. CONCLUSIONS & RELEVANCE: A significant proportion of individuals experience persistent fatigue and/or cognitive impairment following resolution of acute COVID-19. The frequency and debilitating nature of the foregoing symptoms provides the impetus to characterize the underlying neurobiological substrates and how to best treat these phenomena. Study Registration PROSPERO (CRD42021256965)', 'Importance: While much of the attention on the COVID-19 pandemic was directed at the daily counts of cases and those with serious disease overwhelming health services, increasingly, reports have appeared of people who experience debilitating symptoms after the initial infection. This is popularly known as long COVID. Objective: To estimate by country and territory of the number of patients affected by long COVID in 2020 and 2021, the severity of their symptoms and expected pattern of recovery Design: We jointly analyzed ten ongoing cohort studies in ten countries for the occurrence of three major symptom clusters of long COVID among representative COVID cases. The defining symptoms of the three clusters (fatigue, cognitive problems, and shortness of breath) are explicitly mentioned in the WHO clinical case definition. For incidence of long COVID, we adopted the minimum duration after infection of three months from the WHO case definition. We pooled data from the contributing studies, two large medical record databases in the United States, and findings from 44 published studies using a Bayesian meta-regression tool. We separately estimated occurrence and pattern of recovery in patients with milder acute infections and those hospitalized. We estimated the incidence and prevalence of long COVID globally and by country in 2020 and 2021 as well as the severity-weighted prevalence using disability weights from the Global Burden of Disease study. Results: Analyses are based on detailed information for 1906 community infections and 10526 hospitalized patients from the ten collaborating cohorts, three of which included children. We added published data on 37262 community infections and 9540 hospitalized patients as well as ICD-coded medical record data concerning 1.3 million infections. Globally, in 2020 and 2021, 144.7 million (95% uncertainty interval [UI] 54.8-312.9) people suffered from any of the three symptom clusters of long COVID. This corresponds to 3.69% (1.38-7.96) of all infections. The fatigue, respiratory, and cognitive clusters occurred in 51.0% (16.9-92.4), 60.4% (18.9-89.1), and 35.4% (9.4-75.1) of long COVID cases, respectively. Those with milder acute COVID-19 cases had a quicker estimated recovery (median duration 3.99 months [IQR 3.84-4.20]) than those admitted for the acute infection (median duration 8.84 months [IQR 8.10-9.78]). At twelve months, 15.1% (10.3-21.1) continued to experience long COVID symptoms. Conclusions and relevance: The occurrence of debilitating ongoing symptoms of COVID-19 is common. Knowing how many people are affected, and for how long, is important to plan for rehabilitative services and support to return to social activities, places of learning, and the workplace when symptoms start to wane.', 'Background Long COVID is increasingly recognised as public health burden. Demographic and infection-related characteristics have been identified as risk factors, but less research has focused on psychosocial predictors such as stress immediately preceding the index infection. Research on whether stressors predict the development of specific long COVID symptoms is also lacking. Methods Data from 1,966 UK adults who had previously been infected with COVID-19 and who took part in the UCL COVID-19 Social Study were analysed. The number of adversity experiences (e.g., job loss) and the number of worries about adversity experiences within the month prior to COVID-19 infection were used to predict the development of self-reported long COVID and the presence of three specific long COVID symptoms (difficulty with mobility, cognition, and self-care). The interaction between a three-level index of socio-economic position (SEP; with higher values indicating lower SEP) and the exposure variables in relation to long COVID status was also examined. Analyses controlled for a range of COVID-19 infection characteristics, socio-demographics, and health-related factors. Findings Odds of self-reported long COVID increased by 1.25 (95% confidence interval [CI]: 1.04 to 1.51) for each additional worry about adversity in the month prior to COVID-19 infection. Although there was no evidence for an interaction between SEP and either exposure variable, individuals in the lowest SEP group were nearly twice as likely to have developed long COVID as those in the highest SEP group (OR: 1.95; 95% CI: 1.19 to 3.19) and worries about adversity experiences remained a predictor of long COVID (OR: 1.43; 95% CI: 1.04 to 1.98). The number of worries about adversity experiences also corresponded with increased odds of certain long COVID symptoms such as difficulty with cognition (e.g., difficulty remembering or concentrating) by 1.46 (95% CI: 1.02 to 2.09) but not with mobility (e.g., walking or climbing steps) or self-care (e.g., washing all over or dressing). Interpretation Results suggest a key role of stress in the time preceding the acute COVID-19 infection for the development of long COVID and for difficulty with cognition specifically. These findings point to the importance of mitigating worries and experiences of adversities during pandemics both to reduce their psychological impact but also help reduce the societal burden of longer-term illness.', \"BACKGROUND: A significant number of patients with COVID-19 experience prolonged symptoms, known as Long COVID. Few systematic studies have investigated this population, particularly in outpatient settings. Hence, relatively little is known about symptom makeup and severity, expected clinical course, impact on daily functioning, and return to baseline health. METHODS: We conducted an online survey of people with suspected and confirmed COVID-19, distributed via COVID-19 support groups (e.g. Body Politic, Long COVID Support Group, Long Haul COVID Fighters) and social media (e.g. Twitter, Facebook). Data were collected from September 6, 2020 to November 25, 2020. We analyzed responses from 3762 participants with confirmed (diagnostic/antibody positive; 1020) or suspected (diagnostic/antibody negative or untested; 2742) COVID-19, from 56 countries, with illness lasting over 28 days and onset prior to June 2020. We estimated the prevalence of 203 symptoms in 10 organ systems and traced 66 symptoms over seven months. We measured the impact on life, work, and return to baseline health. FINDINGS: For the majority of respondents (>91%), the time to recovery exceeded 35 weeks. During their illness, participants experienced an average of 55.9+/- 25.5 (mean+/-STD) symptoms, across an average of 9.1 organ systems. The most frequent symptoms after month 6 were fatigue, post-exertional malaise, and cognitive dysfunction. Symptoms varied in their prevalence over time, and we identified three symptom clusters, each with a characteristic temporal profile. 85.9% of participants (95% CI, 84.8% to 87.0%) experienced relapses, primarily triggered by exercise, physical or mental activity, and stress. 86.7% (85.6% to 92.5%) of unrecovered respondents were experiencing fatigue at the time of survey, compared to 44.7% (38.5% to 50.5%) of recovered respondents. 1700 respondents (45.2%) required a reduced work schedule compared to pre-illness, and an additional 839 (22.3%) were not working at the time of survey due to illness. Cognitive dysfunction or memory issues were common across all age groups (~88%). Except for loss of smell and taste, the prevalence and trajectory of all symptoms were similar between groups with confirmed and suspected COVID-19. INTERPRETATION: Patients with Long COVID report prolonged, multisystem involvement and significant disability. By seven months, many patients have not yet recovered (mainly from systemic and neurological/cognitive symptoms), have not returned to previous levels of work, and continue to experience significant symptom burden. FUNDING: All authors contributed to this work in a voluntary capacity. The cost of survey hosting (on Qualtrics) and publication fee was covered by AA's research grant (Wellcome Trust/Gatsby Charity via Sainsbury Wellcome center, UCL).\", \"BACKGROUND: There is growing concern about possible cognitive consequences of COVID-19, with reports of âLong COVIDâ symptoms persisting into the chronic phase and case studies revealing neurological problems in severely affected patients. However, there is little information regarding the nature and broader prevalence of cognitive problems post-infection or across the full spread of disease severity. METHODS: We sought to confirm whether there was an association between cross-sectional cognitive performance data from 81,337 participants who between January and December 2020 undertook a clinically validated web-optimized assessment as part of the Great British Intelligence Test, and questionnaire items capturing self-report of suspected and confirmed COVID-19 infection and respiratory symptoms. FINDINGS: People who had recovered from COVID-19, including those no longer reporting symptoms, exhibited significant cognitive deficits versus controls when controlling for age, gender, education level, income, racial-ethnic group, pre-existing medical disorders, tiredness, depression and anxiety. The deficits were of substantial effect size for people who had been hospitalised (N = 192), but also for non-hospitalised cases who had biological confirmation of COVID-19 infection (N = 326). Analysing markers of premorbid intelligence did not support these differences being present prior to infection. Finer grained analysis of performance across sub-tests supported the hypothesis that COVID-19 has a multi-domain impact on human cognition. INTERPRETATION: Interpretation. These results accord with reports of âLong Covidâ cognitive symptoms that persist into the early-chronic phase. They should act as a clarion call for further research with longitudinal and neuroimaging cohorts to plot recovery trajectories and identify the biological basis of cognitive deficits in SARS-COV-2 survivors. FUNDING: Funding. AH is supported by the UK Dementia Research Institute Care Research and Technology Centre and Biomedical Research Centre at Imperial College London. WT is supported by the EPSRC Centre for Doctoral Training in Neurotechnology. SRC is funded by a Wellcome Trust Clinical Fellowship 110,049/Z/15/Z. JMB is supported by Medical Research Council (MR/N013700/1). MAM, SCRW and PJH are, in part, supported by the National Institute for Health Research (NIHR) Biomedical Research Centre at South London and Maudsley NHS Foundation Trust and King's College London\", 'COVID-19, caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has been often characterized as a respiratory disease. However, it is increasingly being understood as an infection that impacts multiple systems, and many patients report neurological symptoms. Indeed, there is accumulating evidence for neural damage in some individuals, with recent studies suggesting loss of gray matter in multiple regions, particularly in the left hemisphere. There are several mechanisms by which the COVID-19 infection may lead to neurological symptoms and structural and functional changes in the brain, and cognitive problems are one of the most commonly reported symptoms in those experiencing Long COVID â the chronic illness following the COVID-19 infection that affects between 10 and 25% of patients. However, there is yet little research testing cognition in Long COVID. The COVID and Cognition Study is a cross-sectional/longitudinal study aiming to understand cognitive problems in Long COVID. The first paper from the study explored the characteristics of our sample of 181 individuals who had experienced the COVID-19 infection, and 185 who had not, and the factors that predicted ongoing symptoms and self-reported cognitive deficits. In this second paper from the study, we assess this sample on tests of memory, language, and executive function. We hypothesize that performance on âobjectiveâ cognitive tests will reflect self-reported cognitive symptoms. We further hypothesize that some symptom profiles may be more predictive of cognitive performance than others, perhaps giving some information about the mechanism. We found a consistent pattern of memory deficits in those that had experienced the COVID-19 infection, with deficits increasing with the severity of self-reported ongoing symptoms. Fatigue/Mixed symptoms during the initial illness and ongoing neurological symptoms were predictive of cognitive performance.', 'BACKGROUND: With the ongoing COVID-19 pandemic, growing evidence shows that a considerable proportion of people who have recovered from COVID-19 have long-term effects on multiple organs and systems. A few longitudinal studies have reported on the persistent health effects of COVID-19, but the follow-up was limited to 1 year after acute infection. The aim of our study was to characterise the longitudinal evolution of health outcomes in hospital survivors with different initial disease severity throughout 2 years after acute COVID-19 infection and to determine their recovery status. METHODS: We did an ambidirectional, longitudinal cohort study of individuals who had survived hospitalisation with COVID-19 and who had been discharged from Jin Yin-tan Hospital (Wuhan, China) between Jan 7 and May 29, 2020. We measured health outcomes 6 months (June 16âSept 3, 2020), 12 months (Dec 16, 2020âFeb 7, 2021), and 2 years (Nov 16, 2021âJan 10, 2022) after symptom onset with a 6-min walking distance (6MWD) test, laboratory tests, and a series of questionnaires on symptoms, mental health, health-related quality of life (HRQoL), return to work, and health-care use after discharge. A subset of COVID-19 survivors received pulmonary function tests and chest imaging at each visit. Age-matched, sex-matched, and comorbidities-matched participants without COVID-19 infection (controls) were introduced to determine the recovery status of COVID-19 survivors at 2 years. The primary outcomes included symptoms, modified British Medical Research Council (mMRC) dyspnoea scale, HRQoL, 6MWD, and return to work, and were assessed in all COVID-19 survivors who attended all three follow-up visits. Symptoms, mMRC dyspnoea scale, and HRQoL were also assessed in controls. FINDINGS: 2469 patients with COVID-19 were discharged from Jin Yin-tan Hospital between Jan 7 and May 29, 2020. 1192 COVID-19 survivors completed assessments at the three follow-up visits and were included in the final analysis, 1119 (94%) of whom attended the face-to-face interview 2 years after infection. The median age at discharge was 57Â·0 years (48Â·0â65Â·0) and 551 (46%) were women. The median follow-up time after symptom onset was 185Â·0 days (IQR 175Â·0â197Â·0) for the visit at 6 months, 349Â·0 days (337Â·0â360Â·0) for the visit at 12 months, and 685Â·0 days (675Â·0â698Â·0) for the visit at 2 years. The proportion of COVID-19 survivors with at least one sequelae symptom decreased significantly from 777 (68%) of 1149 at 6 months to 650 (55%) of 1190 at 2 years (p<0Â·0001), with fatigue or muscle weakness always being the most frequent. The proportion of COVID-19 survivors with an mMRC score of at least 1 was 168 (14%) of 1191 at 2 years, significantly lower than the 288 (26%) of 1104 at 6 months (p<0Â·0001). HRQoL continued to improve in almost all domains, especially in terms of anxiety or depression: the proportion of individuals with symptoms of anxiety or depression decreased from 256 (23%) of 1105 at 6 months to 143 (12%) 1191 at 2 years (p<0Â·0001). The proportion of individuals with a 6MWD less than the lower limit of the normal range declined continuously in COVID-19 survivors overall and in the three subgroups of varying initial disease severity. 438 (89%) of 494 COVID-19 survivors had returned to their original work at 2 years. Survivors with long COVID symptoms at 2 years had lower HRQoL, worse exercise capacity, more mental health abnormality, and increased health-care use after discharge than survivors without long COVID symptoms. COVID-19 survivors still had more prevalent symptoms and more problems in pain or discomfort, as well as anxiety or depression, at 2 years than did controls. Additionally, a significantly higher proportion of survivors who had received higher-level respiratory support during hospitalisation had lung diffusion impairment (43 [65%] of 66 vs 24 [36%] of 66, p=0Â·0009), reduced residual volume (41 [62%] vs 13 [20%], p<0Â·0001), and total lung capacity (26 [39%] vs four [6%], p<0Â·0001) than did controls. INTERPRETATION: Regardless of initial disease severity, COVID-19 survivors had longitudinal improvements in physical and mental health, with most returning to their original work within 2 years; however, the burden of symptomatic sequelae remained fairly high. COVID-19 survivors had a remarkably lower health status than the general population at 2 years. The study findings indicate that there is an urgent need to explore the pathogenesis of long COVID and develop effective interventions to reduce the risk of long COVID.', 'OBJECTIVE: To assess FDG cerebral PET in patients suffering from cognitive impairment linked to Long COVID. The COVID pandemic has affected dozens of millions of people around the world and has resulted in the deaths of more than 3 million people. Following the acute forms, it has been reported sometimes long forms of COVID, with involvements of several organs including the brain. Neurological complications can include cognitive disturbances (brain fog) that are very common and can seriously disturb the life of patients. METHODS: Fluorodeoxyglucose PETs were performed in 3 patients with cognitive decline following COVID infection. RESULTS: We report here 3 cases of brain fog with major hypometabolic areas of the pons revealed by the cerebral FDG PET. CONCLUSION: The dysfunction of the locus coeruleus in these patients could partly explain the cognitive disorders observed. Further studies involving larger cohorts of patients suffering from cognitive dysfunction will be needed to determine if the brainstem is frequently affected in these patients.', 'Importance: Multi-organ impairment associated with Long COVID is a significant burden to individuals, populations and health systems, presenting challenges for diagnosis and care provision. Standardised assessment across multiple organs over time is lacking, particularly in non-hospitalised individuals. Objective: To determine the prevalence of organ impairment in Long COVID patients at 6 and at 12 months after initial symptoms and to explore links to clinical presentation. Design: This was a prospective, longitudinal study in individuals following recovery from acute COVID-19. We assessed symptoms, health status, and multi-organ tissue characterisation and function, using consensus definitions for single and multi-organ impairment. Physiological and biochemical investigations were performed at baseline on all individuals and those with organ impairment were reassessed, including multi-organ MRI, 6 months later. Setting: Two non-acute settings (Oxford and London). Participants: 536 individuals (mean 45 years, 73% female, 89% white, 32% healthcare workers, 13% acute COVID-19 hospitalisation) completed baseline assessment (median: 6 months post-COVID-19). 331 (62%) with organ impairment or incidental findings had follow up, with reduced symptom burden from baseline (median number of symptoms: 10 and 3, at 6 and 12 months). Exposure: SARS-CoV-2 infection 6 months prior to first assessment. Main outcome: Prevalence of single and multi-organ impairment at 6 and 12 months post-COVID-19. Results: Extreme breathlessness (36% and 30%), cognitive dysfunction (50% and 38%) and poor health-related quality of life (EQ-5D-5L<0.7; 55% and 45%) were common at 6 and 12 months, and associated with female gender, younger age and single organ impairment. At baseline, there was fibro-inflammation in the heart (9%), pancreas (9%), kidney (15%) and liver (11%); increased volume in liver (7%), spleen (8%) and kidney (9%); decreased capacity in lungs (2%); and excessive fat deposition in the liver (25%) and pancreas (15%). Single and multi-organ impairment were present in 59% and 23% at baseline, persisting in 59% and 27% at follow-up. Conclusion and Relevance: Organ impairment was present in 59% of individuals at 6 months post-COVID-19, persisting in 59% of those followed up at 1 year, with implications for symptoms, quality of life and longer-term health, signalling need for prevention and integrated care of Long COVID. Trial Registration: ClinicalTrials.gov NCT04369807', 'This cross-sectional study aimed to investigate the post-acute consequences of COVID-19. We conducted a self-administered questionnaire survey on sequelae, psychological distress (K6), impairments in work performance (WFun), and COVID-19ârelated experiences of stigma and discrimination in two designated COVID-19 hospitals in Hiroshima Prefecture, Japan, between August 2020 and March 2021. The prevalence of sequelae was calculated by age and COVID-19 severity. Factors independently associated with sequelae or psychological distress were identified using logistic regression analysis. Among 127 patients who had recovered from COVID-19, 52.0% had persistent symptoms at a median of 29 days [IQR 23â128] after COVID-19 onset. Among patients with mild COVID-19, 49.5% had sequelae. The most frequent symptoms were olfactory disorders (15.0%), taste disorders (14.2%), and cough (14.2%). Multivariate analysis showed that age was an independent risk factor for sequelae (adjusted odds ratios [AOR] for â¥ 60 years vs. < 40 years 3.63, p = 0.0165). Possible psychological distress was noted in 30.7% (17.9% of males and 45.0% of females). Female sex and the presence of sequelae were independent risk factors for psychological distress. Of all participants, 29.1% had possible impairments in work performance. Experiences of stigma and discrimination were reported by 43.3% of participants. This study revealed the significant impacts of Long COVID on health in local communities. A large-scale, long-term cohort study is desired.', 'Background: Covid-19 control policies have entailed lockdowns and confinement. Although these isolation measures are thought to be particularly hard and possibly harmful to people with dementia, their specific impact during the pandemic has not yet been synthesised. We aimed to examine and summarise the global research evidence describing the effect of Covid-19 isolation measures on the health of people living with dementia. Method: We searched Pubmed, PsycINFO and CINAHL up to February 2021 for peer-reviewed quantitative studies of the effects of isolation measures during Covid-19 on cognitive, psychological and functional symptoms of people with any kind of dementia or mild cognitive impairment. We summarised the findings of included papers following current guidelines for rapid reviews. Results: We identified 15 eligible papers, examining a total of 6,442 people with dementia. 13/15 were conducted in people living in the community and 2 in care homes. 60% (9/15) studies reported changes in cognition with 77% (7/9) of them describing declined cognition by >50% of respondents. 93% (14/15) of studies reported worsening or new onset of behavioural and psychological symptoms. 46% (7/15) studies reported changes in daily function, 6 of them reporting a functional decline in a variable proportion of the population studied. Conclusion: Lockdowns and confinement measures brought about by the pandemic have damaged the cognitive and psychological health and functional abilities of people with dementia across the world. It is urgent that infection control measures applied to people with dementia are balanced against the principles of non-maleficence. This systematic review makes 4 specific calls for action.', 'The present study aimed to examine the impact of COVID-19 social isolation upon aspects of emotional and social cognitive function. We predicted that greater impairments in emotional and social cognition would be observed in people who experienced more disruption to their usual social connectivity during COVID-19 social isolation. Healthy volunteers (N = 92) without prior mental health problems completed assessments online in their own homes during the most stringent period of the first COVID-19 \"lockdown\" in the UK (March - May 2020). Measures included two questionnaires probing levels of social isolation, anxiety levels, as well as five neuropsychological tasks assessing emotional and social cognition. Reduced positive bias in emotion recognition was related to reduced contact with friends, household size and communication method during social isolation. In addition, reduced positive bias for attention to emotional faces was related to frequency of contact with friends during social isolation. Greater cooperative behaviour in an ultimatum game was associated with more frequent contact with both friends and family during social isolation. The present study provides important insights into the detrimental effects of subjective and objective social isolation upon affective cognitive processes.', 'Many discharged COVID-19 patients affected by sequelae experience reduced quality of life leading to an increased burden on the healthcare system, their families and society at large. Possible pathophysiological mechanisms of long COVID include: persistent viral replication, chronic hypoxia and inflammation. Ongoing vascular endothelial damage promotes platelet adhesion and coagulation, resulting in the impairment of various organ functions. Meanwhile, thrombosis will further aggravate vasculitis contributing to further deterioration. Thus, long COVID is essentially a thrombotic sequela. Unfortunately, there is currently no effective treatment for long COVID. This article summarizes the evidence for coagulation abnormalities in long COVID, with a focus on the pathophysiological mechanisms of thrombosis. Extracellular vesicles (EVs) released by various types of cells can carry SARS-CoV-2 through the circulation and attack distant tissues and organs. Furthermore, EVs express tissue factor and phosphatidylserine (PS) which aggravate thrombosis. Given the persistence of the virus, chronic inflammation and endothelial damage are inevitable. Pulmonary structural changes such as hypertension, embolism and fibrosis are common in long COVID. The resulting impaired lung function and chronic hypoxia again aggravates vascular inflammation and coagulation abnormalities. In this article, we also summarize recent research on antithrombotic therapy in COVID-19. There is increasing evidence that early anticoagulation can be effective in improving outcomes. In fact, persistent systemic vascular inflammation and dysfunction caused by thrombosis are key factors driving various complications of long COVID. Early prophylactic anticoagulation can prevent the release of or remove procoagulant substances, thereby protecting the vascular endothelium from damage, reducing thrombotic sequelae, and improving quality of life for long-COVID patients.', 'BACKGROUND: The impact of COVID-19 on physical and mental health and employment after hospitalisation with acute disease is not well understood. The aim of this study was to determine the effects of COVID-19-related hospitalisation on health and employment, to identify factors associated with recovery, and to describe recovery phenotypes. METHODS: The Post-hospitalisation COVID-19 study (PHOSP-COVID) is a multicentre, long-term follow-up study of adults (aged â¥18 years) discharged from hospital in the UK with a clinical diagnosis of COVID-19, involving an assessment between 2 and 7 months after discharge, including detailed recording of symptoms, and physiological and biochemical testing. Multivariable logistic regression was done for the primary outcome of patient-perceived recovery, with age, sex, ethnicity, body-mass index, comorbidities, and severity of acute illness as covariates. A post-hoc cluster analysis of outcomes for breathlessness, fatigue, mental health, cognitive impairment, and physical performance was done using the clustering large applications k-medoids approach. The study is registered on the ISRCTN Registry (ISRCTN10980107). FINDINGS: We report findings for 1077 patients discharged from hospital between March 5 and Nov 30, 2020, who underwent assessment at a median of 5Â·9 months (IQR 4Â·9â6Â·5) after discharge. Participants had a mean age of 58 years (SD 13); 384 (36%) were female, 710 (69%) were of white ethnicity, 288 (27%) had received mechanical ventilation, and 540 (50%) had at least two comorbidities. At follow-up, only 239 (29%) of 830 participants felt fully recovered, 158 (20%) of 806 had a new disability (assessed by the Washington Group Short Set on Functioning), and 124 (19%) of 641 experienced a health-related change in occupation. Factors associated with not recovering were female sex, middle age (40â59 years), two or more comorbidities, and more severe acute illness. The magnitude of the persistent health burden was substantial but only weakly associated with the severity of acute illness. Four clusters were identified with different severities of mental and physical health impairment (n=767): very severe (131 patients, 17%), severe (159, 21%), moderate along with cognitive impairment (127, 17%), and mild (350, 46%). Of the outcomes used in the cluster analysis, all were closely related except for cognitive impairment. Three (3%) of 113 patients in the very severe cluster, nine (7%) of 129 in the severe cluster, 36 (36%) of 99 in the moderate cluster, and 114 (43%) of 267 in the mild cluster reported feeling fully recovered. Persistently elevated serum C-reactive protein was positively associated with cluster severity. INTERPRETATION: We identified factors related to not recovering after hospital admission with COVID-19 at 6 months after discharge (eg, female sex, middle age, two or more comorbidities, and more acute severe illness), and four different recovery phenotypes. The severity of physical and mental health impairments were closely related, whereas cognitive health impairments were independent. In clinical care, a proactive approach is needed across the acute severity spectrum, with interdisciplinary working, wide access to COVID-19 holistic clinical services, and the potential to stratify care. FUNDING: UK Research and Innovation and National Institute for Health Research.', 'Patients collectively made Long Covid â and cognate term âlong-haul Covidâ â in the first months of the pandemic. Patients, many with initially âmildâ illness, used various kinds of evidence and advocacy to demonstrate a longer, more complex course of illness than laid out in initial reports from Wuhan. Long Covid has a strong claim to be the first illness created through patients finding one another on social media: it moved from patients, through various media, to formal clinical and policy channels in just a few months. This initial mapping of Long Covid â by two patients with this illness â focuses on actors in the UK and USA and demonstrates how patients marshalled epistemic authority. Patient knowledge needs to be incorporated into how COVID-19 is conceptualised, researched, and treated.', 'COVID-19 has dominated our lives since the start of the pandemic in 2020, as well as greatly impacting dentistry, its patients and the dental profession. A new and potentially further problematic phenomenon is that of long COVID, a term used to describe the effects of COVID-19 that continue for weeks, or even months, beyond the initial illness. It is characterised by debilitating symptoms including extreme fatigue, shortness of breath, insomnia, heart palpitations and prolonged high temperature. With one in ten people in the UK suffering from long COVID, there will undoubtedly be a considerable impact on dentistry provision; there will be ramifications not only for patients, but also the workforce, both physically and mentally. The aim of this article is to explore the obstacles we will face due to long COVID, examining possible challenges but also possible solutions.', 'Background: Severe acute respiratory syndrome-coronavirus 2 (SARS-CoV-2) infection has disproportionately affected older individuals and those with underlying medical conditions. Research has focused on short-term outcomes in hospital, and single organ involvement. Consequently, impact of long COVID (persistent symptoms three months post-infection) across multiple organs in low-risk individuals is yet to be assessed. Methods: An ongoing prospective, longitudinal, two-centre, observational study was performed in individuals symptomatic after recovery from acute SARS-CoV-2 infection. Symptoms and organ function (heart, lungs, kidneys, liver, pancreas, spleen) were assessed by standardised questionnaires (EQ-5D-5L, Dyspnoea-12), blood investigations and quantitative magnetic resonance imaging, defining single and multi-organ impairment by consensus definitions. Findings: Between April and September 2020, 201 individuals (mean age 44 (SD 11.0) years, 70% female, 87% white, 31% healthcare workers) completed assessments following SARS-CoV-2 infection (median 140, IQR 105-160 days after initial symptoms). The prevalence of pre-existing conditions (obesity: 20%, hypertension: 6%; diabetes: 2%; heart disease: 4%) was low, and only 18% of individuals had been hospitalised with COVID-19. Fatigue (98%), muscle aches (88%), breathlessness (87%), and headaches (83%) were the most frequently reported symptoms. Ongoing cardiorespiratory (92%) and gastrointestinal (73%) symptoms were common, and 42% of individuals had ten or more symptoms. There was evidence of mild organ impairment in heart (32%), lungs (33%), kidneys (12%), liver (10%), pancreas (17%), and spleen (6%). Single (66%) and multi-organ (25%) impairment was observed, and was significantly associated with risk of prior COVID-19 hospitalisation (p<0.05). Interpretation: In a young, low-risk population with ongoing symptoms, almost 70% of individuals have impairment in one or more organs four months after initial symptoms of SARS-CoV-2 infection. There are implications not only for burden of long COVID but also public health approaches which have assumed low risk in young people with no comorbidities.', \"BACKGROUND: The coronavirus disease (COVIDâ19) pandemic has had farâreaching effects upon lives, healthcare systems and society. Some who had an apparently 'mild' COVIDâ19 infection continue to suffer from persistent symptoms, including chest pain, breathlessness, fatigue, cognitive impairment, paraesthesia, muscle and joint pains. This has been labelled 'long COVID'. This paper reports the experiences of doctors with long COVID. METHODS: A qualitative study; interviews with doctors experiencing persistent symptoms were conducted by telephone or video call. Interviews were transcribed and analysis conducted using an inductive and thematic approach. RESULTS: Thirteen doctors participated. The following themes are reported: making sense of symptoms, feeling let down, using medical knowledge and connections, wanting to help and be helped, combining patient and professional identity. Experiencing long COVID can be transformative: many expressed hope that good would come of their experiences. Distress related to feelings of being âlet downâ and the hard work of trying to access care. Participants highlighted that they felt better able to care for, and empathize with, patients with chronic conditions, particularly where symptoms are unexplained. CONCLUSIONS: The study adds to the literature on the experiences of doctors as patients, in particular where evidence is emerging and the patient has to take the lead in finding solutions to their problems and accessing their own care. PATIENT AND PUBLIC CONTRIBUTION: The study was developed with experts by experience (including coâauthors HA and TAB) who contributed to the protocol and ethics application, and commented on analysis and implications. All participants were given the opportunity to comment on findings.\", \"The socio-economic impact of diseases associated with cognitive impairment is increasing. According to the Alzheimer's Society there are over 850,000 people with dementia in the UK, costing the UK Â£26 billion in 2013. Therefore, research into treatment of those conditions is vital. Research into the cerebral endothelial glycocalyx (CeGC) could offer effective treatments. The CeGC, consisting of proteoglycans, glycoproteins and glycolipids, is a dynamic structure covering the luminal side oftheendothelial cells of capillaries throughout the body. The CeGC is thicker in cerebral micro vessels, suggesting specialisation for its function as part of the blood-brain barrier (BBB). Recent research evidences that the CeGC is vital in protecting fragile parenchymal tissue and effective functioning of the BBB, as one particularly important CeGC function is to act as a protective barrier and permeability regulator. CeGC degradation is one of the factors which can lead to an increase in BBB permeability. It occurs naturally in aging, nevertheless, premature degradationhas beenevidencedin multipleconditions linked to cognitive impairment, such as inflammation,brain edema, cerebral malaria, Alzheimer's and recently Covid-19. Increasing knowledge of the mechanisms of CeGC damage has led to research into preventative techniques showing that CeGC is a possible diagnostic marker and a therapeutic target. However, the evidence is relatively new, inconsistent and demonstrated mainly in experimental models. This review evaluates the current knowledge of the CeGC, its structure, functions, damage and repair mechanisms and the impact of its degeneration on cognitive impairment in multiple conditions, highlighting the CeGC as a possible diagnostic marker and a potential target for therapeutic treatment.\", 'Abstract Background Demographic and infection-related characteristics have been identified as risk factors for long COVID, but research on the influence of health behaviours (e.g., exercise, smoking) immediately preceding the index infection is lacking. Methods 1,811 UK adults from the UCL COVID-19 Social Study and who had previously been infected with COVID-19 were analysed. Health behaviours in the month before infection were weekly exercise frequency, days of fresh air per week, sleep quality, smoking, consuming more than the number of recommended alcoholic drinks per week (>14), and the number of mental health care behaviours (e.g., online mental health programme). Logistic regressions controlling for covariates (e.g., COVID-19 infection severity and pre-existing health conditions) examined the impact of health behaviours on long COVID and three long COVID symptoms (difficulty with mobility, cognition, and self-care). Results In the month before infection with COVID-19, poor quality sleep increased the odds of long COVID (odds ratio [OR]: 3.53; (95% confidence interval [CI]: 2.01 to 6.21), as did average quality sleep (OR: 2.44; 95% CI: 1.44 to 4.12). Having smoked (OR: 8.39; 95% CI: 1.86 to 37.91) increased and meeting recommended weekly physical activity guidelines (3+ hours) (OR: 0.05; 95% CI: 0.01 to 0.39) reduced the likelihood of difficulty with self-care (e.g., washing all over or dressing) amongst those with long COVID. Conclusion Results point to the importance of sleep quality for long COVID, potentially helping to explain previously demonstrated links between stress and long COVID. Results also suggest that exercise and smoking may be modifiable risk factors for preventing the development of difficulty with self-care. Funding The Nuffield Foundation [WEL/FR-000022583], the MARCH Mental Health Network funded by the Cross-Disciplinary Mental Health Network Plus initiative supported by UK Research and Innovation [ES/S002588/1], and the Wellcome Trust [221400/Z/20/Z and 205407/Z/16/Z].', 'OBJECTIVE To determine the comparative effectiveness and safety of psychological interventions for chronic low back pain. DESIGN Systematic review with network meta-analysis. DATA SOURCES Medline, Embase, PsycINFO, Cochrane Central Register of Controlled Trials, Web of Science, SCOPUS, and CINAHL from database inception to 31 January 2021. ELIGIBILITY CRITERIA FOR STUDY SELECTION Randomised controlled trials comparing psychological interventions with any comparison intervention in adults with chronic, non-specific low back pain. Two reviewers independently screened studies, extracted data, and assessed risk of bias and confidence in the evidence. Primary outcomes were physical function and pain intensity. A random effects network meta-analysis using a frequentist approach was performed at post-intervention (from the end of treatment to <2 months post-intervention); and at short term (â¥2 to <6 months post-intervention), mid-term (â¥6 to <12 months post-intervention), and long term follow-up (â¥12 months post-intervention). Physiotherapy care was the reference comparison intervention. The design-by-treatment interaction model was used to assess global inconsistency and the Bucher method was used to assess local inconsistency. RESULTS 97 randomised controlled trials involving 13 136 participants and 17 treatment nodes were included. Inconsistency was detected at short term and mid-term follow-up for physical function, and short term follow-up for pain intensity, and were resolved through sensitivity analyses. For physical function, cognitive behavioural therapy (standardised mean difference 1.01, 95% confidence interval 0.58 to 1.44), and pain education (0.62, 0.08 to 1.17), delivered with physiotherapy care, resulted in clinically important improvements at post-intervention (moderate quality evidence). The most sustainable effects of treatment for improving physical function were reported with pain education delivered with physiotherapy care, at least until mid-term follow-up (0.63, 0.25 to 1.00; low quality evidence). No studies investigated the long term effectiveness of pain education delivered with physiotherapy care. For pain intensity, behavioural therapy (1.08, 0.22 to 1.94), cognitive behavioural therapy (0.92, 0.43 to 1.42), and pain education (0.91, 0.37 to 1.45), delivered with physiotherapy care, resulted in clinically important effects at post-intervention (low to moderate quality evidence). Only behavioural therapy delivered with physiotherapy care maintained clinically important effects on reducing pain intensity until mid-term follow-up (1.01, 0.41 to 1.60; high quality evidence). CONCLUSIONS For people with chronic, non-specific low back pain, psychological interventions are most effective when delivered in conjunction with physiotherapy care (mainly structured exercise). Pain education programmes (low to moderate quality evidence) and behavioural therapy (low to high quality evidence) result in the most sustainable effects of treatment; however, uncertainty remains as to their long term effectiveness. Although inconsistency was detected, potential sources were identified and resolved. SYSTEMATIC REVIEW REGISTRATION PROSPERO CRD42019138074.', 'BACKGROUND: Long Covid is a public health concern that needs defining, quantifying, and describing. We aimed to explore the initial and ongoing symptoms of Long Covid following SARS-CoV-2 infection and describe its impact on daily life. METHODS: We collected self-reported data through an online survey using convenience non-probability sampling. The survey enrolled adults who reported lab-confirmed (PCR or antibody) or suspected COVID-19 who were not hospitalised in the first two weeks of illness. This analysis was restricted to those with self-reported Long Covid. Univariate comparisons between those with and without confirmed COVID-19 infection were carried out and agglomerative hierarchical clustering was used to identify specific symptom clusters, and their demographic and functional correlates. RESULTS: We analysed data from 2550 participants with a median duration of illness of 7.6 months (interquartile range (IQR) 7.1â7.9). 26.5% reported lab-confirmation of infection. The mean age was 46.5 years (standard deviation 11 years) with 82.8% females and 79.9% of participants based in the UK. 89.5% described their health as good, very good or excellent before COVID-19. The most common initial symptoms that persisted were exhaustion, chest pressure/tightness, shortness of breath and headache. Cognitive dysfunction and palpitations became more prevalent later in the illness. Most participants described fluctuating (57.7%) or relapsing symptoms (17.6%). Physical activity, stress, and sleep disturbance commonly triggered symptoms. A third (32%) reported they were unable to live alone without any assistance at six weeks from start of illness. 16.9% reported being unable to work solely due to COVID-19 illness. 37.0% reported loss of income due to illness, and 64.4% said they were unable to perform usual activities/duties. Acute systems clustered broadly into two groups: a majority cluster (n = 2235, 88%) with cardiopulmonary predominant symptoms, and a minority cluster (n = 305, 12%) with multisystem symptoms. Similarly, ongoing symptoms broadly clustered in two groups; a majority cluster (n = 2243, 88.8%) exhibiting mainly cardiopulmonary, cognitive symptoms and exhaustion, and a minority cluster (n = 283, 11.2%) exhibiting more multisystem symptoms. Belonging to the more severe multisystem cluster was associated with more severe functional impact, lower income, younger age, being female, worse baseline health, and inadequate rest in the first two weeks of the illness, with no major differences in the cluster patterns when restricting analysis to the lab-confirmed subgroup. CONCLUSION: This is an exploratory survey of Long Covid characteristics. Whilst this is a non-representative population sample, it highlights the heterogeneity of persistent symptoms, and the significant functional impact of prolonged illness following confirmed or suspected SARS-CoV-2 infection. To study prevalence, predictors and prognosis, research is needed in a representative population sample using standardised case definitions.', 'OBJECTIVE: Most SARSâCoVâ2âinfected individuals never require hospitalization. However, some develop prolonged symptoms. We sought to characterize the spectrum of neurologic manifestations in nonâhospitalized Covidâ19 âlong haulersâ. METHODS: This is a prospective study of the first 100 consecutive patients (50 SARSâCoVâ2 laboratoryâpositive (SARSâCoVâ2(+)) and 50 laboratoryânegative (SARSâCoVâ2(â)) individuals) presenting to our NeuroâCovidâ19 clinic between May and November 2020. Due to early pandemic testing limitations, patients were included if they met Infectious Diseases Society of America symptoms of Covidâ19, were never hospitalized for pneumonia or hypoxemia, and had neurologic symptoms lasting over 6 weeks. We recorded the frequency of neurologic symptoms and analyzed patientâreported quality of life measures and standardized cognitive assessments. RESULTS: Mean age was 43.2 Â± 11.3 years, 70% were female, and 48% were evaluated in televisits. The most frequent comorbidities were depression/anxiety (42%) and autoimmune disease (16%). The main neurologic manifestations were: âbrain fogâ (81%), headache (68%), numbness/tingling (60%), dysgeusia (59%), anosmia (55%), and myalgias (55%), with only anosmia being more frequent in SARSâCoVâ2(+) than SARSâCoVâ2(â) patients (37/50 [74%] vs. 18/50 [36%]; p < 0.001). Moreover, 85% also experienced fatigue. There was no correlation between time from disease onset and subjective impression of recovery. Both groups exhibited impaired quality of life in cognitive and fatigue domains. SARSâCoVâ2(+) patients performed worse in attention and working memory cognitive tasks compared to a demographicâmatched US population (Tâscore 41.5 [37, 48.25] and 43 [37.5, 48.75], respectively; both p < 0.01). INTERPRETATION: Nonâhospitalized Covidâ19 âlong haulersâ experience prominent and persistent âbrain fogâ and fatigue that affect their cognition and quality of life.', 'INTRODUCTION: As the prevalence of Long COVID increases, there is a critical need for a comprehensive assessment of disability. Our aims are to: (1) characterise disability experiences among people living with Long COVID in Canada, UK, USA and Ireland; and (2) develop a patient-reported outcome measure to assess the presence, severity and episodic nature of disability with Long COVID. METHODS AND ANALYSIS: In phase 1, we will conduct semistructured interviews with adults living with Long COVID to explore experiences of disability (dimensions, uncertainty, trajectories, influencing contextual factors) and establish an episodic disability (ED) framework in the context of Long COVID (n~10 each country). Using the conceptual framework, we will establish the Long COVID Episodic Disability Questionnaire (EDQ). In phase 2, we will examine the validity (construct, structural) and reliability (internal consistency, testâretest) of the EDQ for use in Long COVID. We will electronically administer the EDQ and four health status criterion measures with adults living with Long COVID, and readminister the EDQ 1 week later (n~170 each country). We will use Rasch analysis to refine the EDQ, and confirm structural and cross-cultural validity. We will calculate Cronbachâs alphas (internal consistency reliability), and intraclass correlation coefficients (testâretest reliability), and examine correlations for hypotheses theorising relationships between EDQ and criterion measure scores (construct validity). Using phase 2 data, we will characterise the profile of disability using structural equation modelling techniques to examine relationships between dimensions of disability and the influence of intrinsic and extrinsic contextual factors. This research involves an academicâclinicalâcommunity partnership building on foundational work in ED measurement, Long COVID and rehabilitation. ETHICS AND DISSEMINATION: This study was approved by the University of Toronto Research Ethics Board. Knowledge translation will occur with community collaborators in the form of presentations and publications in open access peer-reviewed journals and presentations.', 'Although coronavirus disease 2019 (COVID-19) affects the respiratory system, it can also have neurological consequences leading to cognitive deficits such as memory problems. The aim of our study was to assess the impact of COVID-19 on working memory function. We developed and implemented an online anonymous survey with a working memory quiz incorporating aspects of gamification to engage participants. 5428 participants successfully completed the survey and memory quiz between 8 th December 2020 and 5 th July 2021 (68.6% non-COVID-19 and 31.4% COVID-19). Most participants (93.3%) completed the survey and memory quiz relatively rapidly (mean time of 8.84 minutes). Categorical regression was used to assess the contribution of COVID status, age, time post-COVID (number of months elapsed since having had COVID), symptoms, ongoing symptoms and gender, followed by non-parametric statistics. A principal component analysis explored the relationship between subjective ratings and objective memory scores. The objective memory scores were significantly correlated with participants own assessment of their cognitive function. The factors significantly affecting memory scores were COVID status, age, time post-COVID and ongoing symptoms. Our main finding was a significant reduction in memory scores in all COVID groups (self-reported, positive-tested and hospitalised) compared to the non-COVID group. Memory scores for all COVID groups combined were significantly reduced compared to the non-COVID group in every age category 25 years and over, but not for the youngest age category (18-24 years old). We found that memory scores gradually increased over a period of 17 months post-COVID-19. However, those with ongoing COVID-19 symptoms continued to show a reduction in memory scores. Our findings demonstrate that COVID-19 negatively impacts working memory function, but only in adults aged 25 years and over. Moreover, our results suggest that working memory deficits with COVID-19 can recover over time, although impairments may persist in those with ongoing symptoms.', 'Following the outbreak of COVID-19, multidisciplinary research focusing on the long-term effects of the COVID-19 infection and the complete recovery is still scarce. With regards to long-term consequences, biomarkers of physiological effects as well as the psychological experiences are of significant importance for comprehensively understanding the complete COVID-19 recovery. The present research surveys the IgG antibody titers and the impact of COVID-19 as a traumatic experience in the aftermath of the active infection period, around 2 months after diagnosis, in a subset of COVID-19 patients from the first wave (March-April 2020) of the outbreak in Northern Cyprus. Associations of antibody titers and psychological survey measures with baseline characteristics and disease severity were explored, and correlations among various measures were evaluated. Of the 47 serology tests conducted for presence of IgG antibodies, 39 (83%) were positive. We identified trends demonstrating individuals experiencing severe or critical COVID-19 disease and/or those with comorbidities are more heavily impacted both physiologically and mentally, with higher IgG titers and negative psychological experience compared to those with milder disease and without comorbidities. We also observed that more than half of the COVID-19 cases had negative psychological experiences, being subjected to discrimination and verbal harassment/insult, by family/friends. In summary, as the first study co-evaluating immune response together with mental status in COVID-19, our findings suggest that further multidisciplinary research in larger sample populations as well as community intervention plans are needed to holistically address the physiological and psychological effects of COVID-19 among the cases.', 'The patient-made term âLong Covidâ is, we argue, a helpful and capacious term that is needed to address key medical, epidemiological and socio-political challenges posed by diverse symptoms persisting beyond four weeks after symptom onset suggestive of coronavirus disease 2019 (COVID-19). An international movement of patients (which includes all six authors) brought the persistence and heterogeneity of long-term symptoms to widespread visibility. The same grassroots movement introduced the term âLong Covidâ (and the cognate term âlong-haulersâ) to intervene in relation to widespread assumptions about disease severity and duration. Persistent symptoms following severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection are now one of the most pressing clinical and public health phenomena to address: their cause(s) is/are unknown, their effects can be debilitating, and the percentage of patients affected is unclear, though likely significant. The term âLong Covidâ is now used in scientific literature, the media, and in interactions with the WHO. Uncertainty regarding its value and meaning, however, remains. In this Open Letter, we explain the advantages of the term âLong Covidâ and bring clarity to some pressing issues of use and definition. We also point to the importance of centring patient experience and expertise in relation to âLong Covidâ research, as well as the provision of care and rehabilitation.', 'While the increased arrhythmic tendency during acute COVID-19 infection is recognised, the long-term cardiac electrophysiological complications are less well known. There are a high number of patients reporting ongoing symptoms post-infection, termed long COVID. A recent hypothesis is that long COVID symptoms could be attributed to dysautonomia, defined as malfunction of the autonomic nervous system (ANS). The most prevalent cardiovascular dysautonomia amongst young people is postural orthostatic tachycardia syndrome (POTS). Numerous reports have described the development of POTS as part of long COVID. Possible underlying mechanisms, although not mutually exclusive or exhaustive, include hypovolaemia, neurotropism, inflammation and autoimmunity. Treatment options for POTS and other long COVID symptoms are currently limited. Future research studies should aim to elucidate the underlying mechanisms of dysautonomia to enable the development of targeted therapies. Furthermore, it is important to educate healthcare professionals to recognise complications and conditions arising from COVID-19, such as POTS, to allow prompt diagnosis and access to early treatment.', 'Objectives: We examined whether providing different types of information about Long COVID would affect expectations about the illness. Design: A 2 (Illness description: Long COVID vs ongoing COVID-19 recovery) x 2 (Illness uncertainty: uncertainty emphasised vs uncertainty not emphasised) x 2 (Efficacy of support: enhanced support vs basic support) between-subjects randomised online experimental study. Setting: The online platform Prolific, collected in October 2021. Participants: A representative sample of 1110 members of the public in the UK. Interventions: Participants were presented with a scenario describing a positive COVID-19 test result and then presented with one of eight scenarios describing a Long COVID diagnosis. Primary and Secondary Outcome Measures: Various outcome measures relating to illness expectations were captured including: symptom severity, symptom duration, quality of life, personal control, treatment control and illness coherence. Results: We ran a series of 2 x 2 x 2 ANOVAs on the outcome variables. We found a main effect of illness description: individuals reported longer symptom duration and less illness coherence when the illness was described as Long COVID (compared to ongoing COVID-19 recovery). There was a main effect of illness uncertainty: when uncertainty was emphasised, participants reported longer expected symptom duration, less treatment control, and less illness coherence than when uncertainty was not emphasised. There was also a main effect of efficacy of support: participants reported higher personal control and higher treatment control when support was enhanced (compared to basic support). We also found an interaction between illness description and efficacy of support: when support was enhanced, participants reported less illness coherence for Long COVID (compared to ongoing COVID-19 recovery). Conclusions: Communications around Long COVID should not emphasise symptom uncertainty and should provide people with information on how they can facilitate their recovery and where they can access additional support. The findings also suggest that use of the term ongoing COVID-19 recovery, where possible, may reduce negative expectations associated with the illness.', 'Many people are not recovering for months after being infected with SARS-CoV-2. Long Covid has emerged as a major public health concern that needs defining, quantifying, and describing. We aimed to explore the initial and ongoing symptoms of Long Covid following SARS-CoV-2 infection and describe its impact on daily life in people who were not admitted to hospital during the first two weeks of the illness. We co-produced a survey with people living with Long Covid. We collected the data through an online survey using convenience non-probability sampling, with the survey posted both specifically on Long Covid support groups and generally on social media. The criteria for inclusion were adults with lab-confirmed (PCR or antibody) or suspected COVID-19 managed in the community (non-hospitalised) in the first two weeks of illness. We used agglomerative hierarchical clustering to identify specific symptom clusters, and their demographic and functional correlates. We analysed data from 2550 participants with a median duration of illness of 7.7 months (interquartile range (IQR) 7.4-8.0). The mean age was 46.5 years (standard deviation 11 years) with 82.8% females and 79.9% of participants based in the UK. 89.5% described their health as good, very good or excellent before COVID-19. The most common initial symptoms that persisted were exhaustion, chest pressure/tightness, shortness of breath and headache. Cough, fever, and chills were common initial symptoms that became less prevalent later in the illness, whereas cognitive dysfunction and palpitations became more prevalent later in the illness. 26.5% reported lab-confirmation of infection. The biggest difference in ongoing symptoms between those who reported testing positive and those who did not was loss of smell/taste. Ongoing symptoms affected at least 3 organ systems in 83.5% of participants. Most participants described fluctuating (57.7%) or relapsing symptoms (17.6%). Physical activity, stress and sleep disturbance commonly triggered symptoms. A third (32%) reported they were unable to live alone without any assistance at six weeks from start of illness. 16.9% reported being unable to work solely due to COVID-19 illness. 66.4% reported taking time off sick (median of 60 days, IQR 20, 129). 37.0% reported loss of income due to illness, and 64.4% said they were unable to perform usual activities/duties. Acute systems clustered broadly into two groups: a majority cluster (n=2235, 88%) with cardiopulmonary predominant symptoms, and a minority cluster (n=305, 12%) with multisystem symptoms. Similarly, ongoing symptoms broadly clustered in two groups; a majority cluster (n=2243, 88.8%) exhibiting mainly cardiopulmonary, cognitive symptoms and exhaustion, and a minority cluster (n=283, 11.2%) exhibited more multisystem symptoms. Belonging to the more severe multisystem cluster was associated with more severe functional impact, lower income, younger age, being female, worse baseline health, and inadequate rest in the first two weeks of the illness, with no major differences in the cluster patterns when restricting analysis to the lab-confirmed subgroup. This is an exploratory survey of Long Covid characteristics. Whilst it is important to acknowledge that it is a non-representative population sample, it highlights the heterogeneity of persistent symptoms, and the significant functional impact of prolonged illness following confirmed or suspected SARS-CoV-2 infection. To study prevalence, predictors and prognosis, research is needed in a representative population sample using standardised case definitions (to include those not lab-confirmed in the first pandemic wave).', 'Introduction: Patients with COVID-19 usually recover and return to normal health, however some patients may have symptoms that last for weeks or even months after recovery. This persistent state of ill health is known as Long COVID if it continues for more than 12 weeks and are not explained by an alternative diagnosis. Long Covid has been overlooked in low and middle income countries. Therefore, we conducted an online survey among the COVID-19 survivors in the community to explore their Long COVID symptoms, factors associated with Long COVID and how Long COVID affected their work. Methods: This was a cross sectional study conducted from July to September 2021, during the implementation of a nationwide movement control order (MCO). Data was collected using the REDCap electronic data capture tool. The questionnaire was distributed in social and news media. The questionnaire covers information such as socio-demographic characteristics, existing comorbidities, self-perception on health, information on the acute COVID-19 condition and treatment received, symptoms and duration of post-COVID condition and effects on occupation. Results: A total of 732 COVID-19 survivors responded. There were slightly more females (58.7%), younger and more highly educated respondents. More than half of them were overweight or obese and about two third were free of comorbidities. Among these respondents, about 56% were without or with mild symptoms during their acute COVID-19 conditions. A total of 21.1% of the respondents reported to experience Long COVID. The most commonly reported symptoms for Long COVID were fatigue, brain fog, depression, anxiety, insomnia, arthralgia or myalgia. Females had 58% higher odds (95% CI: 1.02, 2.45) of experiencing Long COVID. Patients with moderate and severe levels of acute COVID-19 symptoms had OR of 3.01 (95% CI: 1.21, 7.47) and 3.62 (95% CI: 1.31, 10.03) respectively for Long COVID. Conclusion: This study provides additional insight on the symptoms and duration of post-COVID symptoms as well as the associated factors with Long COVID among COVID-19 survivors in Malaysia. Recognition of Long COVID and its associated factors is important in planning prevention, rehabilitation, clinical management to improve recovery and long-term COVID-19 outcomes.', 'Introduction: We systematically reviewed studies to estimate the risk of SARS-CoV-2 reinfection among those previously infected with SARS-CoV-2. Methods: For this systematic review, we searched scientific publications on PubMed and, the pre-print server, MedRxiv through August 18, 2021. Eligible studies were retrieved on August 18, 2021. We used the following search term on PubMed: (((Cohort Studies [Majr]) AND (COVID-19 [Mesh] OR SARS-CoV-2 [Mesh])) OR Reinfection [Majr]) OR Reinfection [Mesh]. We used the following search term on MedRxiv: Cohort Studies AND COVID-19 OR SARS-CoV-2 AND Reinfection. The search terms were broad to encompass all possibilities for applicable studies. There were no restrictions on the date of publication. Studies that did not describe cohorts with estimates of the risk of SARS-CoV-2 reinfection among those with previous infection were excluded. Studies that included vaccinated participants were either excluded or limited to sub-groups of non-vaccinated individuals. To identify relevant studies with appropriate control groups, we developed the following criteria for studies to be included in the systematic analysis: (1) baseline polymerase chain reaction (PCR) testing, (2) a negative comparison group, (3) longitudinal follow-up, (4) a cohort of human participants, i.e., not a case report or case series, and (5) outcome determined by PCR. The review was conducted following PRISMA guidelines. We assessed for selection, information, and analysis bias, per PRISMA guidelines. Results: We identified 1,392 reports. Of those, 10 studies were eligible for our systematic review. The weighted average risk reduction against reinfection was 90.4% with a standard deviation of 7.7%. Protection against SARS-CoV-2 reinfection was observed for up to 10 months. Studies had potential information, selection, and analysis biases. Conclusions: The protective effect of prior SARS-CoV-2 infection on re-infection is high and similar to the protective effect of vaccination. More research is needed to characterize the duration of protection and the impact of different SARS-CoV-2 variants.', 'Although several clinical manifestations of persistent long coronavirus disease (COVID-19) have been documented, their effects on the cardiovascular and autonomic nervous system over the long term remain unclear. Thus, we examined the presence of alterations in cardiac autonomic functioning in individuals with long-term manifestations. The study was conducted from October 2020 to May 2021, and an autonomic assessment was performed to collect heart rate data for the heart rate variability (HRV) analysis. The study participants were divided into the long COVID clinical group, the intragroup, which included patients who were hospitalized, and those who were not hospitalized and were symptomatic for different periods (â¤3, >3, â¤6, and >6 months), with and without dyspnoea. The control group, the intergroup, comprised of COVID-free individuals. Our results demonstrated that the long COVID clinical group showed reduced HRV compared with the COVID-19-uninfected control group. Patients aged 23â59 years developed COVID symptoms within 30 days after infection, whose diagnosis was confirmed by serologic or reverse-transcription polymerase chain reaction (swab) tests, were included in the study. A total of 155 patients with long COVID [95 women (61.29%), mean age 43.88 Â± 10.88 years and 60 men (38.71%), mean age 43.93 Â± 10.11 years] and 94 controls [61 women (64.89%), mean age 40.83 Â± 6.31 and 33 men (35.11%), mean age 40.69 Â± 6.35 years] were included. The intragroup and intergroup comparisons revealed a reduction in global HRV, increased sympathetic modulation influence, and a decrease in parasympathetic modulation in long COVID. The intragroup showed normal sympathovagal balance, while the intergroup showed reduced sympathovagal balance. Our findings indicate that long COVID leads to sympathetic excitation influence and parasympathetic reduction. The excitation can increase the heart rate and blood pressure and predispose to cardiovascular complications. Short-term HRV analysis showed good reproducibility to verify the cardiac autonomic involvement.', 'A proportion of patients surviving acute COVID-19 infection develop post-COVID syndrome (long COVID) encompassing physical and neuropsychiatric symptoms lasting longer than 12 weeks. Here we studied a prospective cohort of individuals with long COVID (the ADAPT study) compared to age/gender matched subjects without long COVID, healthy donors and individuals infected with other non-SARS CoV2 human coronaviruses (the ADAPT-C study). We found an elevated diffuse serum inflammatory cytokine profile in symptomatic long COVID subjects that was maintained at 8 months post-infection and was not observed in asymptomatic COVID-19 survivors. This inflammatory profile consisted of 15 cytokines that positively correlated; revealing an apparent diffuse, potentially coordinated, low level up regulation of a spectrum of immune and inflammatory mediators. In addition, we found an absence of subsets of un-activated naive T and B cells in peripheral blood of long COVID subjects, that did not reconstitute over time. In contrast, individual serum cytokines from the interferon I and III classes, T cell activation markers and plasma ACE2, while elevated in the serum of people previously infected with SARS-CoV-2 were not further elevated in subjects with long COVID symptoms. This work defines immunological parameters associated with long COVID and suggests future opportunities to prevention and treatment.', 'Background and Objectives: Long COVID defines a series of chronic symptoms that patients may experience after resolution of acute COVID-19. Early reports from studies with patients with long COVID suggests a constellation of symptoms with similarities to another chronic medical illnessâmyalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS). A review study comparing and contrasting ME/CFS with reported symptoms of long COVID may yield mutualistic insight into the characterization and management of both conditions. Materials and Methods: A systemic literature search was conducted in MEDLINE and PsycInfo through to 31 January 2021 for studies related to long COVID symptomatology. The literature search was conducted in accordance with PRISMA methodology. Results: Twenty-one studies were included in the qualitative analysis. Long COVID symptoms reported by the included studies were compared to a list of ME/CFS symptoms compiled from multiple case definitions. Twenty-five out of 29 known ME/CFS symptoms were reported by at least one selected long COVID study. Conclusions: Early studies into long COVID symptomatology suggest many overlaps with clinical presentation of ME/CFS. The need for monitoring and treatment for patients post-COVID is evident. Advancements and standardization of long COVID research methodologies would improve the quality of future research, and may allow further investigations into the similarities and differences between long COVID and ME/CFS.', 'In children, the risk of coronavirus disease (COVID) being severe is low. However, the risk of persistent symptoms following infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is uncertain in this age group, and the features of âlong COVIDâ are poorly characterized. We reviewed the 14 studies to date that have reported persistent symptoms following COVID in children and adolescents. Almost all the studies have major limitations, including the lack of a clear case definition, variable follow-up times, inclusion of children without confirmation of SARS-CoV-2 infection, reliance on self- or parent-reported symptoms without clinical assessment, nonresponse and other biases, and the absence of a control group. Of the 5 studies which included children and adolescents without SARS-CoV-2 infection as controls, 2 did not find persistent symptoms to be more prevalent in children and adolescents with evidence of SARS-CoV-2 infection. This highlights that long-term SARS-CoV-2 infectionâassociated symptoms are difficult to distinguish from pandemic-associated symptoms.', 'AIM: Persistent symptoms in adults after COVIDâ19 are emerging and the term long COVID is increasingly appearing in the literature. However, paediatric data are scarce. METHODS: This paper contains a case report of five Swedish children and the longâterm symptoms reported by their parents. It also includes a systematic literature review of the MEDLINE, EMBASE and Web of Science databases and the medRxiv/bioRxiv preâprint servers up to 2 November 2020. RESULTS: The five children with potential long COVID had a median age of 12 years (range 9â15) and four were girls. They had symptoms for 6â8 months after their clinical diagnoses of COVIDâ19. None were hospitalised at diagnosis, but one was later admitted for periâmyocarditis. All five children had fatigue, dyspnoea, heart palpitations or chest pain, and four had headaches, difficulties concentrating, muscle weakness, dizziness and sore throats. Some had improved after 6â8 months, but they all suffered from fatigue and none had fully returned to school. The systematic review identified 179 publications and 19 of these were deemed relevant and read in detail. None contained any information on long COVID in children. CONCLUSION: Children may experience similar long COVID symptoms to adults and females may be more affected.', 'Purpose: People living with long COVID describe a high symptom burden, and a more detailed assessment of chronic fatigue and post-exertional malaise (PEM) may inform the development of rehabilitation recommendations. The aims of this study were to use validated questionnaires to measure the severity of fatigue and compare this with normative data and thresholds for clinical relevance in other diseases; measure and describe the impact of PEM; and describe symptoms of dysfunctional breathing, self-reported physical activity/sitting time, and health-related quality of life. Methods: This was an observational study involving an online survey for adults living with long COVID (data collection from February-April, 2021) following a confirmed or suspected SARS-CoV-2 infection. Questionnaires included the Functional Assessment of Chronic Illness Therapy-Fatigue Scale (FACIT-F) and DePaul Symptom Questionnaire-Post-Exertional Malaise. Results: After data cleaning, n=213 participants were included in the analysis. Participants primarily identified as women (85.5%), aged 40-59 (78.4%), who had been experiencing long COVID symptoms for [â¥]6 months (72.3%). The total FACIT-F score was 18{+/-}10 (where the score can range from 0-52, and a lower score indicates more severe fatigue), and 71.4% were experiencing chronic fatigue. Post-exertional symptom exacerbation affected most participants, and 58.7% met the scoring thresholds used in people living with myalgic encephalomyelitis/chronic fatigue syndrome. PEM occurred alongside a reduced capacity to work, be physically active, and function both physically and socially. Conclusion: Long COVID is characterized by chronic fatigue that is clinically relevant and is at least as severe as fatigue in several other clinical conditions, including cancer. PEM appears to be a common and significant challenge for the majority of this patient group. Patients, researchers, and allied health professionals are seeking information on safe rehabilitation for people living with long COVID, particularly regarding exercise. Fatigue and post-exertional symptom exacerbation must be monitored and reported in studies involving interventions for people with long COVID.', 'COVID-19 often causes sequelae after initial recovery, referred to collectively as long COVID. Long COVID is considered to be caused by the persistence of chronic inflammation after acute COVID-19 infection. We found that all long COVID patients had residual inflammation in the epipharynx, an important site of coronavirus replication, and some long COVID symptoms are similar to those associated with chronic epipharyngitis. Epipharyngeal abrasive therapy (EAT) is a treatment for chronic epipharyngitis in Japan that involves applying zinc chloride as an anti-inflammatory agent to the epipharyngeal mucosa. In this study, we evaluated the efficacy of EAT for the treatment of long COVID. The subjects in this study were 58 patients with long COVID who were treated with EAT in the outpatient department once a week for one month (mean age = 38.4 Â± 12.9 years). The intensities of fatigue, headache, and attention disorder, which are reported as frequent symptoms of long COVID, were assessed before and after EAT using the visual analog scale (VAS). EAT reduced inflammation in the epipharynx and significantly improved the intensity of fatigue, headache, and attention disorder, which may be related to myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS). These results suggest that EAT has potential as a novel method for long COVID treatment.', 'Objective: The systematic review aims to examine the association between COVID-19 and cognitive dysfunction, including the link between the severity of COVID-19 and the occurrence of cognitive impairment and the potential pathophysiological mechanisms related to brain fog among COVID-19 patients. Methods: PubMed, Oxford University Press, ProQuest Health and Medical Complete, ScienceDirect, Ovid, HERDIN, Google Scholar, and Cochrane Library databases were accessed to retrieve literature using the PRISMA guidelines. Results: After critical appraisal, thirteen full journal articles were included in the study. The studies showed the most frequent cognitive impairment are attention, memory, and executive function in COVID-19 patients. Compared with healthy controls (HC) in 3 out of 4 studies, cognitive impairment was only evident in COVID-19 patients. Furthermore, two studies showed no correlation between brain fog and depression, and five studies showed a link between the severity of COVID-19 infection and cognitive impairment. Cases ranging from mild to severe illness presented manifestations of brain fog. However, a disparity in the evidence of the pathophysiology of COVID-19 and cognitive dysfunction exists, prompting the need to investigate further. Additionally, recent studies provide insufficient evidence for direct central nervous system invasion, and there are emerging studies that contrast the presumed pathogenesis of neurological complications from neuroinflammation. Conclusion: There is an association between COVID-19 and cognitive dysfunction. Manifestation of cognitive dysfunction is present regardless of illness severity. Moreover, there are existing pathophysiological mechanisms of the Coronavirus that lead to cognitive dysfunction in COVID-19 patients; however, additional studies are required to substantiate such mechanisms further.', 'Lung injury may persist during the recovery period of COVID-19 as shown through imaging, six-minute walk, and lung function tests. The pathophysiological mechanisms leading to long COVID have not been adequately explained. Our aim is to investigate the basis of pulmonary susceptibility during sequelae and the possibility that prothrombotic states may influence long-term pulmonary symptoms of COVID-19. The patientâs lungs remain vulnerable during the recovery stage due to persistent shedding of the virus, the inflammatory environment, the prothrombotic state, and injury and subsequent repair of the blood-air barrier. The transformation of inflammation to proliferation and fibrosis, hypoxia-involved vascular remodeling, vascular endothelial cell damage, phosphatidylserine-involved hypercoagulability, and continuous changes in serological markers all contribute to post-discharge lung injury. Considering the important role of microthrombus and arteriovenous thrombus in the process of pulmonary functional lesions to organic lesions, we further study the possibility that prothrombotic states, including pulmonary vascular endothelial cell activation and hypercoagulability, may affect long-term pulmonary symptoms in long COVID. Early use of combined anticoagulant and antiplatelet therapy is a promising approach to reduce the incidence of pulmonary sequelae. Essentially, early treatment can block the occurrence of thrombotic events. Because impeded pulmonary circulation causes large pressure imbalances over the alveolar membrane leading to the infiltration of plasma into the alveolar cavity, inhibition of thrombotic events can prevent pulmonary hypertension, formation of lung hyaline membranes, and lung consolidation.', \"Background Long COVID is a term to describe new or persistent symptoms at least four weeks after onset of acute COVID-19. Clinical codes to describe this phenomenon were released in November 2020 in the UK, but it is not known how these codes have been used in practice. Methods Working on behalf of NHS England, we used OpenSAFELY data encompassing 96% of the English population. We measured the proportion of people with a recorded code for long COVID, overall and by demographic factors, electronic health record software system, and week. We also measured variation in recording amongst practices. Results Long COVID was recorded for 23,273 people. Coding was unevenly distributed amongst practices, with 26.7% of practices having not used the codes at all. Regional variation was high, ranging between 20.3 per 100,000 people for East of England (95% confidence interval 19.3-21.4) and 55.6 in London (95% CI 54.1-57.1). The rate was higher amongst women (52.1, 95% CI 51.3-52.9) compared to men (28.1, 95% CI 27.5-28.7), and higher amongst practices using EMIS software (53.7, 95% CI 52.9-54.4) compared to TPP software (20.9, 95% CI 20.3-21.4). Conclusions Long COVID coding in primary care is low compared with early reports of long COVID prevalence. This may reflect under-coding, sub-optimal communication of clinical terms, under-diagnosis, a true low prevalence of long COVID diagnosed by clinicians, or a combination of factors. We recommend increased awareness of diagnostic codes, to facilitate research and planning of services; and surveys of clinicians' experiences, to complement ongoing patient surveys.\", 'Purpose: Several weeks after COVID-19 infection, some children report the persistence or recurrence of functional complaints. This clinical presentation has been referred as âlong COVIDâ in the adult population, and an 18 F-FDG brain PET hypometabolic pattern has recently been suggested as a biomarker. Herein, we present a retrospective analysis of 7 paediatric patients with suspected long COVID who were explored by 18 F-FDG brain PET exam. Metabolic brain findings were confronted to those obtained in adult patients with long COVID, in comparison to their respective age-matched control groups. Methods: . Review of clinical examination, and whole-brain voxel-based analysis of 18 F-FDG PET metabolism of the 7 children in comparison to 20 paediatric controls, 35 adult patients with long COVID and 44 healthy adult subjects. Results: . Paediatric patients demonstrated a similair brain hypometabolic pattern as that found in adult long COVID patients, involving bilateral medial temporal lobes, brainstem and cerebellum (p-voxel < 0.001, p-cluster < 0.05 FWE-corrected), and also the right olfactory gyrus after small volume correction (p-voxel = 0.010 FWE-corrected), with partial recovery in two children at follow-up. Conclusion: These results provide arguments in favour of possible long COVID in children, with a similar functional brain involvement to those found in adults .', 'Importance: There is concern that post-acute SARS-CoV-2 infection health outcomes (\"post-COVID syndrome\") in children could be a serious problem but at the same time there is concern about the validity of reported associations between infection and long-term outcomes. Objective: To systematically assess the validity of reported post-acute SARS-CoV-2 infection health outcomes in children. Evidence Review: A search on PubMed and Web of Science was conducted to identify studies published up to January 22, 2022, that reported on post-acute SARS-CoV-2 infection health outcomes in children (<18 years) with a minimum follow-up of 2 months since detection of infection or 1 month since recovery from acute illness. We assessed the consideration of confounding bias and causality, and the risk of bias. Findings: 21 studies including 81,896 children reported up to 97 symptoms with follow-up periods of 2-11.5 months. Fifteen studies had no control group. The reported proportion of children with post-COVID syndrome was between 0% and 66.5% in children with SARS-CoV-2 infection (n=16,986) and 2% to 53.3% in children without SARS-CoV-2 infection (n=64,910). Only 2 studies made a clear causal interpretation of an association of SARS-CoV-2 infection and the main outcome of \"post-COVID syndrome\" and provided recommendations regarding prevention measures. Two studies mentioned potential limitations in the conclusion of the main text but none of the 21 studies mentioned any limitations in the abstract nor made a clear statement for cautious interpretation. The validity of all 21 studies was seriously limited due to an overall critical risk of bias (critical risk for confounding bias [n=21]; serious or critical risk for selection bias [n=19]; serious risk for misclassification bias [n=3], for bias due to missing data [n=14] and for outcome measurement [n=12]; and critical risk for selective reporting bias [n=16]). Conclusions and Relevance: The validity of reported post-acute SARS-CoV-2 infection health outcomes in children is critically limited. None of the studies provided evidence with reasonable certainty on whether SARS-CoV-2 infection has an impact on post-acute health outcomes, let alone to what extent. Children and their families urgently need much more reliable and methodologically robust evidence to address their concerns and improve care.', 'Long-term complications after coronavirus disease 2019 (COVID-19) are common in hospitalized patients, but the spectrum of symptoms in milder cases needs further investigation. We conducted a long-term follow-up in a prospective cohort study of 312 patientsâ247 home-isolated and 65 hospitalizedâcomprising 82% of total cases in Bergen during the first pandemic wave in Norway. At 6 months, 61% (189/312) of all patients had persistent symptoms, which were independently associated with severity of initial illness, increased convalescent antibody titers and pre-existing chronic lung disease. We found that 52% (32/61) of home-isolated young adults, aged 16â30 years, had symptoms at 6 months, including loss of taste and/or smell (28%, 17/61), fatigue (21%, 13/61), dyspnea (13%, 8/61), impaired concentration (13%, 8/61) and memory problems (11%, 7/61). Our findings that young, home-isolated adults with mild COVID-19 are at risk of long-lasting dyspnea and cognitive symptoms highlight the importance of infection control measures, such as vaccination.', 'SYNOPSIS The term long COVID was coined by patients to describe the long-term consequences of COVID-19. One year into the pandemic, it was clear that all patients-those hospitalized with COVID-19 and those who lived with the disease in the community-were at risk of developing debilitating sequelae that would impact their quality of life. Patients with long COVID asked for rehabilitation. Many of them, including previously healthy and fit clinicians, tried to fight postviral fatigue with exercise-based rehabilitation. We observed a growing number of patients with long COVID who experienced adverse effects from exercise therapy and symptoms strikingly similar to those of myalgic encephalomyelitis (ME). Community-based physical therapists, including those in private practice, unaware of safety issues, are preparing to help an influx of patients with long COVID. In this editorial, we expose growing concerns about long COVID and ME. We issue safety recommendations for rehabilitation and share resources to improve care for those with postviral illnesses. J Orthop Sports Phys Ther 2021;51(5):197-200. doi:10.2519/jospt.2021.0106.', 'Long COVID is characterized by the emergence of multiple debilitating symptoms following SARS-CoV-2 infection. Its etiology is unclear and it often follows a mild acute illness. Anecdotal reports of gradual clinical responses to histamine receptor antagonists (HRAs) suggest a histamine-dependent mechanism that is distinct from anaphylaxis, possibly mediated by T cells, which are also regulated by histamine. T cell perturbations have been previously reported in post-viral syndromes, but the T cell landscape in patients who have recovered from mild COVID-19 and its relationship to both long COVID symptoms and any symptomatic response to HRA remain underexplored. We addressed these questions in an observational study of 65 individuals who had recovered from mild COVID-19. Participants were surveyed between 87 and 408 days after the onset of acute symptoms; none had required hospitalization, 16 had recovered uneventfully, and 49 had developed long COVID. Symptoms were quantified using a structured questionnaire and T cell subsets enumerated in a standard diagnostic assay. Patients with long-COVID had reduced CD4+ and CD8+ effector memory (EM) cell numbers and increased PD-1 (programmed cell death protein 1) expression on central memory (CM) cells, whereas the asymptomatic participants had reduced CD8+ EM cells only and increased CD28 expression on CM cells. 72% of patients with long COVID who received HRA reported clinical improvement, although T cell profiling did not clearly distinguish those who responded to HRA. This study demonstrates that T cell perturbations persist for several months after mild COVID-19 and are associated with long COVID symptoms.', 'Background: The prevalence of true asymptomatic COVID-19 cases is critical to policy makers considering the effectiveness of mitigation measures against the SARS-CoV-2 pandemic. We aimed to synthesize all available research on the asymptomatic rates and transmission rates where possible. Methods: We searched PubMed, Embase, Cochrane COVID-19 trials, and European PMC for pre-print platforms such as MedRxiv. We included primary studies reporting on asymptomatic prevalence where: (a) the sample frame includes at-risk population, and (b) there was sufficiently long follow up to identify pre-symptomatic cases. Meta-analysis used fixed effect and random effects models. Results: We screened 571 articles and included five low risk-of-bias studies from three countries (China (2), USA (2), Italy (1)) that tested 9,242 at-risk people, of which 413 were positive and 65 were asymptomatic. Diagnosis in all studies was confirmed using a RT-qPCR test. The proportion of asymptomatic cases ranged from 6% to 41%. Meta-analysis (fixed effect) found that the proportion of asymptomatic cases was 16% (95% CI: 12% - 20%) overall; higher in non-aged care 19% (15% - 24%), and lower in long-term aged care 8% (4% - 14%). Two studies provided direct evidence of forward transmission of the infection by asymptomatic cases but suggested lower rates than symptomatic cases. Conclusion: Our estimates of the prevalence of asymptomatic COVID-19 cases are lower than many highly publicized studies, but still substantial. Further robust epidemiological evidence is urgently needed, including in sub-populations such as children, to better understand the importance of asymptomatic cases for driving spread of the pandemic.', 'BACKGROUND: While much of the scientific focus thus far has been on cognitive sequelae in patients with severe COVIDâ19, subjective cognitive complaints are being reported across the spectrum of disease severity, with recent studies beginning to corroborate patientsâ perceived deficits. In response to this, the aims of this study were to (1) explore the frequency of impaired performance across cognitive domains in postâCOVID patients with subjective complaints and (2) uncover whether impairment existed within a single domain or across multiple. METHODS: Sixtyâthree patients with subjective cognitive complaints postâCOVID were assessed with a comprehensive protocol consisting of various neuropsychological tests and mood measures. Cognitive test performance was transformed into T scores and classified based on recommended guidelines. After performing a principal component analysis to define cognitive domain factors, distributions of test scores within and across domains were analyzed. RESULTS: Results revealed pervasive impact on attention abilities, both as the singularly affected domain (19% of singleâdomain impairment) as well as coupled with decreased performance in executive functions, learning, and longâterm memory. These salient attentional and associated executive deficits were largely unrelated to clinical factors such as hospitalization, disease duration, biomarkers, or affective measures. DISCUSSION: These findings stress the importance of comprehensive evaluation and intervention to address cognitive sequelae in postâCOVID patients of varying disease courses, not just those who were hospitalized or experienced severe symptoms. Future studies should investigate to what extent these cognitive abilities are recuperated over time as well as employ neuroimaging techniques to uncover underlying mechanisms of neural damage.', 'This report describes persistent symptoms associated with post-acute COVID-19 syndrome (PACS) and the impact of these symptoms on physical function, cognitive function, health-related quality of life, and participation. DESIGN: This study used a cross-sectional observational study design. Patients attending Mount Sinaiâs post-acute COVID-19 syndrome clinic completed surveys containing patient-reported outcomes. RESULTS: A total of 156 patients completed the survey, at a median (range) time of 351 days (82â457 days) after COVID-19 infection. All patients were prevaccination. The most common persistent symptoms reported were fatigue (n = 128, 82%), brain fog (n = 105, 67%), and headache (n = 94, 60%). The most common triggers of symptom exacerbation were physical exertion (n = 134, 86%), stress (n = 107, 69%), and dehydration (n = 77, 49%). Increased levels of fatigue (Fatigue Severity Scale) and dyspnea (Medical Research Council) were reported, alongside reductions in levels of regularly completed physical activity. Ninety-eight patients (63%) scored for at least mild cognitive impairment (Neuro-Qol), and the domain of the EuroQol: 5 dimension, 5 level most impacted was Self-care, Anxiety/Depression and Usual Activities. CONCLUSIONS: Persistent symptoms associated with post-acute COVID-19 syndrome seem to impact physical and cognitive function, health-related quality of life, and participation in society. More research is needed to further clarify the relationship between COVID-19 infection and post-acute COVID-19 syndrome symptoms, the underlying mechanisms, and treatment options.', 'Understanding how hospitalization affects cognitive development is crucial to safeguard childrenâs cognition; however, there is little research evaluating the associations between NICU or PICU hospitalization and survivorsâ cognition. The objective of this study is to identify and characterize the associations between a neonatal or pediatric ICU hospitalization and the short- and long-term cognition of survivors. The databases Cochrane Library, Medline, EBSCO, Embase, and Google Scholar and the journals JAMA Pediatrics, Journal of Pediatrics, Pediatrics, Archives of Disease in Childhood, Academic Pediatrics, Pediatric Critical Care Medicine and Child Development were searched until April 2021. Retrieved article references were analyzed. Included articles investigated cognition as an outcome of ICU hospitalization in non-preterm neonatal or pediatric patients. Case studies and studies analyzing diagnosis or treatment interventions were excluded. Four prospective cohort or case-control studies and two retrospective cohort studies were included, totaling 2172 neonatal and 42368 pediatric patients. Quality assessment using the BMJ Criteria and Cochrane Collaborationâs Risk-of-Bias tool displayed good results. Significant negative associations were found between neonatal cognition and length-of-ICU-stay at 9- (p<0.001) and 24 months (p<0.01), and between pediatric cognition and length-of-ICU-stay at discharge (p<0.001). Additional weeks on the neonatal ICU increased odds of impairment at 9- (OR 1.08, 95%CI 1.034â1.112) and 24 months (OR 1.11, 95%CI 1.065â1.165). Conclusion: There is a significant negative correlation between NICU and PICU hospitalization and the short- and long-term cognitive status. Future research must identify patient- and hospital-related risk factors and develop management strategies.', \"BACKGROUND: Preliminary evidence has highlighted a possible association between severe COVID-19 and persistent cognitive deficits. Further research is required to confirm this association, determine whether cognitive deficits relate to clinical features from the acute phase or to mental health status at the point of assessment, and quantify rate of recovery. METHODS: 46 individuals who received critical care for COVID-19 at Addenbrooke's hospital between 10th March 2020 and 31st July 2020 (16 mechanically ventilated) underwent detailed computerised cognitive assessment alongside scales measuring anxiety, depression and post-traumatic stress disorder under supervised conditions at a mean follow up of 6.0 (Â± 2.1) months following acute illness. Patient and matched control (N = 460) performances were transformed into standard deviation from expected scores, accounting for age and demographic factors using N = 66,008 normative datasets. Global accuracy and response time composites were calculated (G_SScore & G_RT). Linear modelling predicted composite score deficits from acute severity, mental-health status at assessment, and time from hospital admission. The pattern of deficits across tasks was qualitatively compared with normal age-related decline, and early-stage dementia. FINDINGS: COVID-19 survivors were less accurate (G_SScore=-0.53SDs) and slower (G_RT=+0.89SDs) in their responses than expected compared to their matched controls. Acute illness, but not chronic mental health, significantly predicted cognitive deviation from expected scores (G_SScore (p=\\u200b\\u200b0.0037) and G_RT (p = 0.0366)). The most prominent task associations with COVID-19 were for higher cognition and processing speed, which was qualitatively distinct from the profiles of normal ageing and dementia and similar in magnitude to the effects of ageing between 50 and 70 years of age. A trend towards reduced deficits with time from illness (râ¼=0.15) did not reach statistical significance. INTERPRETATION: Cognitive deficits after severe COVID-19 relate most strongly to acute illness severity, persist long into the chronic phase, and recover slowly if at all, with a characteristic profile highlighting higher cognitive functions and processing speed. FUNDING: This work was funded by the 10.13039/501100000272National Institute for Health Research (NIHR) Cambridge Biomedical Research Centre (BRC), NIHR Cambridge Clinical Research Facility (BRC-1215-20014), the Addenbrooke's Charities Trust and NIHR COVID-19 BioResource RG9402. AH is funded by the 10.13039/100007472UK Dementia Research Institute Care Research and Technology Centre and Imperial College London Biomedical Research Centre. ETB and DKM are supported by 10.13039/100006662NIHR Senior Investigator awards. JBR is supported by the 10.13039/100010269Wellcome Trust (220258) and Medical Research Council (SUAG/051 G101400). VFJN is funded by an Academy of Medical Sciences/ The Health Foundation Clinician Scientist Fellowship. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care.\", 'The primate prefrontal cortex (PFC) subserves our highest order cognitive operations, and yet is tremendously dependent on a precise neurochemical environment for proper functioning. Depletion of noradrenaline and dopamine, or of acetylcholine from the dorsolateral PFC (dlPFC), is as devastating as removing the cortex itself, and serotonergic influences are also critical to proper functioning of the orbital and medial PFC. Most neuromodulators have a narrow inverted U dose response, which coordinates arousal state with cognitive state, and contributes to cognitive deficits with fatigue or uncontrollable stress. Studies in monkeys have revealed the molecular signaling mechanisms that govern the generation and modulation of mental representations by the dlPFC, allowing dynamic regulation of network strength, a process that requires tight regulation to prevent toxic actions, e.g., as occurs with advanced age. Brain imaging studies in humans have observed drug and genotype influences on a range of cognitive tasks and on PFC circuit functional connectivity, e.g., showing that catecholamines stabilize representations in a baseline-dependent manner. Research in monkeys has already led to new treatments for cognitive disorders in humans, encouraging future research in this important field.', 'There is strong evidence of brain-related abnormalities in COVID-19(1â13). However, it remains unknown whether the impact of SARS-CoV-2 infection can be detected in milder cases, and whether this can reveal possible mechanisms contributing to brain pathology. Here we investigated brain changes in 785 participants of UK Biobank (aged 51â81 years) who were imaged twice using magnetic resonance imaging, including 401 cases who tested positive for infection with SARS-CoV-2 between their two scansâwith 141 days on average separating their diagnosis and the second scanâas well as 384 controls. The availability of pre-infection imaging data reduces the likelihood of pre-existing risk factors being misinterpreted as disease effects. We identified significant longitudinal effects when comparing the two groups, including (1) a greater reduction in grey matter thickness and tissue contrast in the orbitofrontal cortex and parahippocampal gyrus; (2) greater changes in markers of tissue damage in regions that are functionally connected to the primary olfactory cortex; and (3) a greater reduction in global brain size in the SARS-CoV-2 cases. The participants who were infected with SARS-CoV-2 also showed on average a greater cognitive decline between the two time points. Importantly, these imaging and cognitive longitudinal effects were still observed after excluding the 15 patients who had been hospitalised. These mainly limbic brain imaging results may be the in vivo hallmarks of a degenerative spread of the disease through olfactory pathways, of neuroinflammatory events, or of the loss of sensory input due to anosmia. Whether this deleterious effect can be partially reversed, or whether these effects will persist in the long term, remains to be investigated with additional follow-up.', 'Introduction: Although the efficacy of SARS-CoV-2 vaccination to prevent symptomatic COVID-19 is well established, there are no published studies on the impact on symptoms in patients with Long Covid. Anecdotal reports have suggested both a potential benefit and worsening of symptoms post vaccination with the uncertainty leading to some vaccine hesitancy amongst affected individuals. Methods: Patients initially hospitalised with COVID-19 were prospectively recruited to an observational study with clinical follow-up at 3 months (June-July 2020) and 8 months (Dec 2020-Jan 2021) post-admission. Participants who received the Pfizer-BioNTech (BNT162b2) or Oxford-AstraZeneca (ChAdOx1 nCoV-19) vaccine between January to February 2021 were identified and matched 2:1 (in terms of 8-month symptoms) with participants from the same cohort who were unvaccinated. All were re-assessed at 1 month post vaccination (or matched timepoint for unvaccinated cohort). Validated quality of life (SF-36), mental wellbeing (WEMWBS) and ongoing symptoms were assessed at all timepoints. Formal statistical analysis compared the effect of vaccination on recent quality of life using baseline symptoms, age, and gender in linear regression. Results: Forty-four vaccinated participants were assessed at a median of 32 days (IQR 20-41) post vaccination with 22 matched unvaccinated participants. Most were highly symptomatic of Long Covid at 8 months (82% in both groups had at least 1 persistent symptom), with fatigue (61%), breathlessness (50%) and insomnia (38%) predominating. There was no significant worsening in quality-of-life or mental wellbeing metrics pre versus post vaccination. Nearly two-thirds (n=27) reported transient (<72hr duration) systemic effects (including fever, myalgia and headache). When compared to matched unvaccinated participants from the same cohort, those who had receive a vaccine had a small overall improvement in Long Covid symptoms, with a decrease in worsening symptoms (5.6% vaccinated vs 14.2% unvaccinated) and increase in symptom resolution (23.2% vaccinated vs 15.4% unvaccinated) (p=0.035). No difference in response was identified between Pfizer-BioNTech or Oxford-AstraZeneca vaccines. Conclusions: Receipt of vaccination with either an mRNA or adenoviral vector vaccine was not associated with a worsening of Long Covid symptoms, quality of life, or mental wellbeing. Individuals with prolonged COVID-19 symptoms should receive vaccinations as suggested by national guidance.', 'Prior studies of patient survivorship after an intensive care unit (ICU) stay suggest that many critically ill patients with COVID-19 will face long-lasting physical, cognitive and/or mental health impairments. This anticipated survivorship experience highlights the importance of collaboration between the fields of critical care and rehabilitation to optimize post-COVID-19 recovery.', \"Chronic neuroinflammation with sustained microglial activation occurs following severe traumatic brain injury (TBI) and is believed to contribute to subsequent neurodegeneration and neurological deficits . Microglia, the primary innate immune cells in brain, are dependent on colony stimulating factor 1 receptor (CSF1R) signaling for their survival. In this pre-clinical study, we examined the effects of delayed depletion of chronically activated microglia on functional recovery and neurodegeneration up to three months post-injury. A CSF1R inhibitor, PLX5622, was administered to adult male C57Bl/6J mice at one month after controlled cortical impact to remove chronically activated microglia, and the inhibitor was withdrawn 1-week later to allow for microglial repopulation. Following TBI, the repopulated microglia displayed a ramified morphology similar to that of sham uninjured mice, whereas microglia in vehicle-treated TBI mice showed the typical chronic posttraumatic hypertrophic morphology. PLX5622 treatment limited TBI-associated neuropathological changes at 3 months post-injury; these included a smaller cortical lesion, reduced hippocampal neuron cell death, and decreased NOX2- and NLRP3 inflammasome-associated neuroinflammation. Furthermore, delayed depletion of chronically activated microglia after TBI led to widespread changes in the cortical transcriptome and altered gene pathways involved in neuroinflammation, oxidative stress, and neuroplasticity. Using a variety of complementary neurobehavioral tests, PLX5622-treated TBI mice also had improved long-term motor and cognitive function recovery through 3 months post-injury. Together, these studies demonstrate that chronic phase removal of neurotoxic microglia after TBI using CSF1R inhibitors markedly reduce chronic neuroinflammation and associated neurodegeneration, as well as related motor and cognitive deficits.SIGNIFICANCE STATEMENTTraumatic brain injury (TBI) is a debilitating neurological disorder that can seriously impact the patient's quality of life. Microglial-mediated neuroinflammation is induced after severe TBI and contributes to neurological deficits and on-going neurodegenerative processes. Here, we investigated the effect of breaking the neurotoxic neuroinflammatory loop at 1-month after controlled cortical impact in mice by pharmacological removal of chronically activated microglia using a CSF1R inhibitor, PLX5622. Overall, we show that short-term elimination of microglia during the chronic phase of TBI followed by repopulation results in long-term improvements in neurological function, suppression of neuroinflammatory and oxidative stress pathways, and a reduction in persistent neurodegenerative processes. These studies are clinically relevant and support new concepts that the therapeutic window for TBI may be far longer than traditionally believed if chronic and evolving microglial-mediated neuroinflammation can be inhibited or regulated in a precise manner.\", 'INTRODUCTION: We conducted a systematic review and metaâanalysis of the cognitive effects of coronavirus disease 2019 (COVIDâ19) in adults with no prior history of cognitive impairment. METHODS: Searches in Medline/Web of Science/Embase from January 1, 2020, to December 13, 2021, were performed following Preferred Reporting Items for Systematic Reviews and MetaâAnalyses (PRISMA) guidelines. A metaâanalysis of the Montreal Cognitive Assessment (MoCA) total score comparing recovered COVIDâ19 and healthy controls was performed. RESULTS: Oof 6202 articles, 27 studies with 2049 individuals were included (mean age = 56.05 years, evaluation time ranged from the acute phase to 7 months postâinfection). Impairment in executive functions, attention, and memory were found in postâCOVIDâ19 patients. The metaâanalysis was performed with a subgroup of 290 individuals and showed a difference in MoCA score between postâCOVIDâ19 patients versus controls (mean difference = â0.94, 95% confidence interval [CI] â1.59, â0.29; P = .0049). DISCUSSION: Patients recovered from COVIDâ19 have lower general cognition compared to healthy controls up to 7 months postâinfection.', 'Objectives: Dogs can be trained to identify several substances not detected by humans, corresponding to specific volatile organic compounds (VOCs). The presence of VOCs, triggered by SARS-CoV-2 infection, was tested in sweat from Long COVID patients. Patients and methods: An axillary sweat sample of Long COVID patients and of COVID-19 negative, asymptomatic individuals was taken at home to avoid any hospital contact. Swabs were randomly placed in olfaction detection cones, and the material sniffed by at least 2 trained dogs. Results: Forty-five Long COVID patients, mean age 45 (6-71), 73.3% female, with prolonged symptoms evolving for a mean of 15.2 months (5-22) were tested. Dogs discriminated in a positive way 23/45 (51.1%) Long COVID patients versus 0/188 (0%) control individuals (p<.0001). Conclusion:This study suggests the persistence of a viral infection in some Long COVID patients and the possibility of providing a simple, highly sensitive, non-invasive test to detect viral presence, during acute and extended phases of COVID-19.', 'BACKGROUND: Little is known about long-term recovery from COVID-19 disease, especially in non-hospitalized individuals. In this longitudinal study we present symptoms registered during the acute phase as well as long COVID, i.e. long-lasting COVID-19 symptoms, in patients from the Faroe Islands. METHODS: All consecutive patients with confirmed RT-PCR testing from April to June 2020 were invited to participate in this study for the assessment of long COVID. Demographic and clinical characteristics and self-reported acute and persistent symptoms were assessed using a standardized detailed questionnaire administered at enrollment and at repeated phone interviews in the period 22 (th) April to Aug 16 (th). RESULTS: Of the 180 participants (96.3% of the 187 eligible COVID-19 patients), 53.1% reported persistence of at least one symptom after a mean of 125 days after symptoms onset, 33.3% reported one or two symptoms and 19.4% three or more symptoms. At the last follow-up, 46.7% were asymptomatic compared with 4.4 % during the acute phase. The most prevalent persistent symptoms were fatigue, loss of smell and taste, and arthralgias. CONCLUSIONS: Our results show that it might take months for symptoms to resolve, even among non-hospitalized persons with mild illness course in the acute phase. Continued monitoring for long COVID is needed.', 'Patients who survive influenza A (H7N9) virus infection are at risk of physical and psychological complications of lung injury and multi-organ dysfunction. However, there were no prospectively individualized assessments of physiological, functional and quality-of-life measures after hospital discharge. The current study aims to assess the main determinants of functional disability of these patients during the follow-up. Fifty-six influenza A (H7N9) survivors were investigated during the 2-year after discharge from the hospital. Results show interstitial change and fibrosis on pulmonary imaging remained 6 months after hospital discharge. Both ventilation and diffusion dysfunction improved, but restrictive and obstructive patterns on ventilation function test persisted throughout the follow-up period. For patients with acute respiratory distress syndrome lung functions improved faster during the first six months. Role-physical and Role-emotional domains in the 36-Item Short-Form Health Survey were worse than those of a sex- and age-matched general population group. The quality of life of survivors with ARDS was lower than those with no ARDS. Our findings suggest that pulmonary function and imaging findings improved during the first 6 months especially for those with ARDS, however long-term lung disability and psychological impairment in H7N9 survivors persisted at 2 years after discharge from the hospital.', 'Background: Long COVID or long-term complication after COVID-19 has the ability to affect health and quality of life. Knowledge about the burden and predictors could aid in their prevention and management. Most of the studies are from high-income countries and focus on severe cases. We did this study to estimate the prevalence and identify the characteristics and predictors of Long COVID among our patients. Methodology: We recruited adult ([â¥]18 years) patients who were diagnosed as Reverse Transcription Polymerase Chain Reaction (RTPCR) confirmed SARS-COV-2 infection and were either hospitalized or tested on outpatient basis. Eligible participants were followed up telephonically after four weeks of diagnosis of SARS-COV-2 infection to collect data on sociodemographic, clinical history, vaccination history, Cycle threshold (Ct) values during diagnosis and other variables. Characteristic of Long COVID were elicited, and multivariable logistic regression was done to find the predictors of Long COVID. Results: We have analyzed 487 individual data with a median follow-up of 44 days (Inter quartile range (IQR): 39,47). Overall, Long COVID was reported by 29.2% (95% Confidence interval (CI): 25.3%,33.4%) participants. Prevalence of Long COVID among patients with mild/moderate disease (n = 415) was 23.4% (95% CI: 19.5%,27.7%) as compared to 62.5% (95% CI: 50.7%,73%) in severe/critical cases(n=72). The most common Long COVID symptom was fatigue (64.8%) followed by cough (32.4%). Statistically significant predictors of Long COVID were - Pre-existing medical conditions (Adjusted Odds ratio (aOR)=2.00, 95% CI: 1.16,3.44), having a more significant number of symptoms during acute phase of COVID-19 disease (aOR=11.24, 95% CI: 4.00,31.51), two doses of COVID-19 vaccination (aOR=2.32, 95% CI: 1.17,4.58), the severity of illness (aOR=5.71, 95% CI: 3.00,10.89) and being admitted to hospital (Odds ratio (OR)=3.89, 95% CI: 2.49,6.08). Conclusion: A considerable proportion of COVID-19 cases reported Long COVID symptoms. More research is needed in Long COVID to objectively assess the symptoms and find the biological and radiological markers.', 'Abstract Objectives This study aims to estimate the prevalence and longevity of detectable SARS-CoV-2 antibodies as well as T and B memory cells during infection with SARS-CoV-2 and after recovery. In addition, prevalence of COVID-19 reinfection, and the preventive efficacy of previous infection with SARS-CoV-2 were investigated. Methods and analyses A synthesis of existing research was conducted. The Cochrane Library for COVID-19 resources, the China Academic Journals Full Text Database, PubMed, and Scopus as well as preprint servers were searched for studies conducted between 1 January 2020 to 1 April 2021. We included studies with the relevant outcomes of interest. All included studies were assessed for methodological quality and pooled estimates of relevant outcomes were obtained in a meta-analysis using a bias adjusted synthesis method. Proportions were synthesized with the Freeman-Tukey double arcsine transformation and binary outcomes using the odds ratio (OR). Heterogeneity between included studies was assessed using the I2 and Cochrans Q statistics and publication bias was assessed using Doi plots. Results Fifty-four studies, from 18 countries, with a total of 12 011 447 individuals, followed up to 8 months after recovery were included. At 6-8 months after recovery, the prevalence of SARS-CoV-2 specific immunological memory remained high; IgG 90.4% (95%CI 72.2 to 99.9, I2=89.0%, 5 studies), CD4+ 91.7% (95%CI 78.2 to 97.1, one study), and memory B cells 80.6% (95%CI 65.0 to 90.2, one study) and the pooled prevalence of reinfection was 0.2% (95%CI 0.0 to 0.7, I2 = 98.8, 9 studies). Individuals previously infected with SARS-CoV-2 had an 81% reduction in odds of a reinfection (OR 0.19, 95% CI 0.1 to 0.3, I2 = 90.5%, 5 studies). Conclusion Around 90% of people previously infected with SARS-CoV-2 had evidence of immunological memory to SARS-CoV-2, which was sustained for at least 6-8 months after recovery, and had a low risk of reinfection.', 'The COVID-19 pandemic and the lockdown orders adopted to prevent the spread of the disease had a huge impact on a personal, social, and economic level for the world population. In Europe, Italy was one of the frontrunner countries dealing with an emergency that significantly affected peopleâs lives. Previous research on the psychological impact of the pandemic revealed an increase in anxiety, depression, and feelings of distress; however, these studies were conducted on non-representative samples of the population reached through social media channels, a method that is likely to lead to many forms of statistical and methodological bias. For the first time to our knowledge, we assessed the psychological impact of COVID-19 on 6700 Italian individuals, representative of the Italian population in terms of age, gender, and geographical areas revealing higher scores of depressive symptoms in females, younger adults, people reporting professional uncertainty and lower socio-economic status. A positive correlation was also found for individuals living alone, those who could not leave home for going to work, and people with a case of COVID-19 in the family, whereas the region of residence was not a significant predictor of depressive symptoms. These findings underline the importance of considering the psychological effects of COVID-19 and providing support to individuals seeking mental health care.', 'Background: The COVID-19 pandemic and associated lockdowns have had a dramatic impact on many people, but individuals with an intellectual disability, given the prevalence of congregate living and high levels of co-morbid conditions, may be particularly vulnerable at this time. A prior initial survey of participants of the Intellectual Disability Supplement to the Irish Longitudinal Study on Ageing (IDS-TILDA) found that, despite a majority of participants being tested, only a small proportion had tested positive for COVID-19. Furthermore, despite some reporting positive aspects to the lockdown, a similar proportion were experiencing stress or anxiety during the pandemic. The pandemic and lockdowns have continued, and it is possible that experiences and consequences have changed over time. Aim: To explore over time and in greater depth the impact of COVID-19 and associated lockdowns and to further establish rates of infection, rates of vaccination and participantsâ experiences. Methods: A structured questionnaire for people with intellectual disability participating in the IDS-TILDA longitudinal study, to be administered by telephone/video in summer 2021. Where participants are unable to respond independently, a proxy respondent will be invited to either assist the participant or answer questions on their behalf. This questionnaire will include questions from the first COVID-19 questionnaire, with extra questions assessing âlong COVIDâ (i.e. COVID-19 lasting for 12 weeks or longer), infection control behaviours, changes in mental health, social contacts and loneliness, frailty, healthcare, and incidence of vaccination. Impact: The results of this survey will be used to inform healthcare provision for people with intellectual disability during the latter stages of the lockdown and into the future.', 'BACKGROUND: Physical exercise is an effective strategy for preserving functional capacity and improving the symptoms of frailty in older adults. In addition to functional gains, exercise is considered to be a cornerstone for enhancing cognitive function in frail older adults with cognitive impairment and dementia. We assessed the effects of the Vivifrail exercise intervention for functional capacity, cognition, and wellâbeing status in communityâdwelling older adults. METHODS: In a multicentre randomized controlled trial conducted in three tertiary hospitals in Spain, a total of 188 older patients with mild cognitive impairment or mild dementia (aged >75 years) were randomly assigned to an exercise intervention (n = 88) or a usualâcare, control (n = 100) group. The intervention was based on the Vivifrail tailored multicomponent exercise programme, which included resistance, balance, flexibility (3 days/week), and gaitâretraining exercises (5 days/week) and was performed for three consecutive months (http://vivifrail.com). The usualâcare group received habitual outpatient care. The main endpoint was change in functional capacity from baseline to 1 and 3 months, assessed with the Short Physical Performance Battery (SPPB). Secondary endpoints were changes in cognitive function and handgrip strength after 1 and 3 months, and wellâbeing status, falls, hospital admission rate, visits to the emergency department, and mortality after 3 months. RESULTS: The Vivifrail exercise programme provided significant benefits in functional capacity over usualâcare. The mean adherence to the exercise sessions was 79% in the first month and 68% in the following 2 months. The intervention group showed a mean increase (over the control group) of 0.86 points on the SPPB scale (95% confidence interval [CI] 0.32, 1.41 points; P < 0.01) after 1 month of intervention and 1.40 points (95% CI 0.82, 1.98 points; P < 0.001) after 3 months. Participants in the usualâcare group showed no significant benefit in functional capacity (mean change of â0.17 points [95% CI â0.54, 0.19 points] after 1 month and â0.33 points [95% CI â0.70, 0.04 points] after 3 months), whereas the exercise intervention reversed this trend (0.69 points [95% CI 0.29, 1.09 points] after 1 month and 1.07 points [95% CI 0.63, 1.51 points] after 3 months). Exercise group also obtained significant benefits in cognitive function, muscle function, and depression after 3 months over control group (P < 0.05). No betweenâgroup differences were obtained in other secondary endpoints (P > 0.05). CONCLUSIONS: The Vivifrail exercise training programme is an effective and safe therapy for improving functional capacity in communityâdwelling frail/prefrail older patients with mild cognitive impairment or mild dementia and also seems to have beneficial effect on cognition, muscle function, and mood status.', 'Background The medium-term effects of Coronavirus disease (COVID-19) on multiple organ health, exercise capacity, cognition, quality of life and mental health are poorly understood. Methods Fifty-eight COVID-19 patients post-hospital discharge and 30 comorbidity-matched controls were prospectively enrolled for multiorgan (brain, lungs, heart, liver and kidneys) magnetic resonance imaging (MRI), spirometry, six-minute walk test, cardiopulmonary exercise test (CPET), quality of life, cognitive and mental health assessments. Findings At 2-3 months from disease-onset, 64% of patients experienced persistent breathlessness and 55% complained of significant fatigue. On MRI, tissue signal abnormalities were seen in the lungs (60%), heart (26%), liver (10%) and kidneys (29%) of patients. COVID-19 patients also exhibited tissue changes in the thalamus, posterior thalamic radiations and sagittal stratum on brain MRI and demonstrated impaired cognitive performance, specifically in the executive and visuospatial domain relative to controls. Exercise tolerance (maximal oxygen consumption and ventilatory efficiency on CPET) and six-minute walk distance (405{+/-}118m vs 517{+/-}106m in controls, p<0.0001) were significantly reduced in patients. The extent of extra-pulmonary MRI abnormalities and exercise tolerance correlated with serum markers of ongoing inflammation and severity of acute illness. Patients were more likely to report symptoms of moderate to severe anxiety (35% versus 10%, p=0.012) and depression (39% versus 17%, p=0.036) and significant impairment in all domains of quality of life compared to controls. Interpretation A significant proportion of COVID-19 patients discharged from hospital experience ongoing symptoms of breathlessness, fatigue, anxiety, depression and exercise limitation at 2-3 months from disease-onset. Persistent lung and extra-pulmonary organ MRI findings are common. In COVID-19 survivors, chronic inflammation may underlie multiorgan abnormalities and contribute to impaired quality of life.', 'There is increasing reporting by patients organization and researchers of long covid (or post-acute sequelae of SARS-CoV-2 - PASC), characterized by symptoms such as fatigue, dyspnea, chest pain, cognitive and sleeping disturbances, arthralgia and decline in quality of life. Immune system dysregulation with a hyperinflammatory state, direct viral toxicity, endothelial damage and microvascular injury have been proposed as pathologenic mechanisms. Recently, cohorts of children with PASC have been reported in Italy, Sweden and Russia. However, immunological studies of children with PASC have never been performed. In this study, we documented significant immunologic differences between children that completely recovered from acute infection and those with PASC, providing the first objective laboratory sign of the existence of PASC in children.', 'Background There are currently no effective pharmacological or non-pharmacological interventions for Long-COVID. To identify potential therapeutic targets, we focussed on previously described four recovery clusters five months after hospital discharge, their underlying inflammatory profiles and relationship with clinical outcomes at one year. Methods PHOSP-COVID is a prospective longitudinal cohort study, recruiting adults hospitalised with COVID-19 across the UK. Recovery was assessed using patient reported outcomes measures (PROMs), physical performance, and organ function at five-months and one-year after hospital discharge. Hierarchical logistic regression modelling was performed for patient-perceived recovery at one-year. Cluster analysis was performed using clustering large applications (CLARA) k-medoids approach using clinical outcomes at five-months. Inflammatory protein profiling from plasma at the five-month visit was performed. Findings 2320 participants have been assessed at five months after discharge and 807 participants have completed both five-month and one-year visits. Of these, 35.6% were female, mean age 58.7 (SD 12.5) years, and 27.8% received invasive mechanical ventilation (IMV). The proportion of patients reporting full recovery was unchanged between five months 501/165 (25.6%) and one year 232/804 (28.9%). Factors associated with being less likely to report full recovery at one year were: female sex OR 0.68 (95% CI 0.46-0.99), obesity OR 0.50 (95%CI 0.34-0.74) and IMV OR 0.42 (95%CI 0.23-0.76). Cluster analysis (n=1636) corroborated the previously reported four clusters: very severe, severe, moderate/cognitive, mild relating to the severity of physical, mental health and cognitive impairments at five months in a larger sample. There was elevation of inflammatory mediators of tissue damage and repair in both the very severe and the moderate/cognitive clusters compared to the mild cluster including interleukin-6 which was elevated in both comparisons. Overall, there was a substantial deficit in median (IQR) EQ5D-5L utility index from pre-COVID (retrospective assessment) 0.88 (0.74-1.00), five months 0.74 (0.60-0.88) to one year: 0.74 (0.59-0.88), with minimal improvements across all outcome measures at one-year after discharge in the whole cohort and within each of the four clusters. Interpretation The sequelae of a hospital admission with COVID-19 remain substantial one year after discharge across a range of health domains with the minority in our cohort feeling fully recovered. Patient perceived health-related quality of life remains reduced at one year compared to pre-hospital admission. Systematic inflammation and obesity are potential treatable traits that warrant further investigation in clinical trials.', 'PURPOSE OF REVIEW: To review the current state of knowledge on the newly proposed COVID Stress Syndrome. RECENT FINDINGS: The syndrome consists of five inter-correlated elements: (a) fear of SARSCoV2 infection and fear of coming into contact with objects or surfaces contaminated with the coronavirus; (b) fear of socio-economic impacts of the pandemic; (c) fear of foreigners for fear that they are infected; (d) pandemic-related compulsive checking and reassurance-seeking; and (e) pandemic-related traumatic stress symptoms. A severe form of the syndrome, characterized by clinically significant distress and impairment in functioning, is the COVID Stress Disorder, which is regarded as a pandemic-related adjustment disorder. Several treatment options exist but further research is needed. SUMMARY: Research during the COVID-19 pandemic has identified a pandemic-related adjustment disorder. The diagnosis of COVID Stress Syndrome should be made only after ruling out other disorders that could potentially account for the pattern of symptoms, such as obsessive-compulsive disorder and posttraumatic stress disorder. Further studies are needed to investigate the long-term course of the syndrome. Similar adjustment disorders may arise in future pandemics. Accordingly, understanding the COVID Stress Syndrome may facilitate efforts to understand and treat psychopathology in future pandemics.', 'Objective: To estimate the prevalence of long COVID in children and adolescents and identify the full spectrum of signs and symptoms present after acute SARS-CoV-2 infection. Methods: Two independent investigators searched PubMed and Embase in order to identify observational studies that met the following criteria: 1) a minimum of 30 patients, 2) ages ranged from 0 to 18 years, 3) published in English, 4) published before February 10th, 2022, and 5) meets the National Institute for Healthcare Excellence (NICE) definition of long COVID, which consists of both ongoing (4 to 12 weeks) and post COVID 19 ([â¥]12 weeks) symptoms. For COVID symptoms reported in two or more studies, random-effects meta-analyses were performed using the MetaXL software to estimate the pooled prevalence, and Review Manager (RevMan) software 5.4 was utilized to estimate the Odds Ratios (ORs) with a 95% confidence interval (CI). Heterogeneity was assessed using I2 statistics. The Preferred Reporting Items for Systematic Reviewers and Meta-analysis (PRISMA) reporting guideline was followed (registration PROSPERO CRD42021275408). Results: The literature search yielded 68 articles for long COVID in children and adolescents. After screening, 21 studies met the inclusion criteria and were included in the systematic review and meta-analyses. A total of 80,071 children and adolescents with COVID-19 were included. The prevalence of long COVID was 25.24% (95% CI 18.17-33.02), and the most prevalent clinical manifestations were mood symptoms (16.50%; 95% CI 7.37-28.15), fatigue (9.66%; 95% CI 4.45-16.46), and sleep disorders (8.42%; 95% CI 3.41-15.20). When compared to controls, children infected by SARS-CoV-2 had a higher risk of persistent dyspnea (OR 2.69 95%CI 2.30-3.14), anosmia/ageusia (OR 10.68, 95%CI 2.48, 46.03), and/or fever (OR 2.23, 95%CI 1.22-4.07). The main limitation of these meta-analyses is the probability of bias, which includes lack of standardized definitions, recall, selection, misclassification, nonresponse and/or loss of follow-up, and the high level of heterogeneity. Conclusion: These meta-analyses provide an overview of the broad symptomatology of long COVID in minors, which may help improve management, rehabilitation programs, and future development of guidelines and therapeutic research for COVID-19.', 'BACKGROUND: Approximately 10% of patients with Covid-19 experience symptoms beyond 3â4 weeks. Patients call this âlong Covidâ. We sought to document such patientsâ lived experience, including accessing and receiving healthcare and ideas for improving services. METHODS: We held 55 individual interviews and 8 focus groups (n = 59) with people recruited from UK-based long Covid patient support groups, social media and snowballing. We restricted some focus groups to health professionals since they had already self-organised into online communities. Participants were invited to tell their stories and comment on othersâ stories. Data were audiotaped, transcribed, anonymised and coded using NVIVO. Analysis incorporated sociological theories of illness, healing, peer support, clinical relationships, access, and service redesign. RESULTS: Of 114 participants aged 27â73 years, 80 were female. Eighty-four were White British, 13 Asian, 8 White Other, 5 Black, and 4 mixed ethnicity. Thirty-two were doctors and 19 other health professionals. Thirty-one had attended hospital, of whom 8 had been admitted. Analysis revealed a confusing illness with many, varied and often relapsing-remitting symptoms and uncertain prognosis; a heavy sense of loss and stigma; difficulty accessing and navigating services; difficulty being taken seriously and achieving a diagnosis; disjointed and siloed care (including inability to access specialist services); variation in standards (e.g. inconsistent criteria for seeing, investigating and referring patients); variable quality of the therapeutic relationship (some participants felt well supported while others felt âfobbed offâ); and possible critical events (e.g. deterioration after being unable to access services). Emotionally significant aspects of participantsâ experiences informed ideas for improving services. CONCLUSION: Suggested quality principles for a long Covid service include ensuring access to care, reducing burden of illness, taking clinical responsibility and providing continuity of care, multi-disciplinary rehabilitation, evidence-based investigation and management, and further development of the knowledge base and clinical services. TRIAL REGISTRATION: NCT04435041.', 'The perspectives of people with lived experience of any condition being researched must actively inform the research questions asked and the way in which we go about answering them. The experience of Long Covid gives a contemporary example of how working together with patients is integral to medical research.', 'About 10% of people infected by severe acute respiratory syndrome coronavirus 2 experience post COVID-19 disease. We analysed data from 968 adult patients (5350 person-months) with a confirmed infection enroled in the ComPaRe long COVID cohort, a disease prevalent prospective e-cohort of such patients in France. Day-by-day prevalence of post COVID-19 symptoms was determined from patientsâ responses to the Long COVID Symptom Tool, a validated self-reported questionnaire assessing 53 symptoms. Among patients symptomatic after 2 months, 85% still reported symptoms one year after their symptom onset. Evolution of symptoms showed a decreasing prevalence over time for 27/53 symptoms (e.g., loss of taste/smell); a stable prevalence over time for 18/53 symptoms (e.g., dyspnoea), and an increasing prevalence over time for 8/53 symptoms (e.g., paraesthesia). The disease impact on patientsâ lives began increasing 6 months after onset. Our results are of importance to understand the natural history of post COVID-19 disease.', 'BACKGROUND: This multicentre study aimed to provide a qualitative and consensual description of brain hypometabolism observed through the visual analysis of (18)F-FDG PET images of patients with suspected neurological long COVID, regarding the previously reported long-COVID hypometabolic pattern involving hypometabolism in the olfactory bulbs and other limbic/paralimbic regions, as well as in the brainstem and cerebellum. METHODS: From the beginning of August 2021 to the end of October 2021, the brain (18)F-FDG PET scans of patients referred for suspected neurological long COVID with positive reverse transcription polymerase chain reaction (RT-PCR) and/or serology tests for SARS-CoV-2 infection were retrospectively reviewed in three French nuclear medicine departments (143 patients; 47.4 years old Â± 13.6; 98 women). Experienced nuclear physicians from each department classified brain (18)F-FDG PET scans according to the same visual interpretation analysis as being normal, mildly to moderately (or incompletely) affected, or otherwise severely affected within the previously reported long-COVID hypometabolic pattern. RESULTS: On the 143 brain (18)F-FDG PET scans performed during this 3-month period, 53% of the scans were visually interpreted as normal, 21% as mildly to moderately or incompletely affected, and 26% as severely affected according to the COVID hypometabolic pattern. On average, PET scans were performed at 10.9 months from symptom onset (Â± 4.8). Importantly, this specific hypometabolic pattern was similarly identified in the three nuclear medicine departments. Typical illustrative examples are provided to help nuclear physicians interpret long-COVID profiles. CONCLUSION: The proposed PET metabolic pattern is easily identified upon visual interpretation in clinical routine for approximately one half of patients with suspected neurological long COVID, requiring special consideration for frontobasal paramedian regions, the brainstem and the cerebellum, and certainly further adapted follow-up and medical care, while the second half of patients have normal brain PET metabolism on average 10.9 months from symptom onset.', 'Although COVID-19 is predominantly a respiratory disease, it is known to affect multiple organ systems. In this article, we highlight the impact of SARS-CoV-2 (the coronavirus causing COVID-19) on the central nervous system as there is an urgent need to understand the longitudinal impacts of COVID-19 on brain function, behaviour and cognition. Furthermore, we address the possibility of intergenerational impacts of COVID-19 on the brain, potentially via both maternal and paternal routes. Evidence from preclinical models of earlier coronaviruses has shown direct viral infiltration across the bloodâbrain barrier and indirect secondary effects due to other organ pathology and inflammation. In the most severely ill patients with pneumonia requiring intensive care, there appears to be additional severe inflammatory response and associated thrombophilia with widespread organ damage, including the brain. Maternal viral (and other) infections during pregnancy can affect the offspring, with greater incidence of neurodevelopmental disorders, such as autism, schizophrenia and epilepsy. Available reports suggest possible vertical transmission of SARS-CoV-2, although longitudinal cohort studies of such offspring are needed. The impact of paternal infection on the offspring and intergenerational effects should also be considered. Research targeted at mechanistic insights into all aspects of pathogenesis, including neurological, neuropsychiatric and haematological systems alongside pulmonary pathology, will be critical in informing future therapeutic approaches. With these future challenges in mind, we highlight the importance of national and international collaborative efforts to gather the required clinical and preclinical data to effectively address the possible long-term sequelae of this global pandemic, particularly with respect to the brain and mental health.', 'BACKGROUND: There is disagreement about the level of asymptomatic severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. We conducted a living systematic review and meta-analysis to address three questions: (1) Amongst people who become infected with SARS-CoV-2, what proportion does not experience symptoms at all during their infection? (2) Amongst people with SARS-CoV-2 infection who are asymptomatic when diagnosed, what proportion will develop symptoms later? (3) What proportion of SARS-CoV-2 transmission is accounted for by people who are either asymptomatic throughout infection or presymptomatic? METHODS AND FINDINGS: We searched PubMed, Embase, bioRxiv, and medRxiv using a database of SARS-CoV-2 literature that is updated daily, on 25 March 2020, 20 April 2020, and 10 June 2020. Studies of people with SARS-CoV-2 diagnosed by reverse transcriptase PCR (RT-PCR) that documented follow-up and symptom status at the beginning and end of follow-up or modelling studies were included. One reviewer extracted data and a second verified the extraction, with disagreement resolved by discussion or a third reviewer. Risk of bias in empirical studies was assessed with an adapted checklist for case series, and the relevance and credibility of modelling studies were assessed using a published checklist. We included a total of 94 studies. The overall estimate of the proportion of people who become infected with SARS-CoV-2 and remain asymptomatic throughout infection was 20% (95% confidence interval [CI] 17â25) with a prediction interval of 3%â67% in 79 studies that addressed this review question. There was some evidence that biases in the selection of participants influence the estimate. In seven studies of defined populations screened for SARS-CoV-2 and then followed, 31% (95% CI 26%â37%, prediction interval 24%â38%) remained asymptomatic. The proportion of people that is presymptomatic could not be summarised, owing to heterogeneity. The secondary attack rate was lower in contacts of people with asymptomatic infection than those with symptomatic infection (relative risk 0.35, 95% CI 0.10â1.27). Modelling studies fit to data found a higher proportion of all SARS-CoV-2 infections resulting from transmission from presymptomatic individuals than from asymptomatic individuals. Limitations of the review include that most included studies were not designed to estimate the proportion of asymptomatic SARS-CoV-2 infections and were at risk of selection biases; we did not consider the possible impact of false negative RT-PCR results, which would underestimate the proportion of asymptomatic infections; and the database does not include all sources. CONCLUSIONS: The findings of this living systematic review suggest that most people who become infected with SARS-CoV-2 will not remain asymptomatic throughout the course of the infection. The contribution of presymptomatic and asymptomatic infections to overall SARS-CoV-2 transmission means that combination prevention measures, with enhanced hand hygiene, masks, testing tracing, and isolation strategies and social distancing, will continue to be needed.', 'The impact of SARS-CoV-2 on the olfactory pathway was studied over several time points using Syrian golden hamsters. We found an incomplete recovery of the olfactory sensory neurons, prolonged activation of glial cells in the olfactory bulb, and a decrease in the density of dendritic spines within the hippocampus. These data may be useful for elucidating the mechanism underlying long-lasting olfactory dysfunction and cognitive impairment as a post-acute COVID-19 syndrome.', 'Myalgic Encephalomyelitis/Chronic Fatigue Syndrome (ME/CFS) is a common and disabling disorder primarily characterized by persistent fatigue and exercise intolerance, with associated sleep disturbances, autonomic dysfunction, and cognitive problems. The causes of ME/CFS are not well understood but may coincide with immune and inflammatory responses following viral infections. During the current SARS-CoV2 coronavirus pandemic, ME/CFS has been increasingly reported to overlap with persistent âlong COVIDâ symptoms, also called the post-acute sequelae of COVID-19 (PASC). Given the prominence of activity and sleep problems in ME/CFS, circadian rhythm disruption has been examined as a contributing factor in ME/CFS. While these studies of circadian rhythms have been pursued for decades, evidence linking circadian rhythms to ME/CFS remains inconclusive. A major limitation of older chronobiology studies of ME/CFS was the unavailability of modern molecular methods to study circadian rhythms and incomplete understanding of circadian rhythms outside the brain in peripheral organ systems. Major methodological and conceptual advancements in chronobiology have since been made. Over the same time, biomarker research in ME/CFS has progressed. Together, these new developments may justify renewed interest in circadian rhythm research in ME/CFS. Presently, we review ME/CFS from the perspective of circadian rhythms, covering both older and newer studies that make use of modern molecular methods. We focus on transforming growth factor beta (TGFB), a cytokine that has been previously associated with ME/CFS and has an important role in circadian rhythms, especially in peripheral cells. We propose that disrupted TGFB signaling in ME/CFS may play a role in disrupting physiological rhythms in sleep, activity, and cognition, leading to the insomnia, energy disturbances, cognition problems, depression, and autonomic dysfunction associated with ME/CFS. Since SARS-like coronavirus infections cause persistent changes in TGFB and previous coronavirus outbreaks have caused ME/CFS-like syndromes, chronobiological considerations may have immediate implications for understanding ME/CFS in the context of the COVID-19 pandemic and possibly suggest new avenues for therapeutic interventions.', 'Neuropsychiatric complications associated with coronavirus disease 2019 caused by the Coronavirus SARS-CoV-2 (COVID-19) are increasingly appreciated. While most studies have focused on severely affected individuals during acute infection it remains unclear whether mild COVID-19 results in neurocognitive deficits in young patients. Here, we established a screening approach to detect cognitive deficiencies in post-COVID-19 patients. In this cross-sectional study, we recruited 18 mostly young patients 20 to 105 days (median 85 days) after recovery from mild to moderate disease who visited our outpatient clinic for post-COVID-19 care. Notably, 14 (78%) patients reported sustained mild cognitive deficits and performed worse in the Modified Telephone Interview for Cognitive Status (TICS-M) screening test for mild cognitive impairment compared to 10 age-matched healthy controls. While short-term memory, attention and concentration were particularly affected by COVID-19, screening results did not correlate with hospitalisation, treatment, viremia or acute inflammation. Additionally, TICS-M scores did not correlate with depressed mood or fatigue. In two severely affected patients we excluded structural or other inflammatory causes by magnetic resonance imaging, serum and cerebrospinal fluid analyses. Together, our results demonstrate that sustained subclinical cognitive impairments might be a common complication after recovery from COVID-19 in young adults, regardless of clinical course that were unmasked by our diagnostic approach.', 'Background COVID 19 infection has a similar clinical spectrum of disease presentation such as SARS and MERS in the past. These led to the assumption of the possibility to treat COVID 19 infection with antivirals which had been used to treat SARS and MERS. Methods A retrospective analysis was done on the data of SEV COVID Trial in symptomatic adult patients of COVID 19 infection with objectives to explore whether ribavirin antiviral combinations reduces the need of both noninvasive and invasive ventilators in treatment of COVID 19 infections. Results The patients were categorized as Cohort A consisting of 40 patients and Cohort B of 61 patients as Cohort A being the group of patients who received the standard therapy and Cohort B the group of patients who received the ribavirin combination therapy. Conclusion The study concluded that there was no statistically significant difference in regard to the need of noninvasive ventilation and invasive ventilation and also the development of multiorgan dysfunction in between the two Cohorts. Also, with progress of time, the proportion of patients with single organ dysfunctions in the two cohorts showed gradual recovery without any statistically significant differences.', \"Studies examining the effect of social isolation on cognitive function typically involve older adults and/or specialist groups (e.g., expeditions). We considered the effects of COVIDâ19âinduced social isolation on cognitive function within a representative sample of the general population. We additionally considered how participants âshieldingâ due to underlying health complications, or living alone, performed. We predicted that performance would be poorest under strictest, mostâisolating conditions. At five timepoints over 13 weeks, participants (N = 342; aged 18â72 years) completed online tasks measuring attention, memory, decisionâmaking, timeâestimation, and learning. Participants indicated their mood as âlockdownâ was eased. Performance typically improved as opportunities for social contact increased. Interactions between participant subâgroups and timepoint demonstrated that performance was shaped by individuals' social isolation levels. Social isolation is linked to cognitive decline in the absence of ageing covariates. The impact of social isolation on cognitive function should be considered when implementing prolonged pandemicârelated restrictive conditions.\", \"BACKGROUND: The COVID-19 pandemic has had a detrimental effect on the mental health of older adults living in nursing homes. Very few studies have examined the effects of Internet-based Cognitive Behavioral Therapy (ICBT) on older adults living in nursing homes during the pandemic. We conducted a feasibility study using a single-group design, to explore the effectiveness of ICBT on psychological distress in 137 older adults (without cognitive impairment) from 8 nursing homes in 4 southeast cities in China, between January and March 2020. METHODS: Symptoms of depression, anxiety, general psychological distress, and functional disability were measured at baseline, post-treatment (5 weeks) and at a 1-month follow-up. Mixed-effects model was used to assess the effects of ICBT. RESULTS: Statistically significant changes with large effect sizes were observed from pre- to post-treatment on the PHQ-9 (p < .001, Cohen's d = 1.74), GAD-7 (p < .001, d = 1.71), GDS (p < .001, d = 1.30), K-10 (p < .001, d = 1.93), and SDS (p < .001, d = 2.03). Furthermore, improvements in treatment outcomes were sustained at 1-month follow-up, and high levels of adherence and satisfaction were indicated. CONCLUSION: ICBT was effective in reducing psychological distress in older adults without cognitive impairments living in nursing homes during the COVID-19 pandemic. Thus, it could be applied in improving the mental health of this vulnerable group during the pandemic.\", 'BACKGROUND: The full range of long-term health consequences of COVID-19 in patients who are discharged from hospital is largely unclear. The aim of our study was to comprehensively compare consequences between 6 months and 12 months after symptom onset among hospital survivors with COVID-19. METHODS: We undertook an ambidirectional cohort study of COVID-19 survivors who had been discharged from Jin Yin-tan Hospital (Wuhan, China) between Jan 7 and May 29, 2020. At 6-month and 12-month follow-up visit, survivors were interviewed with questionnaires on symptoms and health-related quality of life (HRQoL), and received a physical examination, a 6-min walking test, and laboratory tests. They were required to report their health-care use after discharge and work status at the 12-month visit. Survivors who had completed pulmonary function tests or had lung radiographic abnormality at 6 months were given the corresponding tests at 12 months. Non-COVID-19 participants (controls) matched for age, sex, and comorbidities were interviewed and completed questionnaires to assess prevalent symptoms and HRQoL. The primary outcomes were symptoms, modified British Medical Research Council (mMRC) score, HRQoL, and distance walked in 6 min (6MWD). Multivariable adjusted logistic regression models were used to evaluate the risk factors of 12-month outcomes. FINDINGS: 1276 COVID-19 survivors completed both visits. The median age of patients was 59Â·0 years (IQR 49Â·0â67Â·0) and 681 (53%) were men. The median follow-up time was 185Â·0 days (IQR 175Â·0â198Â·0) for the 6-month visit and 349Â·0 days (337Â·0â361Â·0) for the 12-month visit after symptom onset. The proportion of patients with at least one sequelae symptom decreased from 68% (831/1227) at 6 months to 49% (620/1272) at 12 months (p<0Â·0001). The proportion of patients with dyspnoea, characterised by mMRC score of 1 or more, slightly increased from 26% (313/1185) at 6-month visit to 30% (380/1271) at 12-month visit (p=0Â·014). Additionally, more patients had anxiety or depression at 12-month visit (26% [331/1271] at 12-month visit vs 23% [274/1187] at 6-month visit; p=0Â·015). No significant difference on 6MWD was observed between 6 months and 12 months. 88% (422/479) of patients who were employed before COVID-19 had returned to their original work at 12 months. Compared with men, women had an odds ratio of 1Â·43 (95% CI 1Â·04â1Â·96) for fatigue or muscle weakness, 2Â·00 (1Â·48â2Â·69) for anxiety or depression, and 2Â·97 (1Â·50â5Â·88) for diffusion impairment. Matched COVID-19 survivors at 12 months had more problems with mobility, pain or discomfort, and anxiety or depression, and had more prevalent symptoms than did controls. INTERPRETATION: Most COVID-19 survivors had a good physical and functional recovery during 1-year follow-up, and had returned to their original work and life. The health status in our cohort of COVID-19 survivors at 12 months was still lower than that in the control population. FUNDING: Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences, the National Natural Science Foundation of China, the National Key Research and Development Program of China, Major Projects of National Science and Technology on New Drug Creation and Development of Pulmonary Tuberculosis, the China Evergrande Group, Jack Ma Foundation, Sino Biopharmaceutical, Ping An Insurance (Group), and New Sunshine Charity Foundation.', 'Background: The coronavirus disease 2019 (COVID-19) has spread worldwide with alarming levels of spread and severity. The distribution of angiotensin converting enzyme 2 (ACE2) and transmembrane protease serine 2 (TMPRSS2) from bioinformatics evidence, the autopsy report for COVID-19 and the published study on sperm quality indicated COVID-19 could have a negative impact on male fertility. However, whether the negative impact of COVID-19 on male fertility is persistent remains unknown, which requires long-term follow-up investigation. Methods: Semen samples were collected from 36 male COVID-19 patients with a median recovery time of 177.5 days and 45 control subjects. Then, analysis of sperm quality and alterations of total sperm number with recovery time were performed. Results: There was no significant difference in semen parameters between male recovered patients and control subjects. And the comparisons of semen parameters between first follow-up and second follow-up revealed no significant difference. In addition, we explored the alterations of sperm count with recovery time. It showed that the group with recovery time of â¥120 and <150 days had a significantly lower total sperm number than controls while the other two groups with recovery time of â¥150 days displayed no significance with controls, and total sperm number showed a significant decline after a recovery time of 90 days and an improving trend after a recovery time of about 150 days. Conclusions: The sperm quality of COVID-19 recovered patients improved after a recovery time of nearly half a year, while the total sperm number showed an improvement after a recovery time of about 150 days. COVID-19 patients should pay close attention to the quality of semen, and might be considered to be given medical interventions if necessary within about two months after recovery, in order to improve the fertility of male patients as soon as possible.', 'PURPOSE: To ascertain delirium prevalence and outcomes in COVID-19. METHODS: We conducted a point-prevalence study in a cohort of COVID-19 inpatients at University College Hospital. Delirium was defined by DSM-IV criteria. The primary outcome was all-cause mortality at 4 weeks; secondary outcomes were physical and cognitive function. RESULTS: In 71 patients (mean age 61, 75% men), 31 (42%) had delirium, of which only 12 (39%) had been recognised by the clinical team. At 4 weeks, 20 (28%) had died, 26 (36%) were interviewed by telephone and 21 (30%) remained as inpatients. Physical function was substantially worse in people after delirium â 50 out of 166 points (95% CI â 83 to â 17, p = 0.01). Mean cognitive scores at follow-up were similar and delirium was not associated with mortality in this sample. CONCLUSIONS: Our findings indicate that delirium is common, yet under-recognised. Delirium is associated with functional impairments in the medium term. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this article (10.1007/s41999-020-00353-8) contains supplementary material, which is available to authorized users.', 'Preservation of cognitive function is an important outcome in oncology. Optimal patient management requires an understanding of cognitive effects of the disease and its treatment and an efficacious approach to assessment and management of cognitive dysfunction, including selection of treatments to minimize the risk of cognitive impairment. Awareness is increasing of the potentially detrimental effects of cancer-related cognitive dysfunction on functional independence and quality of life. Prostate cancer occurs most often in older men, who are more likely to develop cognitive dysfunction than younger individuals; this population may be particularly vulnerable to treatment-related cognitive disorders. Prompt identification of treatment-induced cognitive dysfunction is a crucial aspect of effective cancer management. We review the potential etiologies of cognitive decline in patients with prostate cancer, including the potential role of androgen receptor pathway inhibitors; commonly used tools for assessing cognitive function validated in metastatic castration-resistant prostate cancer and adopted in non-metastatic castration-resistant prostate cancer trials; and strategies for management of cognitive symptoms. Many methods are currently used to assess cognitive function. The prevalence and severity of cognitive dysfunction vary according to the instruments and criteria applied. Consensus on the definition of cognitive dysfunction and on the most appropriate approaches to quantify its extent and progression in patients treated for prostate cancer is lacking. Evidence-based guidance on the appropriate tools and time to assess cognitive function in patients with prostate cancer is required.', \"The exponential growth in international student mobility since the 1990s brings issues of travel health preparedness in overseas students and the impact of studying in a foreign country on their health to the fore. The direction of travel for much 'education first' educational tourism is from less- to more-economically developed countries, yet the existing literature tends to focus on the health-related expectations, precautions and behaviours of students travelling in the other direction. We explore the health implications of the international sojourn for students in UK higher education from various developing countries, and attendant risks such as the translocation of disease (a concern elevated by the COVID-19 pandemic). Drawing on, fusing and extending ideas from research on education and health mobilities, we examine students' experiences beyond the much-discussed first few weeks after arrival which is typically understood in terms of culture shock. Using narrative research and interviews with a purposive sample of students from ten countries/territories to establish their travel health preparations and perceptions, we reveal that they made extensive use of non-medical sources of advice including family and friends, the internet and study abroad agents. When they did become ill it was an isolating, distressing and frustrating experience due to delays in obtaining a doctor's appointment and the lack of social support away from home. Of some concern, these students perceived a low risk to traveling to and studying in the UK, and had given little thought to the possibility of inadvertently transmitting disease across borders. These findings will inform our understanding of international students' health, and have the potential to shape related policy and practice in origin and destination countries alike.\", 'BACKGROUND A substantial proportion of persons who develop COVID-19 report persistent symptoms after acute illness. Various pathophysiologic mechanisms have been implicated in the pathogenesis of postacute sequelae of SARS-CoV-2 infection (PASC). OBJECTIVE To characterize medical sequelae and persistent symptoms after recovery from COVID-19 in a cohort of disease survivors and controls. DESIGN Cohort study. (ClinicalTrials.gov: NCT04411147). SETTING National Institutes of Health Clinical Center, Bethesda, Maryland. PARTICIPANTS Self-referred adults with laboratory-documented SARS-CoV-2 infection who were at least 6 weeks from symptom onset were enrolled regardless of presence of PASC. A control group comprised persons with no history of COVID-19 or serologic evidence of SARS-CoV-2 infection, recruited regardless of their current health status. Both groups were enrolled over the same period and from the same geographic area. MEASUREMENTS All participants had the same evaluations regardless of presence of symptoms, including physical examination, laboratory tests and questionnaires, cognitive function testing, and cardiopulmonary evaluation. A subset also underwent exploratory immunologic and virologic evaluations. RESULTS 189 persons with laboratory-documented COVID-19 (12% of whom were hospitalized during acute illness) and 120 antibody-negative control participants were enrolled. At enrollment, symptoms consistent with PASC were reported by 55% of the COVID-19 cohort and 13% of control participants. Increased risk for PASC was noted in women and those with a history of anxiety disorder. Participants with findings meeting the definition of PASC reported lower quality of life on standardized testing. Abnormal findings on physical examination and diagnostic testing were uncommon. Neutralizing antibody levels to spike protein were negative in 27% of the unvaccinated COVID-19 cohort and none of the vaccinated COVID-19 cohort. Exploratory studies found no evidence of persistent viral infection, autoimmunity, or abnormal immune activation in participants with PASC. LIMITATIONS Most participants with COVID-19 had mild to moderate acute illness that did not require hospitalization. The prevalence of reported PASC was likely overestimated in this cohort because persons with PASC may have been more motivated to enroll. The study did not capture PASC that resolved before enrollment. CONCLUSION A high burden of persistent symptoms was observed in persons after COVID-19. Extensive diagnostic evaluation revealed no specific cause of reported symptoms in most cases. Antibody levels were highly variable after COVID-19. PRIMARY FUNDING SOURCE Division of Intramural Research, National Institute of Allergy and Infectious Diseases.', 'BACKGROUND: Despite emerging evidence about the association between social frailty and cognitive impairment, little is known about the role of executive function in this interplay, and whether the coexistence of social frailty and cognitive impairment predisposes to adverse health outcomes in healthy community-dwelling older adults. OBJECTIVES: We aim to examine independent associations between social frailty with the MMSE and FAB, and to determine if having both social frailty and cognitive impairment is associated with worse health outcomes than either or neither condition. METHODS: We studied 229 cognitively intact and functionally independent community-dwelling older adults (mean age= 67.2Â±7.43). Outcome measures comprise physical activity; physical performance and frailty; geriatric syndromes; life space and quality of life. We compared Chinese Mini Mental State Examination (CMMSE) and Chinese Frontal Assessment Battery (FAB) scores across the socially non-frail, socially pre-frail and socially frail. Participants were further recategorized into three subgroups (neither, either or both) based on presence of social frailty and cognitive impairment. Cognitive impairment was defined as a score below the educational adjusted cut-offs in either CMMSE or FAB. We performed logistic regression adjusted for significant covariates and mood to examine association with outcomes across the three subgroups. RESULTS: Compared with CMMSE, Chinese FAB scores significantly decreased across the social frailty spectrum (p<0.001), suggesting strong association between executive function with social frailty. We derived three subgroups relative to relationship with socially frailty and executive dysfunction: (i) Neither, N=140(61.1%), (ii) Either, N=79(34.5%), and (iii) Both, N=10(4.4%). Compared with neither or either subgroups, having both social frailty and executive dysfunction was associated with anorexia (OR=4.79, 95% CI= 1.04â22.02), near falls and falls (OR= 5.23, 95% CI= 1.10â24.90), lower life-space mobility (odds ratio, OR=9.80, 95% CI=2.07â46.31) and poorer quality of life (OR= 13.2, 95% CI= 2.38â73.4). CONCLUSION: Our results explicated the association of executive dysfunction with social frailty, and their synergistic relationship independent of mood with geriatric syndromes, decreased life space and poorer quality of life. In light of the current COVID-19 pandemic, the association between social frailty and executive dysfunction merits further study as a possible target for early intervention in relatively healthy older adults.', 'As the spread of severe acute respiratory syndrome coronavirus 2 (SARSâCoVâ2) continues to surge worldwide, our knowledge of coronavirus disease 2019 (COVIDâ19) is rapidly expanding. Although most COVIDâ19 patients recover within weeks of symptom onset, some experience lingering symptoms that last for months (âlong COVIDâ19â). Early reports of COVIDâ19 sequelae, including cardiovascular, pulmonary, and neurological conditions, have raised concerns about the longâterm effects of COVIDâ19, especially in hardâhit communities. It is becoming increasingly evident that cancer patients are more susceptible to SARSâCoVâ2 infection and are at a higher risk of severe COVIDâ19 than the general population. Nevertheless, whether long COVIDâ19 increases the risk of cancer in those with no prior malignancies, remains unclear. Given, the disproportionate impact of the disease on the African American community, yet another unanswered question is whether racial disparities are to be expected in COVIDâ19 sequelae. Herein, we propose that long COVIDâ19 may predispose recovered patients to cancer development and accelerate cancer progression. This hypothesis is based on growing evidence of the ability of SARSâCoVâ2 to modulate oncogenic pathways, promote chronic lowâgrade inflammation, and cause tissue damage. Comprehensive studies are urgently required to elucidate the effects of long COVIDâ19 on cancer susceptibility.', 'Background: Sarcopenia is defined as a progressive age-related loss in muscle mass and strength affecting physical performance. It is associated with many negative outcomes including falls, disability, cognitive decline, and mortality. Protein enriched diet and resistance training have shown to improve muscle strength and function but there is limited evidence on impact of dual-task exercise in possible sarcopenia. Objective: To evaluate impact of community-based dual-task exercise on muscle strength and physical function in possible sarcopenia defined by either slow gait (SG) or poor handgrip strength (HGS). The secondary aims include effect on cognition, frailty, falls, social isolation, and perceived health. Methods: Community-dwelling older adults â¥60 years old were recruited from screening program intended to identify seniors at risk, and invited to participate in dual-task exercise program called HAPPY (Healthy Aging Promotion Program for You). One hundred and eleven participants with possible sarcopenia completed 3 months follow-up. Questionnaire was administered on demographics, frailty, sarcopenia, falls, perceived health, social network, functional, and cognitive status. Physical performance included assessment of HGS, gait speed, and Short Physical Performance Battery test (SPPB). Results: The mean age of the Exercise group was 75.9 years old and 73.0% were women. The Exercise group had more female (73.0 vs. 47.5%), were older (75.9 vs. 72.5 years old), had higher prevalence of falls (32.4 vs. 15.0%), lower BMI (23.7 vs. 25.8), and education (4.0 vs. 7.2 years). The gait speed of the Exercise group increased significantly with significant reduction in the prevalence of SG and poor HGS. All components of SPPB as well as the total score increased significantly while the prevalence of pre-frailty and falls dropped by half. The risk of social isolation reduced by 25% with significant improvement in perceived health and cognition in the Exercise group. Significant impact on improvement gait speed and SPPB persisted after adjustment for baseline factors. Conclusion: Dual-task exercise program is effective in improving gait speed, SPPB score, and reducing the prevalence of poor HGS with significant improvement in perceived health, cognition, and reduction in falls and frailty. Future prospective randomized control trials are needed to evaluate the effectiveness of dual-task interventions in reversing sarcopenia.', 'Background Prospective and longitudinal data on pulmonary injury over one year after acute coronavirus disease 2019 (COVID-19) are sparse. Research question: With this study, we aim to investigate pulmonary outcome following SARS-CoV-2 infection including pulmonary function, computed chest tomography, respiratory symptoms and quality of life over 12 months. Study design and Methods 180 patients after acute COVID-19 were enrolled into a single-centre, prospective observational study and examined 6 weeks, 3, 6 and 12 months after onset of COVID-19 symptoms. Chest CT-scans, pulmonary function and symptoms assessed by St. Georges Respiratory Questionnaire were used to evaluate objective and subjective respiratory limitations. Patients were stratified according to acute COVID-19 disease severity. Results Of 180 patients enrolled, 42/180 were not hospitalized during acute SARS-CoV-2 infection, 29/180 were hospitalized without need for oxygen, 43/180 with need for low-flow and 24/180 with high-flow oxygen, 26/180 required invasive mechanical ventilation and 16/180 were treated with ECMO. After acute COVID-19, pulmonary restriction and reduced carbon monoxide diffusion capacity was associated with disease severity after the acute phase and improved over 12 months except for those requiring ECMO treatment. Patients with milder disease showed a predominant reduction of ventilated area instead of simple restriction. The CT score of lung involvement in the acute phase increased significantly with COVID-19 severity and was associated with restriction and reduction in diffusion capacity in follow-up. Respiratory symptoms improved for patients in higher severity groups during follow-up, but not for patients with mild initially disease. Interpretation Severity of respiratory failure during COVID-19 correlates with the degree of pulmonary function impairment and respiratory quality of life in the year after acute infection. Patients with mild vs. severe disease show different patterns of lung involvement and symptom resolution.', 'Emerging as a new epidemic, long COVID or post-acute sequelae of coronavirus disease 2019 (COVID-19), a condition characterized by the persistence of COVID-19 symptoms beyond 3 months, is anticipated to substantially alter the lives of millions of people globally. Cardiopulmonary symptoms including chest pain, shortness of breath, fatigue, and autonomic manifestations such as postural orthostatic tachycardia are common and associated with significant disability, heightened anxiety, and public awareness. A range of cardiovascular (CV) abnormalities has been reported among patients beyond the acute phase and include myocardial inflammation, myocardial infarction, right ventricular dysfunction, and arrhythmias. Pathophysiological mechanisms for delayed complications are still poorly understood, with a dissociation seen between ongoing symptoms and objective measures of cardiopulmonary health. COVID-19 is anticipated to alter the long-term trajectory of many chronic cardiac diseases which are abundant in those at risk of severe disease. In this review, we discuss the definition of long COVID and its epidemiology, with an emphasis on cardiopulmonary symptoms. We further review the pathophysiological mechanisms underlying acute and chronic CV injury, the range of post-acute CV sequelae, and impact of COVID-19 on multiorgan health. We propose a possible model for referral of post-COVID-19 patients to cardiac services and discuss future directions including research priorities and clinical trials that are currently underway to evaluate the efficacy of treatment strategies for long COVID and associated CV sequelae.', 'Vaccines have been shown to be extremely effective in preventing COVID-19 hospitalizations and deaths. However, a question remains whether vaccine breakthrough cases can still lead to Post-Acute Sequelae of SARS-CoV-2 (PASC), also known as Long Covid. To address this question, the Survivor Corps group, a grassroots COVID-19 organization focused on patient support and research, posted a poll to its 169,900 members that asked about breakthrough cases, Long Covid, and hospitalizations. 1,949 people who self-report being fully vaccinated have responded to date. While robust data are needed in a larger, unbiased sample to extrapolate rates to the population, we analyzed the results of this public poll to determine what people were reporting regarding Long Covid after breakthrough infection and to prompt discussion of how breakthrough cases are measured. The poll was posted in the Survivor Corps Facebook group (~169,900 members). Of the 1,949 participants who responded to the poll, 44 reported a symptomatic breakthrough case and 24 of those reported that the case led to symptoms of Long Covid. 1 of these 24 cases was reported to have led to hospitalization in addition to Long Covid.', 'BACKGROUND: Whereas several predictors of COVID-19 vaccine hesitancy have been examined, the role of cognitive function following the widely publicised development of an inoculation is unknown. Accordingly, our objective was to test the association between scores from an array of cognitive function tests and self-reported vaccine hesitancy after the announcement of the successful testing of the Oxford University/AstraZeneca vaccine. METHODS: We used individual-level data from a pandemic-focused study (COVID Survey), a prospective cohort study nested within United Kingdom Understanding Society (Main Survey). In the week immediately following the announcement of successful testing of the first efficacious inoculation (November/December 2020), data on vaccine intentionality were collected in 11740 individuals (6702 women) aged 16-95. Pre-pandemic scores on general cognitive function, ascertained from a battery of six tests, were captured in 2011/12 wave of the Main Survey. Study members self-reported their intention to take up a vaccination for COVID-19. RESULTS: Of the study sample, 17.2% (N=1842) indicated they were hesitant about having the vaccine. After adjustment for age, sex, and ethnicity, study members with a lower baseline cognition score were markedly more likely to be vaccine hesitant (odds ratio per standard deviation lower score in cognition; 95% confidence interval: 1.76; 1.62, 1.90). Adjustment for mental and physical health plus household shielding status had no impact on these results, whereas controlling for educational attainment led to partial attenuation but the probability of hesitancy was still elevated (1.52; 1.37, 1.67). There was a linear association for vaccine hesitancy across the full range of cognition scores (p for trend: p<0.0001). CONCLUSIONS: Erroneous social media reports might have complicated personal decision-making, leading to people with lower cognitive ability test scores being vaccine-hesitant. With people with lower cognition also experiencing higher rates of COVID-19 in studies conducted prior to vaccine distribution, these new findings are suggestive of a potential additional disease burden.', 'Background: The World Health Organization has recently recognized Long COVID, calling the international medical community to strengthen research and comprehensive care of patients with this condition. However, if Long COVID pertains to children as well is not yet clear. Methods An anonymous, online survey was developed by an organization of parents of children suffering from persisting symptoms since initial infection. Parents were asked to report signs and symptoms, physical activity and mental health issues. Only children with symptoms persisting for more than four weeks were included. Results 510 children were included (56.3% females) infected between January 2020 and January 2021. At their initial COVID-19 infection, 22 (4.3%) children were hospitalized. Overall, children had persisting COVID-19 for a mean of 8.2 months (SD 3.9). Most frequent symptoms were: Tiredness and weakness (444 patients, 87.1% of sample), Fatigue (410, 80.4%), Headache (401, 78.6%), Abdominal pain (387, 75.9%), Muscle and joint pain (309, 60.6%), Post-exertional malaise (274, 53.7%), rash (267, 52.4%). 484 (94.9%) children had had at least four symptoms. 129 (25.3%) children have suffered constant COVID-19 infection symptoms, 252 (49.4%) have had periods of apparent recovery and then symptoms returning, and 97 (19.0%) had a prolonged period of wellness followed by symptoms. Only 51 (10.0%) children have returned to previous levels of physical activity. Parents reported a significant prevalence of Neuropsychiatric symptoms. Conclusions Our study provides further evidence on Long COVID in children. Symptoms like fatigue, headache, muscle and joint pain, rashes and heart palpitations, and mental health issues like lack of concentration and short memory problems, were particularly frequent and confirm previous observations, suggesting that they may characterize this condition. A better comprehension of Long COVID is urgently needed..']}\n"
     ]
    }
   ],
   "source": [
    "eval_examples = []\n",
    "n_correct_cord_uid_not_in_top_k = 0\n",
    "\n",
    "# Iterate through the rows of the validation tweet_info DataFrame\n",
    "for index, row in tweet_info_dev.iterrows():\n",
    "    query_text = row['tweet_text']\n",
    "    correct_cord_uid = row['cord_uid'] # This is the ID of the correct paper\n",
    "    top_k_candidate_uids = row['tfidf_topk'] # This is the list of UIDs from the first stage ranker\n",
    "\n",
    "    # We need the abstract for the correct paper and for all candidate papers\n",
    "    positive_abstract = None\n",
    "    negative_abstracts_map = {} # Map UID to abstract text for candidates\n",
    "\n",
    "    # Get the positive abstract\n",
    "    if correct_cord_uid in paper_info and isinstance(paper_info[correct_cord_uid], str) and paper_info[correct_cord_uid].strip():\n",
    "        positive_abstract = paper_info[correct_cord_uid]\n",
    "\n",
    "    # Get abstracts for all negative candidates (all in the top-k list except the positive abstract)\n",
    "    for uid in top_k_candidate_uids:\n",
    "        if uid in paper_info and uid != correct_cord_uid:\n",
    "            negative_abstracts_map[uid] = paper_info[uid]\n",
    "\n",
    "    if positive_abstract and negative_abstracts_map: # Ensure we have the positive and at least one negative abstract\n",
    "        negative_uids_list = list(negative_abstracts_map.keys())\n",
    "        negative_abstracts_list = list(negative_abstracts_map.values())\n",
    "\n",
    "        if correct_cord_uid in top_k_candidate_uids:\n",
    "            eval_examples.append({\n",
    "                \"query\": query_text,\n",
    "                \"positive\": [positive_abstract],\n",
    "                \"negative\": negative_abstracts_list\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            # This case means the correct paper was not found in the top-k list from the first stage ranker.\n",
    "            n_correct_cord_uid_not_in_top_k += 1\n",
    "\n",
    "if n_correct_cord_uid_not_in_top_k > 0:\n",
    "    print(f\"Warning: {n_correct_cord_uid_not_in_top_k} correct cord_uid's are not in top-k for validation tweets, cannot evaluate re-ranking for them.\")\n",
    "\n",
    "print(f\"Created {len(eval_examples)} evaluation examples for RerankingEvaluator.\")\n",
    "print(eval_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import RerankingEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55daedf8",
   "metadata": {},
   "source": [
    "------- BEGIN of the training -------\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417' max='4020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 417/4020 11:14 < 1:37:35, 0.62 it/s, Epoch 1.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Reranking Map</th>\n",
       "      <th>Validation Reranking Mrr@10</th>\n",
       "      <th>Validation Reranking Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.712622</td>\n",
       "      <td>0.707910</td>\n",
       "      <td>0.755799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Create a trainer & train\u001b[39;00m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mdev_evaluator,\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/transformers/trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3742\u001b[0m ):\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/sentence_transformers/trainer.py:406\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    401\u001b[0m     model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[1;32m    404\u001b[0m ):\n\u001b[1;32m    405\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_model_in_loss(loss_fn, model)\n\u001b[0;32m--> 406\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/sentence_transformers/losses/MultipleNegativesRankingLoss.py:102\u001b[0m, in \u001b[0;36mMultipleNegativesRankingLoss.forward\u001b[0;34m(self, sentence_features, labels)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Compute the embeddings and distribute them to anchor and candidates (positive and optionally negatives)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m    103\u001b[0m     anchors \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings[\u001b[38;5;241m1\u001b[39m:])  \u001b[38;5;66;03m# (batch_size * (1 + num_negatives), embedding_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/sentence_transformers/losses/MultipleNegativesRankingLoss.py:102\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Compute the embeddings and distribute them to anchor and candidates (positive and optionally negatives)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m    103\u001b[0m     anchors \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (batch_size, embedding_dim)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings[\u001b[38;5;241m1\u001b[39m:])  \u001b[38;5;66;03m# (batch_size * (1 + num_negatives), embedding_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/accelerate/utils/operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/accelerate/utils/operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:758\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    757\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    444\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:944\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    937\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m    938\u001b[0m             attention_mask,\n\u001b[1;32m    939\u001b[0m             input_shape,\n\u001b[1;32m    940\u001b[0m             embedding_output,\n\u001b[1;32m    941\u001b[0m             past_key_values_length,\n\u001b[1;32m    942\u001b[0m         )\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 944\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m~/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:448\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    445\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5 # WAS accidently low - but we can't go as high as e.g. 32 with the 4090.\n",
    "num_epochs = 10 # You can adjust this number. 5-10 is a common starting point.\n",
    "\n",
    "# Define train loss\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir='output/bi-encoder-roberta-large-baseline',\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    run_name=\"bi-encoder-roberta-large-baseline-tweet-paper-reranking\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# (Optional) Create an evaluator & evaluate the base model\n",
    "# !!! WARNING !!! This evaluator EXCLUDES cases where the true paper IS NOT in the top k !!\n",
    "dev_evaluator = RerankingEvaluator(eval_examples, batch_size=BATCH_SIZE, name='validation_reranking')\n",
    "dev_evaluator(model)\n",
    "\n",
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13c494",
   "metadata": {},
   "source": [
    "------- END of the training -------\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9072799",
   "metadata": {},
   "source": [
    "Create the repo on huggingface if it does not already exists\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id_str = 'LukasXperiaZ/roberta-large-tweet-paper-reranker-baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the repo when needed\n",
    "#from huggingface_hub import delete_repo\n",
    "#delete_repo(repo_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed740ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model on hugging face (in a new repo)\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "def create_repo_on_huggingface(repo_id_str):\n",
    "    try:\n",
    "        repo_url = create_repo(repo_id=repo_id_str, exist_ok=True, private=True)\n",
    "        print(f\"Created or found repository on Hugging Face Hub: {repo_url}\")\n",
    "        # create_repo returns the URL of the repository, not the repo_id string.\n",
    "        # Let's keep the repo_id string for upload_folder\n",
    "        repo_id = repo_id_str\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        print(\"It seems your huggingface_hub library version is incompatible.\")\n",
    "        print(\"Please update it: pip install -U huggingface_hub\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating the repository: {e}\")\n",
    "    return repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = create_repo_on_huggingface(repo_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49b8bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads the model to hugging face\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "def upload_model_to_huggingface(local_folder_path, repo_id):\n",
    "    # Path to your local directory containing the trained model files\n",
    "\n",
    "    print(f\"Uploading files from {local_folder_path} to {repo_id}...\")\n",
    "\n",
    "    upload_folder(\n",
    "        folder_path=local_folder_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type='model', # Specify the type of repository\n",
    "        commit_message='Upload final model from checkpoint',\n",
    "    )\n",
    "\n",
    "    print(\"Upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45706524",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder_path = 'output/bi-encoder-roberta-large-baseline/checkpoint-12855'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab7e7f",
   "metadata": {},
   "source": [
    "Download the trained model from Hugging Face\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9286cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub: LukasXperiaZ/roberta-large-tweet-paper-reranker-baseline\n"
     ]
    }
   ],
   "source": [
    "model_from_hub = SentenceTransformer(repo_id)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625286f4",
   "metadata": {},
   "source": [
    "Let the model rerank the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "656251e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# --- Define the Re-ranking Function for a single tweet ---\n",
    "def rerank_tweet(tweet_text: str, initial_top_k_uids: list, paper_info: pd.Series, model: SentenceTransformer) -> list:\n",
    "    \"\"\"\n",
    "    Re-ranks a list of candidate paper UIDs for a given tweet using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        tweet_text: The text of the query tweet.\n",
    "        initial_top_k_uids: A list of paper UIDs from the initial ranker.\n",
    "        paper_info: A pandas Series mapping CORD UIDs to abstract texts.\n",
    "        model: The loaded SentenceTransformer model for encoding.\n",
    "\n",
    "    Returns:\n",
    "        A list of re-ranked paper UIDs sorted by relevance score (descending).\n",
    "        Returns an empty list if no valid candidates are available or re-ranking fails.\n",
    "    \"\"\"\n",
    "    candidate_abstracts = {}\n",
    "    candidate_uids = []\n",
    "\n",
    "    # Retrieve Abstracts for the initial Top-k UIDs\n",
    "    if initial_top_k_uids and isinstance(initial_top_k_uids, list):\n",
    "        for uid in initial_top_k_uids:\n",
    "            if uid in paper_info and isinstance(paper_info[uid], str) and paper_info[uid].strip():\n",
    "                candidate_abstracts[uid] = paper_info[uid]\n",
    "                candidate_uids.append(uid)\n",
    "\n",
    "    # Perform Re-ranking if valid candidates exist\n",
    "    if candidate_uids:\n",
    "        candidate_abstract_texts = [candidate_abstracts[uid] for uid in candidate_uids]\n",
    "\n",
    "        try:\n",
    "            # Encode the Tweet and Candidate Abstracts\n",
    "            # Ensure inputs are on the same device as the model\n",
    "            query_embedding = model.encode(tweet_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "            candidate_embeddings = model.encode(candidate_abstract_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "            # Calculate Similarity Scores (Dot Product)\n",
    "            query_embedding = query_embedding.unsqueeze(0) # Ensure 2D\n",
    "            scores = util.dot_score(query_embedding, candidate_embeddings)[0] # Get scores for single query\n",
    "\n",
    "            # Pair UIDs with Scores and Sort\n",
    "            score_uid_pairs = sorted(zip(scores.tolist(), candidate_uids), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Return the re-ranked list of UIDs\n",
    "            return [uid for score, uid in score_uid_pairs]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during encoding or scoring for tweet: '{tweet_text[:50]}...' - {e}\")\n",
    "            return [] # Return empty list on error\n",
    "\n",
    "\n",
    "    else:\n",
    "        # No valid candidates, return an empty list\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73fb194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_tweets(model_from_hub):\n",
    "    # --- Prepare DataFrame to store re-ranked results ---\n",
    "    # Create a new column in tweet_info_dev\n",
    "    tweet_info_dev['reranked_uids'] = None\n",
    "\n",
    "\n",
    "    # --- Iterate and Re-rank for each tweet ---\n",
    "    print(f\"\\nStarting re-ranking for {len(tweet_info_dev)} tweets ...\")\n",
    "\n",
    "    for index, row in tweet_info_dev.iterrows():\n",
    "        query_text = row['tweet_text']\n",
    "        initial_top_k_uids = row['tfidf_topk']\n",
    "\n",
    "        # Call the rerank_tweet function\n",
    "        reranked_list = rerank_tweet(query_text, initial_top_k_uids, paper_info, model_from_hub)\n",
    "\n",
    "        # Store the re-ranked list in the DataFrame\n",
    "        tweet_info_dev.at[index, 'reranked_uids'] = reranked_list\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if (index + 1) % 100 == 0:\n",
    "            print(f\"Processed {index + 1}/{len(tweet_info_dev)} tweets.\")\n",
    "\n",
    "\n",
    "    print(f\"\\nRe-ranking complete for all tweets.\")\n",
    "\n",
    "    # --- The 'reranked_uids' column in tweet_info_dev now contains the results ---\n",
    "    # You can access and evaluate tweet_info_dev['reranked_uids']\n",
    "    print(tweet_info_dev['reranked_uids'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e04543",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_tweets(model_from_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d8960d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(\n",
    "            lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                      if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89873840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cord_uid                                               topk\n",
      "0  3qvh482o  [styavbvi, hg3xpej0, nksd3wuw, 00ia8k0b, 5hxsa...\n",
      "1  r58aohnu  [r58aohnu, cnz7jlw4, mcmqx7og, mz1bof2x, icgsb...\n",
      "Reranking Results on the dev set: {1: np.float64(0.5135714285714286), 5: np.float64(0.5918333333333333), 10: np.float64(0.6000099206349205)}\n"
     ]
    }
   ],
   "source": [
    "df_dev_eval = pd.DataFrame({\n",
    "    \"cord_uid\": tweet_info_dev[\"cord_uid\"],\n",
    "    \"topk\": tweet_info_dev[\"reranked_uids\"]\n",
    "})\n",
    "print(df_dev_eval[:2])\n",
    "\n",
    "# Evaluate MRR@k\n",
    "results_def_rerank_roberta = get_performance_mrr(df_dev_eval, 'cord_uid', 'topk')\n",
    "print(f\"Reranking Results on the dev set: {results_def_rerank_roberta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3ca15",
   "metadata": {},
   "source": [
    "Using a model pretrained on medical data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af59e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pritamdeka/S-PubMedBert-MS-MARCO\n"
     ]
    }
   ],
   "source": [
    "model = load_sentence_transformer_model('pritamdeka/S-PubMedBert-MS-MARCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1430' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1430/1430 23:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Reranking Map</th>\n",
       "      <th>Validation Reranking Mrr@10</th>\n",
       "      <th>Validation Reranking Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688993</td>\n",
       "      <td>0.683346</td>\n",
       "      <td>0.732197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.712590</td>\n",
       "      <td>0.707548</td>\n",
       "      <td>0.755158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721051</td>\n",
       "      <td>0.716551</td>\n",
       "      <td>0.764737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.728895</td>\n",
       "      <td>0.724555</td>\n",
       "      <td>0.771824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723946</td>\n",
       "      <td>0.719508</td>\n",
       "      <td>0.768025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726987</td>\n",
       "      <td>0.722547</td>\n",
       "      <td>0.770266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730909</td>\n",
       "      <td>0.726515</td>\n",
       "      <td>0.773619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.077200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730399</td>\n",
       "      <td>0.726058</td>\n",
       "      <td>0.773472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730984</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.773487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733795</td>\n",
       "      <td>0.729329</td>\n",
       "      <td>0.775430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1430, training_loss=0.3234364772176409, metrics={'train_runtime': 1392.4486, 'train_samples_per_second': 92.305, 'train_steps_per_second': 1.027, 'total_flos': 0.0, 'train_loss': 0.3234364772176409, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 90\n",
    "num_epochs = 10\n",
    "\n",
    "# Define train loss\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir='output/bi-encoder-S-PubMedBert-MS-MARCO',\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    run_name=\"bi-encoder-S-PubMedBert-MS-MARCO-tweet-paper-reranking\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# (Optional) Create an evaluator & evaluate the base model\n",
    "# !!! WARNING !!! This evaluator EXCLUDES cases where the true paper IS NOT in the top k !!\n",
    "dev_evaluator = RerankingEvaluator(eval_examples, batch_size=BATCH_SIZE, name='validation_reranking')\n",
    "dev_evaluator(model)\n",
    "\n",
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8c8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id_str = 'LukasXperiaZ/S-PubMedBert-MS-MARCO-tweet-paper-reranker-abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d23171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created or found repository on Hugging Face Hub: https://huggingface.co/LukasXperiaZ/S-PubMedBert-MS-MARCO-tweet-paper-reranker-abstract\n"
     ]
    }
   ],
   "source": [
    "repo_id = create_repo_on_huggingface(repo_id_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ceacae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files from output/bi-encoder-S-PubMedBert-MS-MARCO/checkpoint-1430 to LukasXperiaZ/S-PubMedBert-MS-MARCO-tweet-paper-reranker-abstract...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   0%|          | 377k/438M [00:00<02:07, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "training_args.bin: 100%|ââââââââââ| 6.10k/6.10k [00:00<00:00, 27.1kB/s]\n",
      "scheduler.pt: 100%|ââââââââââ| 1.47k/1.47k [00:00<00:00, 6.12kB/s]\n",
      "model.safetensors:   0%|          | 1.16M/438M [00:00<02:03, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   0%|          | 1.56M/438M [00:00<02:08, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "rng_state.pth: 100%|ââââââââââ| 14.6k/14.6k [00:00<00:00, 25.9kB/s]/s]\n",
      "model.safetensors:   1%|          | 2.54M/438M [00:00<02:01, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|          | 3.52M/438M [00:01<02:04, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|          | 4.03M/438M [00:01<02:06, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|          | 4.52M/438M [00:01<02:06, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|          | 5.01M/438M [00:01<02:02, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|â         | 5.51M/438M [00:01<02:03, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|â         | 6.00M/438M [00:01<02:00, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   1%|â         | 6.50M/438M [00:01<02:01, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 7.06M/438M [00:02<02:08, 3.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 7.55M/438M [00:02<02:13, 3.21MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 8.04M/438M [00:02<02:17, 3.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 8.54M/438M [00:02<02:20, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 9.03M/438M [00:02<02:27, 2.92MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 9.32M/438M [00:02<02:28, 2.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 9.62M/438M [00:02<02:30, 2.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 10.0M/438M [00:03<02:48, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   2%|â         | 10.5M/438M [00:03<02:48, 2.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 11.0M/438M [00:03<02:47, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 11.5M/438M [00:03<02:46, 2.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 12.0M/438M [00:03<02:43, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 13.7M/438M [00:04<02:04, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 14.5M/438M [00:04<01:54, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   3%|â         | 15.0M/438M [00:04<01:56, 3.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 15.5M/438M [00:04<01:59, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 16.0M/438M [00:05<02:53, 2.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 17.0M/438M [00:05<02:02, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 17.5M/438M [00:05<01:57, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 18.1M/438M [00:05<01:56, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 18.7M/438M [00:05<01:51, 3.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   4%|â         | 19.4M/438M [00:06<01:53, 3.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 20.0M/438M [00:06<01:54, 3.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 20.6M/438M [00:06<01:58, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 21.3M/438M [00:06<01:58, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 21.9M/438M [00:06<02:00, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 22.5M/438M [00:06<02:00, 3.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 23.1M/438M [00:07<02:00, 3.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   5%|â         | 23.7M/438M [00:07<02:00, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 24.3M/438M [00:07<02:02, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 25.0M/438M [00:07<02:00, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 25.6M/438M [00:07<02:00, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 26.2M/438M [00:08<02:04, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 26.6M/438M [00:08<02:06, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 26.9M/438M [00:08<02:07, 3.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 27.2M/438M [00:08<02:11, 3.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   6%|â         | 27.5M/438M [00:08<02:12, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 29.0M/438M [00:08<01:43, 3.93MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 29.4M/438M [00:08<01:44, 3.89MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 29.8M/438M [00:09<01:50, 3.68MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 30.2M/438M [00:09<01:56, 3.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 30.5M/438M [00:09<02:02, 3.32MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 30.9M/438M [00:09<02:44, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 31.2M/438M [00:09<02:42, 2.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 31.5M/438M [00:09<02:40, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 31.8M/438M [00:09<02:40, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   7%|â         | 32.8M/438M [00:10<02:49, 2.40MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 33.1M/438M [00:10<02:37, 2.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 33.7M/438M [00:10<02:22, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 34.4M/438M [00:10<02:14, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 35.0M/438M [00:11<02:08, 3.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 35.6M/438M [00:11<02:04, 3.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 36.2M/438M [00:11<02:05, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 36.9M/438M [00:11<02:05, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   8%|â         | 37.2M/438M [00:11<02:07, 3.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 37.5M/438M [00:11<02:10, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 37.8M/438M [00:11<02:10, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 38.5M/438M [00:12<02:15, 2.94MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 39.9M/438M [00:12<01:57, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 40.5M/438M [00:12<01:38, 4.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 41.0M/438M [00:12<01:38, 4.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:   9%|â         | 41.5M/438M [00:13<01:55, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 42.1M/438M [00:13<01:53, 3.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 42.7M/438M [00:13<02:00, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 43.0M/438M [00:13<02:04, 3.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 43.4M/438M [00:13<02:07, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 43.7M/438M [00:13<02:10, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 44.0M/438M [00:13<02:21, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 44.3M/438M [00:14<02:30, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 44.6M/438M [00:14<02:41, 2.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 44.9M/438M [00:14<02:47, 2.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 45.5M/438M [00:14<02:33, 2.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  10%|â         | 45.9M/438M [00:14<02:29, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 46.2M/438M [00:14<02:23, 2.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 46.5M/438M [00:14<02:21, 2.76MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 46.8M/438M [00:15<02:17, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 47.1M/438M [00:15<02:20, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 47.7M/438M [00:15<02:13, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 48.0M/438M [00:15<04:29, 1.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 48.8M/438M [00:15<02:41, 2.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  11%|â         | 49.1M/438M [00:16<02:35, 2.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 50.6M/438M [00:16<01:56, 3.32MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 51.3M/438M [00:16<01:47, 3.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 51.9M/438M [00:16<01:50, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 52.5M/438M [00:16<01:50, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 53.1M/438M [00:17<01:53, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 53.7M/438M [00:17<01:52, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 54.1M/438M [00:17<01:52, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  12%|ââ        | 54.7M/438M [00:17<01:57, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 55.3M/438M [00:17<01:55, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 55.9M/438M [00:18<01:55, 3.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 56.5M/438M [00:18<01:54, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 56.9M/438M [00:18<01:57, 3.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 57.2M/438M [00:18<02:00, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 57.5M/438M [00:18<02:03, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 57.8M/438M [00:18<02:07, 2.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 58.1M/438M [00:18<02:10, 2.90MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 58.4M/438M [00:18<02:11, 2.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  13%|ââ        | 58.7M/438M [00:18<02:10, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 59.4M/438M [00:19<02:03, 3.07MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 60.0M/438M [00:19<01:58, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 60.6M/438M [00:19<01:54, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 61.2M/438M [00:19<01:52, 3.35MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 61.8M/438M [00:19<01:53, 3.32MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 62.2M/438M [00:19<01:54, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 62.5M/438M [00:20<01:56, 3.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 62.8M/438M [00:20<02:01, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  14%|ââ        | 63.1M/438M [00:20<02:05, 2.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 63.7M/438M [00:20<02:08, 2.92MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 64.0M/438M [00:20<03:22, 1.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 64.8M/438M [00:20<02:03, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 65.5M/438M [00:21<01:57, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 66.1M/438M [00:21<01:56, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 66.7M/438M [00:21<01:57, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 67.0M/438M [00:21<01:57, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 67.4M/438M [00:21<01:55, 3.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  15%|ââ        | 67.7M/438M [00:21<01:57, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 68.4M/438M [00:22<01:58, 3.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 68.7M/438M [00:22<01:59, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 69.1M/438M [00:22<02:01, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 69.4M/438M [00:22<02:02, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 69.7M/438M [00:22<02:02, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 70.0M/438M [00:22<02:07, 2.89MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 70.6M/438M [00:22<02:07, 2.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 70.9M/438M [00:22<02:09, 2.83MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 71.2M/438M [00:23<02:11, 2.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 71.5M/438M [00:23<02:10, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  16%|ââ        | 72.1M/438M [00:23<02:13, 2.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 72.6M/438M [00:23<02:19, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 72.9M/438M [00:23<02:13, 2.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 73.2M/438M [00:23<02:10, 2.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 74.1M/438M [00:24<02:04, 2.92MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 74.7M/438M [00:24<01:59, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 75.0M/438M [00:24<01:59, 3.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  17%|ââ        | 75.7M/438M [00:24<02:00, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 77.5M/438M [00:25<01:32, 3.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 78.1M/438M [00:25<01:25, 4.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 78.7M/438M [00:25<01:18, 4.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 79.2M/438M [00:25<01:37, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 79.7M/438M [00:25<01:43, 3.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 80.1M/438M [00:26<02:36, 2.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  18%|ââ        | 80.8M/438M [00:26<01:58, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 81.4M/438M [00:26<01:49, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 82.0M/438M [00:26<01:43, 3.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 82.7M/438M [00:26<01:40, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 83.3M/438M [00:26<01:38, 3.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 83.9M/438M [00:26<01:36, 3.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 84.5M/438M [00:27<01:35, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  19%|ââ        | 85.1M/438M [00:27<01:35, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  20%|ââ        | 85.8M/438M [00:27<01:36, 3.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  20%|ââ        | 86.4M/438M [00:27<01:34, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  20%|ââ        | 87.6M/438M [00:27<01:29, 3.90MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  20%|ââ        | 88.2M/438M [00:28<01:27, 4.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  20%|ââ        | 88.8M/438M [00:28<01:25, 4.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|ââ        | 89.9M/438M [00:28<01:23, 4.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|ââ        | 90.5M/438M [00:28<01:23, 4.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|ââ        | 91.1M/438M [00:28<01:22, 4.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|ââ        | 91.8M/438M [00:28<01:22, 4.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|ââ        | 92.4M/438M [00:29<01:23, 4.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  21%|âââ       | 93.6M/438M [00:29<01:25, 4.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 94.2M/438M [00:29<01:26, 3.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 94.9M/438M [00:29<01:27, 3.93MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 95.5M/438M [00:29<01:28, 3.86MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 96.0M/438M [00:30<02:26, 2.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 97.5M/438M [00:30<01:35, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  22%|âââ       | 98.1M/438M [00:30<01:36, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 98.8M/438M [00:30<01:38, 3.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 99.4M/438M [00:32<05:46, 978kB/s] \n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 100M/438M [00:32<04:37, 1.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 101M/438M [00:33<03:49, 1.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 101M/438M [00:33<03:14, 1.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 102M/438M [00:33<02:47, 2.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 102M/438M [00:33<02:38, 2.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  23%|âââ       | 103M/438M [00:33<02:21, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 103M/438M [00:33<02:18, 2.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 103M/438M [00:34<02:15, 2.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 104M/438M [00:34<02:13, 2.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 104M/438M [00:34<02:11, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 105M/438M [00:34<01:58, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 105M/438M [00:34<01:52, 2.96MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 106M/438M [00:34<01:48, 3.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 106M/438M [00:35<01:44, 3.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  24%|âââ       | 107M/438M [00:35<01:45, 3.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 108M/438M [00:35<01:41, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 108M/438M [00:35<01:42, 3.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 109M/438M [00:35<01:43, 3.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 109M/438M [00:35<01:44, 3.14MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 109M/438M [00:36<01:53, 2.90MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 110M/438M [00:36<01:51, 2.93MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  25%|âââ       | 112M/438M [00:36<01:27, 3.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 112M/438M [00:37<02:39, 2.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 113M/438M [00:37<01:47, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 113M/438M [00:37<01:41, 3.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 114M/438M [00:37<01:44, 3.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 114M/438M [00:37<01:38, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 115M/438M [00:37<01:35, 3.38MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  26%|âââ       | 116M/438M [00:37<01:31, 3.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 116M/438M [00:38<01:30, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 117M/438M [00:38<01:27, 3.68MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 117M/438M [00:38<01:23, 3.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 119M/438M [00:38<01:17, 4.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 119M/438M [00:38<01:19, 4.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  27%|âââ       | 120M/438M [00:39<01:20, 3.94MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 121M/438M [00:39<01:22, 3.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 121M/438M [00:39<01:23, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 122M/438M [00:39<01:25, 3.70MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 122M/438M [00:39<01:25, 3.68MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 123M/438M [00:39<01:24, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 124M/438M [00:40<01:25, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  28%|âââ       | 124M/438M [00:40<01:25, 3.65MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 125M/438M [00:40<01:25, 3.65MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 126M/438M [00:40<01:25, 3.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 126M/438M [00:40<01:24, 3.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 127M/438M [00:40<01:25, 3.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 128M/438M [00:41<01:29, 3.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 128M/438M [00:41<02:02, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  29%|âââ       | 129M/438M [00:41<01:30, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 129M/438M [00:41<01:31, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 130M/438M [00:41<01:31, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 130M/438M [00:42<01:36, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 130M/438M [00:42<01:38, 3.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 131M/438M [00:42<02:04, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 131M/438M [00:42<01:53, 2.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 132M/438M [00:42<01:46, 2.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 133M/438M [00:42<01:41, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  30%|âââ       | 133M/438M [00:43<01:39, 3.07MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|âââ       | 134M/438M [00:43<01:36, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|âââ       | 134M/438M [00:43<01:34, 3.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|âââ       | 135M/438M [00:43<01:33, 3.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|âââ       | 136M/438M [00:43<01:32, 3.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|âââ       | 136M/438M [00:44<01:32, 3.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|ââââ      | 137M/438M [00:44<01:30, 3.32MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  31%|ââââ      | 138M/438M [00:44<01:29, 3.35MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 138M/438M [00:44<01:30, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 139M/438M [00:44<01:32, 3.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 139M/438M [00:44<01:34, 3.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 139M/438M [00:44<01:35, 3.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 140M/438M [00:45<01:37, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 141M/438M [00:45<01:11, 4.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  32%|ââââ      | 142M/438M [00:45<01:15, 3.93MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 142M/438M [00:45<01:17, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 143M/438M [00:45<01:19, 3.70MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 144M/438M [00:46<01:24, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 145M/438M [00:46<01:41, 2.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 145M/438M [00:46<01:34, 3.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 146M/438M [00:47<01:29, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  33%|ââââ      | 147M/438M [00:47<01:27, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 147M/438M [00:47<01:25, 3.38MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 148M/438M [00:47<01:20, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 149M/438M [00:47<01:16, 3.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 149M/438M [00:47<01:13, 3.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 150M/438M [00:48<01:12, 3.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 150M/438M [00:48<01:12, 3.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  34%|ââââ      | 151M/438M [00:48<01:15, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 152M/438M [00:48<01:17, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 152M/438M [00:48<01:19, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 153M/438M [00:48<01:19, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 153M/438M [00:49<01:23, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 154M/438M [00:49<01:25, 3.32MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 154M/438M [00:49<01:26, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  35%|ââââ      | 155M/438M [00:49<01:27, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 156M/438M [00:49<01:08, 4.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 157M/438M [00:50<01:16, 3.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 157M/438M [00:50<01:19, 3.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 158M/438M [00:50<01:28, 3.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 158M/438M [00:50<01:37, 2.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 158M/438M [00:50<01:47, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 159M/438M [00:50<01:58, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 159M/438M [00:50<02:21, 1.97MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 159M/438M [00:51<02:23, 1.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  36%|ââââ      | 160M/438M [00:51<02:14, 2.07MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 160M/438M [00:51<02:05, 2.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 160M/438M [00:51<03:17, 1.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 161M/438M [00:52<02:09, 2.14MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 161M/438M [00:52<02:03, 2.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 161M/438M [00:52<01:57, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 162M/438M [00:52<01:49, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 162M/438M [00:52<01:43, 2.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 163M/438M [00:52<01:33, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 163M/438M [00:52<01:30, 3.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  37%|ââââ      | 164M/438M [00:52<01:27, 3.14MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  38%|ââââ      | 166M/438M [00:53<01:08, 3.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  38%|ââââ      | 166M/438M [00:53<01:13, 3.70MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  38%|ââââ      | 167M/438M [00:53<01:15, 3.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  38%|ââââ      | 168M/438M [00:54<01:15, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  38%|ââââ      | 168M/438M [00:54<01:15, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 169M/438M [00:54<01:16, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 170M/438M [00:54<01:14, 3.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 170M/438M [00:54<01:11, 3.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 171M/438M [00:54<01:09, 3.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 171M/438M [00:55<01:09, 3.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 172M/438M [00:55<01:08, 3.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  39%|ââââ      | 173M/438M [00:55<01:08, 3.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 173M/438M [00:55<01:11, 3.72MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 174M/438M [00:55<01:14, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 174M/438M [00:55<01:15, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 175M/438M [00:56<01:18, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 176M/438M [00:56<01:15, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 176M/438M [00:56<01:55, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  40%|ââââ      | 177M/438M [00:56<01:22, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|ââââ      | 178M/438M [00:56<01:18, 3.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|ââââ      | 178M/438M [00:57<01:14, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|ââââ      | 178M/438M [00:57<01:27, 2.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|ââââ      | 179M/438M [00:57<01:25, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|ââââ      | 179M/438M [00:57<01:26, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|âââââ     | 181M/438M [00:58<01:15, 3.40MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  41%|âââââ     | 182M/438M [00:58<01:03, 4.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 182M/438M [00:58<01:03, 4.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 183M/438M [00:58<01:14, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 184M/438M [00:58<01:14, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 184M/438M [00:58<01:13, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 184M/438M [00:58<01:14, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 185M/438M [00:59<01:13, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 185M/438M [00:59<01:33, 2.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 185M/438M [00:59<01:31, 2.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  42%|âââââ     | 186M/438M [00:59<01:30, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 186M/438M [00:59<01:23, 3.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 187M/438M [00:59<01:17, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 187M/438M [00:59<01:12, 3.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 188M/438M [01:00<01:10, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 189M/438M [01:00<01:10, 3.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 189M/438M [01:00<01:09, 3.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  43%|âââââ     | 190M/438M [01:00<01:08, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 191M/438M [01:00<01:08, 3.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 191M/438M [01:00<01:08, 3.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 192M/438M [01:01<01:08, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 192M/438M [01:01<01:45, 2.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 193M/438M [01:01<01:19, 3.08MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 194M/438M [01:01<01:08, 3.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  44%|âââââ     | 195M/438M [01:02<01:06, 3.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  45%|âââââ     | 195M/438M [01:02<01:03, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  45%|âââââ     | 196M/438M [01:02<01:03, 3.83MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  45%|âââââ     | 198M/438M [01:02<01:00, 3.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  45%|âââââ     | 198M/438M [01:03<00:58, 4.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  45%|âââââ     | 199M/438M [01:03<01:04, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 199M/438M [01:03<01:05, 3.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 200M/438M [01:03<01:07, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 200M/438M [01:03<01:08, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 201M/438M [01:03<01:08, 3.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 201M/438M [01:03<01:11, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 201M/438M [01:03<01:12, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 202M/438M [01:04<01:35, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 202M/438M [01:04<01:32, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 202M/438M [01:04<01:29, 2.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 203M/438M [01:04<01:28, 2.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  46%|âââââ     | 203M/438M [01:04<01:26, 2.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 204M/438M [01:04<01:23, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 204M/438M [01:05<01:23, 2.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 205M/438M [01:06<05:20, 728kB/s] \n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 205M/438M [01:07<04:14, 915kB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 205M/438M [01:07<03:24, 1.14MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 206M/438M [01:07<02:23, 1.62MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 207M/438M [01:07<01:52, 2.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 207M/438M [01:07<01:35, 2.40MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  47%|âââââ     | 208M/438M [01:07<01:24, 2.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  48%|âââââ     | 209M/438M [01:08<01:26, 2.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  48%|âââââ     | 209M/438M [01:08<01:22, 2.76MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  48%|âââââ     | 210M/438M [01:08<01:19, 2.88MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  48%|âââââ     | 212M/438M [01:09<01:07, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  48%|âââââ     | 212M/438M [01:09<01:00, 3.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 213M/438M [01:09<00:59, 3.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 213M/438M [01:09<01:06, 3.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 214M/438M [01:09<01:06, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 214M/438M [01:09<01:06, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 215M/438M [01:10<01:06, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 215M/438M [01:10<01:08, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 215M/438M [01:10<01:10, 3.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 216M/438M [01:10<01:11, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 216M/438M [01:10<01:14, 2.97MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 216M/438M [01:10<01:14, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  49%|âââââ     | 217M/438M [01:10<01:18, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 217M/438M [01:10<01:18, 2.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 217M/438M [01:10<01:20, 2.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 218M/438M [01:11<01:22, 2.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 218M/438M [01:11<01:24, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 218M/438M [01:11<01:23, 2.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 219M/438M [01:11<01:20, 2.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 219M/438M [01:11<01:19, 2.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 219M/438M [01:11<01:18, 2.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 220M/438M [01:11<01:16, 2.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 220M/438M [01:11<01:16, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 220M/438M [01:12<01:18, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 221M/438M [01:12<01:21, 2.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  50%|âââââ     | 221M/438M [01:12<01:25, 2.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 221M/438M [01:12<01:26, 2.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 222M/438M [01:12<01:25, 2.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 222M/438M [01:12<01:23, 2.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 223M/438M [01:13<01:01, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 224M/438M [01:13<00:51, 4.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|âââââ     | 224M/438M [01:13<01:22, 2.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|ââââââ    | 225M/438M [01:13<01:14, 2.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  51%|ââââââ    | 225M/438M [01:13<01:12, 2.93MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 226M/438M [01:13<01:09, 3.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 226M/438M [01:14<01:05, 3.21MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 227M/438M [01:14<01:03, 3.35MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 228M/438M [01:14<01:01, 3.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 228M/438M [01:14<01:00, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 229M/438M [01:14<01:01, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 229M/438M [01:14<01:00, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  52%|ââââââ    | 230M/438M [01:15<01:01, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 230M/438M [01:15<01:07, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 231M/438M [01:15<01:07, 3.07MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 231M/438M [01:15<01:11, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 231M/438M [01:15<01:14, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 232M/438M [01:15<01:18, 2.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 232M/438M [01:15<01:18, 2.62MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 232M/438M [01:16<01:22, 2.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 233M/438M [01:16<01:27, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 233M/438M [01:16<01:30, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 233M/438M [01:16<01:33, 2.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  53%|ââââââ    | 234M/438M [01:16<01:12, 2.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 235M/438M [01:17<01:19, 2.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 235M/438M [01:17<01:27, 2.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 236M/438M [01:17<01:29, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 236M/438M [01:17<01:28, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 237M/438M [01:18<01:28, 2.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 237M/438M [01:18<01:27, 2.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 237M/438M [01:18<01:24, 2.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 238M/438M [01:18<01:11, 2.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 238M/438M [01:18<01:10, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  54%|ââââââ    | 239M/438M [01:18<01:11, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 239M/438M [01:18<01:09, 2.86MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 239M/438M [01:18<01:08, 2.89MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 240M/438M [01:19<01:49, 1.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 241M/438M [01:19<01:16, 2.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 241M/438M [01:19<01:15, 2.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 242M/438M [01:19<01:18, 2.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  55%|ââââââ    | 243M/438M [01:20<01:05, 2.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 243M/438M [01:20<00:59, 3.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 244M/438M [01:20<01:01, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 244M/438M [01:20<01:05, 2.94MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 244M/438M [01:20<01:09, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 245M/438M [01:21<01:13, 2.62MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 245M/438M [01:21<01:18, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 245M/438M [01:21<01:19, 2.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 245M/438M [01:21<01:19, 2.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 246M/438M [01:21<01:18, 2.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 246M/438M [01:21<01:17, 2.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 246M/438M [01:21<01:19, 2.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 247M/438M [01:21<01:22, 2.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 247M/438M [01:22<01:26, 2.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  56%|ââââââ    | 247M/438M [01:22<01:26, 2.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 248M/438M [01:22<01:26, 2.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 248M/438M [01:22<01:26, 2.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 248M/438M [01:22<01:25, 2.21MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 249M/438M [01:22<01:24, 2.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 249M/438M [01:22<01:23, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 249M/438M [01:23<01:21, 2.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 250M/438M [01:23<01:20, 2.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 250M/438M [01:23<01:19, 2.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 250M/438M [01:23<01:18, 2.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 250M/438M [01:23<01:17, 2.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  57%|ââââââ    | 251M/438M [01:23<01:09, 2.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 253M/438M [01:24<00:43, 4.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 254M/438M [01:24<00:47, 3.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 254M/438M [01:24<00:46, 3.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 255M/438M [01:24<00:49, 3.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 255M/438M [01:24<00:50, 3.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 256M/438M [01:24<00:51, 3.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  58%|ââââââ    | 256M/438M [01:25<01:16, 2.38MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 257M/438M [01:25<01:00, 3.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 257M/438M [01:25<00:58, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 258M/438M [01:25<00:56, 3.21MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 259M/438M [01:25<00:55, 3.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 259M/438M [01:26<00:53, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  59%|ââââââ    | 260M/438M [01:26<00:54, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 261M/438M [01:26<00:54, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 262M/438M [01:26<00:54, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 262M/438M [01:27<00:53, 3.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 263M/438M [01:27<00:48, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 264M/438M [01:27<00:46, 3.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  60%|ââââââ    | 265M/438M [01:27<00:46, 3.76MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  61%|ââââââ    | 265M/438M [01:27<00:45, 3.83MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  61%|ââââââ    | 266M/438M [01:27<00:43, 3.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  61%|ââââââ    | 267M/438M [01:28<00:40, 4.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  61%|ââââââ    | 268M/438M [01:28<00:39, 4.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  61%|âââââââ   | 268M/438M [01:28<00:40, 4.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  62%|âââââââ   | 270M/438M [01:28<00:40, 4.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  62%|âââââââ   | 271M/438M [01:29<00:36, 4.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  62%|âââââââ   | 272M/438M [01:29<00:38, 4.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  62%|âââââââ   | 272M/438M [01:29<00:58, 2.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  62%|âââââââ   | 273M/438M [01:29<00:48, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 274M/438M [01:30<00:46, 3.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 274M/438M [01:30<00:46, 3.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 275M/438M [01:30<00:46, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 276M/438M [01:30<00:46, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 276M/438M [01:30<00:46, 3.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 277M/438M [01:30<00:47, 3.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 277M/438M [01:30<00:47, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 277M/438M [01:31<00:47, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  63%|âââââââ   | 278M/438M [01:31<00:48, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 278M/438M [01:31<00:57, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 278M/438M [01:31<01:00, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 279M/438M [01:31<01:01, 2.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 279M/438M [01:31<01:01, 2.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 279M/438M [01:31<01:04, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 280M/438M [01:32<01:00, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 280M/438M [01:32<00:56, 2.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 281M/438M [01:32<00:55, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 281M/438M [01:32<00:53, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  64%|âââââââ   | 282M/438M [01:32<00:51, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  65%|âââââââ   | 282M/438M [01:32<00:52, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  65%|âââââââ   | 285M/438M [01:33<00:37, 4.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  65%|âââââââ   | 286M/438M [01:33<00:38, 3.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  65%|âââââââ   | 286M/438M [01:33<00:42, 3.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  65%|âââââââ   | 287M/438M [01:34<00:43, 3.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 287M/438M [01:34<00:42, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 288M/438M [01:34<00:44, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 288M/438M [01:34<01:15, 1.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 289M/438M [01:34<00:56, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 289M/438M [01:35<00:54, 2.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 290M/438M [01:35<00:53, 2.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 290M/438M [01:35<00:52, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 291M/438M [01:35<00:52, 2.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 291M/438M [01:35<00:52, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  66%|âââââââ   | 291M/438M [01:35<00:52, 2.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 291M/438M [01:35<00:53, 2.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 292M/438M [01:35<00:53, 2.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 292M/438M [01:36<00:53, 2.72MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 292M/438M [01:36<00:53, 2.72MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 293M/438M [01:36<00:51, 2.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 293M/438M [01:36<00:44, 3.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 294M/438M [01:36<00:43, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 295M/438M [01:36<00:42, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  67%|âââââââ   | 295M/438M [01:37<00:43, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  68%|âââââââ   | 296M/438M [01:37<00:42, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  68%|âââââââ   | 296M/438M [01:37<00:42, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  68%|âââââââ   | 298M/438M [01:37<00:37, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  68%|âââââââ   | 299M/438M [01:38<00:33, 4.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  68%|âââââââ   | 300M/438M [01:38<00:35, 3.86MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 300M/438M [01:38<00:36, 3.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 301M/438M [01:38<00:36, 3.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 302M/438M [01:38<00:35, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 303M/438M [01:40<02:31, 892kB/s] \n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 303M/438M [01:40<01:59, 1.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 304M/438M [01:40<01:30, 1.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  69%|âââââââ   | 304M/438M [01:41<01:39, 1.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 305M/438M [01:41<01:03, 2.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 306M/438M [01:41<00:55, 2.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 306M/438M [01:41<00:49, 2.66MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 307M/438M [01:41<00:45, 2.86MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 308M/438M [01:42<00:41, 3.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 308M/438M [01:42<00:39, 3.31MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  70%|âââââââ   | 309M/438M [01:42<00:38, 3.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 309M/438M [01:42<00:40, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 310M/438M [01:42<00:40, 3.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 310M/438M [01:42<00:39, 3.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 311M/438M [01:43<00:40, 3.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 311M/438M [01:43<00:41, 3.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 311M/438M [01:43<00:42, 2.96MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|âââââââ   | 312M/438M [01:43<00:43, 2.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|ââââââââ  | 312M/438M [01:43<00:45, 2.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|ââââââââ  | 312M/438M [01:43<00:45, 2.75MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|ââââââââ  | 313M/438M [01:43<00:46, 2.68MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  71%|ââââââââ  | 313M/438M [01:43<00:49, 2.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 315M/438M [01:44<00:33, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 315M/438M [01:44<00:34, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 315M/438M [01:44<00:36, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 316M/438M [01:44<00:38, 3.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 316M/438M [01:44<00:39, 3.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 316M/438M [01:44<00:41, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 317M/438M [01:45<00:43, 2.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 317M/438M [01:45<00:45, 2.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 317M/438M [01:45<00:47, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  72%|ââââââââ  | 317M/438M [01:45<00:48, 2.47MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 318M/438M [01:45<00:50, 2.40MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 318M/438M [01:45<00:49, 2.42MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 318M/438M [01:45<00:51, 2.35MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 319M/438M [01:45<00:52, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 319M/438M [01:46<00:52, 2.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 319M/438M [01:46<00:55, 2.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 320M/438M [01:46<00:50, 2.35MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 320M/438M [01:46<00:49, 2.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 321M/438M [01:47<00:51, 2.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 321M/438M [01:47<00:50, 2.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 322M/438M [01:47<00:48, 2.38MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  73%|ââââââââ  | 322M/438M [01:47<00:47, 2.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 323M/438M [01:47<00:37, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 324M/438M [01:47<00:30, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 324M/438M [01:48<00:31, 3.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 324M/438M [01:48<00:31, 3.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 325M/438M [01:48<00:31, 3.64MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 325M/438M [01:48<00:31, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 326M/438M [01:48<00:39, 2.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  74%|ââââââââ  | 326M/438M [01:48<00:39, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 327M/438M [01:48<00:35, 3.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 327M/438M [01:49<00:34, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 328M/438M [01:49<00:32, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 329M/438M [01:49<00:30, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 329M/438M [01:49<00:31, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 330M/438M [01:49<00:31, 3.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  75%|ââââââââ  | 330M/438M [01:49<00:31, 3.45MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 331M/438M [01:50<00:32, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 331M/438M [01:50<00:32, 3.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 332M/438M [01:50<00:31, 3.37MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 333M/438M [01:50<00:30, 3.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 333M/438M [01:50<00:29, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 334M/438M [01:50<00:28, 3.65MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  76%|ââââââââ  | 335M/438M [01:51<00:27, 3.72MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  77%|ââââââââ  | 335M/438M [01:51<00:27, 3.71MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  77%|ââââââââ  | 336M/438M [01:51<00:27, 3.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  77%|ââââââââ  | 338M/438M [01:52<00:26, 3.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  77%|ââââââââ  | 338M/438M [01:52<00:23, 4.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  77%|ââââââââ  | 339M/438M [01:52<00:25, 3.94MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 340M/438M [01:52<00:24, 3.96MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 340M/438M [01:52<00:28, 3.48MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 340M/438M [01:52<00:27, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 341M/438M [01:52<00:27, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 341M/438M [01:53<00:27, 3.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 342M/438M [01:53<00:37, 2.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 342M/438M [01:53<00:37, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 342M/438M [01:53<00:38, 2.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 343M/438M [01:53<00:37, 2.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 343M/438M [01:53<00:37, 2.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 343M/438M [01:53<00:37, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 343M/438M [01:54<00:37, 2.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  78%|ââââââââ  | 344M/438M [01:54<00:37, 2.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 344M/438M [01:54<00:37, 2.52MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 344M/438M [01:54<00:36, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 345M/438M [01:54<00:35, 2.60MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 345M/438M [01:54<00:35, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 345M/438M [01:54<00:35, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 346M/438M [01:54<00:34, 2.65MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 346M/438M [01:54<00:35, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 346M/438M [01:55<00:33, 2.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 347M/438M [01:55<00:30, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 347M/438M [01:55<00:28, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  79%|ââââââââ  | 348M/438M [01:55<00:27, 3.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  80%|ââââââââ  | 349M/438M [01:55<00:27, 3.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  80%|ââââââââ  | 351M/438M [01:56<00:21, 4.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  80%|ââââââââ  | 351M/438M [01:56<00:22, 3.83MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  80%|ââââââââ  | 352M/438M [01:56<00:20, 4.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  80%|ââââââââ  | 352M/438M [01:57<00:38, 2.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|ââââââââ  | 353M/438M [01:57<00:28, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|ââââââââ  | 354M/438M [01:57<00:26, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|ââââââââ  | 354M/438M [01:57<00:30, 2.74MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|ââââââââ  | 355M/438M [01:57<00:29, 2.81MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|ââââââââ  | 355M/438M [01:57<00:26, 3.08MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|âââââââââ | 356M/438M [01:58<00:25, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  81%|âââââââââ | 357M/438M [01:58<00:24, 3.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 357M/438M [01:58<00:22, 3.51MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 358M/438M [01:58<00:22, 3.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 358M/438M [01:58<00:21, 3.62MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 359M/438M [01:58<00:22, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 360M/438M [01:59<00:21, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 360M/438M [01:59<00:21, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  82%|âââââââââ | 361M/438M [01:59<00:21, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  83%|âââââââââ | 362M/438M [01:59<00:21, 3.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  83%|âââââââââ | 362M/438M [01:59<00:21, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  83%|âââââââââ | 363M/438M [02:00<00:21, 3.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  83%|âââââââââ | 363M/438M [02:00<00:20, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  83%|âââââââââ | 365M/438M [02:00<00:20, 3.58MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 366M/438M [02:00<00:17, 4.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 367M/438M [02:01<00:18, 3.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 367M/438M [02:01<00:19, 3.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 369M/438M [02:01<00:22, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 369M/438M [02:02<00:21, 3.24MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  84%|âââââââââ | 370M/438M [02:02<00:22, 3.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 370M/438M [02:02<00:22, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 371M/438M [02:02<00:21, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 371M/438M [02:02<00:21, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 372M/438M [02:02<00:21, 3.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 373M/438M [02:03<00:20, 3.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 373M/438M [02:03<00:20, 3.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 373M/438M [02:03<00:21, 3.06MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 374M/438M [02:03<00:21, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 374M/438M [02:03<00:22, 2.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  85%|âââââââââ | 374M/438M [02:03<00:24, 2.63MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 374M/438M [02:03<00:25, 2.53MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 375M/438M [02:03<00:25, 2.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 375M/438M [02:04<00:26, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 375M/438M [02:04<00:27, 2.29MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 376M/438M [02:04<00:27, 2.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 376M/438M [02:04<00:30, 2.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 377M/438M [02:04<00:22, 2.68MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 377M/438M [02:05<00:26, 2.28MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 378M/438M [02:05<00:27, 2.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 378M/438M [02:05<00:27, 2.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 378M/438M [02:05<00:28, 2.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  86%|âââââââââ | 379M/438M [02:05<00:27, 2.14MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 379M/438M [02:05<00:27, 2.15MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 379M/438M [02:05<00:26, 2.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 379M/438M [02:06<00:26, 2.21MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 380M/438M [02:06<00:25, 2.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 380M/438M [02:06<00:23, 2.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 381M/438M [02:06<00:21, 2.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 381M/438M [02:06<00:21, 2.67MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 382M/438M [02:06<00:19, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 382M/438M [02:06<00:19, 2.89MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 383M/438M [02:07<00:18, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  87%|âââââââââ | 383M/438M [02:07<00:18, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  88%|âââââââââ | 384M/438M [02:07<00:17, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  88%|âââââââââ | 384M/438M [02:07<00:30, 1.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  88%|âââââââââ | 385M/438M [02:08<00:18, 2.91MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  88%|âââââââââ | 387M/438M [02:08<00:15, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 388M/438M [02:08<00:11, 4.18MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 389M/438M [02:08<00:12, 3.98MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 389M/438M [02:09<00:12, 3.90MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 390M/438M [02:09<00:11, 4.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 391M/438M [02:09<00:11, 4.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  89%|âââââââââ | 392M/438M [02:09<00:11, 4.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  90%|âââââââââ | 393M/438M [02:09<00:10, 4.34MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  90%|âââââââââ | 394M/438M [02:10<00:09, 4.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  90%|âââââââââ | 395M/438M [02:10<00:09, 4.49MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  90%|âââââââââ | 396M/438M [02:10<00:09, 4.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  91%|âââââââââ | 397M/438M [02:10<00:09, 4.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  91%|âââââââââ | 398M/438M [02:11<00:09, 4.27MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  91%|âââââââââ | 399M/438M [02:11<00:09, 4.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  91%|âââââââââ | 399M/438M [02:11<00:09, 4.17MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  91%|ââââââââââ| 400M/438M [02:11<00:13, 2.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 401M/438M [02:12<00:09, 3.78MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 402M/438M [02:12<00:09, 3.77MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 403M/438M [02:12<00:09, 3.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 403M/438M [02:12<00:09, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 404M/438M [02:14<00:34, 978kB/s] \n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  92%|ââââââââââ| 405M/438M [02:14<00:26, 1.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 406M/438M [02:14<00:20, 1.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 406M/438M [02:14<00:18, 1.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 407M/438M [02:15<00:14, 2.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 407M/438M [02:15<00:11, 2.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 408M/438M [02:15<00:10, 2.84MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 409M/438M [02:15<00:09, 3.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  93%|ââââââââââ| 409M/438M [02:15<00:08, 3.30MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  94%|ââââââââââ| 412M/438M [02:16<00:06, 4.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  94%|ââââââââââ| 413M/438M [02:16<00:05, 4.61MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 414M/438M [02:16<00:05, 4.56MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 415M/438M [02:17<00:05, 4.40MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 415M/438M [02:17<00:05, 4.44MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 416M/438M [02:17<00:08, 2.55MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 417M/438M [02:17<00:05, 3.69MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  95%|ââââââââââ| 418M/438M [02:17<00:05, 3.80MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 419M/438M [02:18<00:04, 3.87MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 419M/438M [02:18<00:04, 3.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 420M/438M [02:18<00:04, 4.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 421M/438M [02:18<00:04, 4.11MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 421M/438M [02:18<00:04, 4.10MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 422M/438M [02:18<00:03, 4.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  96%|ââââââââââ| 422M/438M [02:19<00:04, 3.41MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 423M/438M [02:19<00:04, 3.39MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 423M/438M [02:19<00:04, 3.43MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 424M/438M [02:19<00:04, 3.23MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 424M/438M [02:19<00:04, 3.09MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 425M/438M [02:19<00:04, 2.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 425M/438M [02:19<00:04, 3.00MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 425M/438M [02:20<00:04, 3.01MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 426M/438M [02:20<00:03, 3.05MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 426M/438M [02:20<00:03, 3.08MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  97%|ââââââââââ| 427M/438M [02:20<00:03, 3.50MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 427M/438M [02:20<00:03, 3.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 428M/438M [02:20<00:03, 3.33MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 428M/438M [02:20<00:03, 3.25MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 430M/438M [02:21<00:01, 4.22MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 430M/438M [02:21<00:01, 4.12MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  98%|ââââââââââ| 431M/438M [02:21<00:01, 3.82MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 431M/438M [02:21<00:01, 3.73MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 432M/438M [02:21<00:01, 3.54MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 432M/438M [02:22<00:02, 2.36MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 433M/438M [02:22<00:01, 2.95MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 433M/438M [02:22<00:01, 3.02MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 434M/438M [02:22<00:01, 3.19MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 434M/438M [02:22<00:01, 3.26MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 435M/438M [02:22<00:00, 3.57MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors:  99%|ââââââââââ| 435M/438M [02:23<00:00, 3.70MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors: 100%|ââââââââââ| 436M/438M [02:23<00:00, 3.79MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors: 100%|ââââââââââ| 437M/438M [02:23<00:00, 3.85MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors: 100%|ââââââââââ| 438M/438M [02:23<00:00, 3.89MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model.safetensors: 100%|ââââââââââ| 438M/438M [02:24<00:00, 3.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "optimizer.pt: 100%|ââââââââââ| 871M/871M [03:26<00:00, 4.22MB/s]\n",
      "\n",
      "Upload 5 LFS files: 100%|ââââââââââ| 5/5 [03:26<00:00, 41.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete!\n"
     ]
    }
   ],
   "source": [
    "local_folder_path = 'output/bi-encoder-S-PubMedBert-MS-MARCO/checkpoint-1430'\n",
    "upload_model_to_huggingface(local_folder_path, repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6822bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from Hugging Face Hub: LukasXperiaZ/S-PubMedBert-MS-MARCO-tweet-paper-reranker-abstract\n"
     ]
    }
   ],
   "source": [
    "model_from_hub = SentenceTransformer(repo_id)\n",
    "print(f\"Model loaded successfully from Hugging Face Hub: {repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddddb96",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "906088de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting re-ranking for 1400 tweets ...\n",
      "Processed 100/1400 tweets.\n",
      "Processed 200/1400 tweets.\n",
      "Processed 300/1400 tweets.\n",
      "Processed 400/1400 tweets.\n",
      "Processed 500/1400 tweets.\n",
      "Processed 600/1400 tweets.\n",
      "Processed 700/1400 tweets.\n",
      "Processed 800/1400 tweets.\n",
      "Processed 900/1400 tweets.\n",
      "Processed 1000/1400 tweets.\n",
      "Processed 1100/1400 tweets.\n",
      "Processed 1200/1400 tweets.\n",
      "Processed 1300/1400 tweets.\n",
      "Processed 1400/1400 tweets.\n",
      "\n",
      "Re-ranking complete for all tweets.\n",
      "0    [hg3xpej0, 8t2tic9n, krtqq8uo, 00ia8k0b, 5hxsa...\n",
      "1    [r58aohnu, kiq6xb6k, s2vckt2w, sljh3o2k, tgd6g...\n",
      "2    [cgc0v1dg, o47v5vgw, q92unteg, osct0lj6, lavcs...\n",
      "3    [k0f4cwig, 8j3bb6zx, 3sr2exq9, sv48gjkk, kca5r...\n",
      "4    [ybwwmyqy, ouvq2wpq, vabb2f26, rs3umc1x, sxx3y...\n",
      "Name: reranked_uids, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rerank_tweets(model_from_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679ec820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking Results on the dev set: {1: np.float64(0.5407142857142857), 5: np.float64(0.6139761904761905), 10: np.float64(0.6230518707482994)}\n"
     ]
    }
   ],
   "source": [
    "df_dev_eval = pd.DataFrame({\n",
    "    \"cord_uid\": tweet_info_dev[\"cord_uid\"],\n",
    "    \"topk\": tweet_info_dev[\"reranked_uids\"]\n",
    "})\n",
    "\n",
    "# Evaluate MRR@k\n",
    "results_def_rerank_S_PubMedBert = get_performance_mrr(df_dev_eval, 'cord_uid', 'topk')\n",
    "print(f\"Reranking Results on the dev set: {results_def_rerank_S_PubMedBert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700a411",
   "metadata": {},
   "source": [
    "Using additionally Title, Authors and Journal\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      title  \\\n",
      "cord_uid                                                      \n",
      "umvrwgaw  Professional and Home-Made Face Masks Reduce E...   \n",
      "\n",
      "                                                   abstract  \\\n",
      "cord_uid                                                      \n",
      "umvrwgaw  BACKGROUND: Governments are preparing for a po...   \n",
      "\n",
      "                                                    authors   journal  \n",
      "cord_uid                                                               \n",
      "umvrwgaw  van der Sande, Marianne; Teunis, Peter; Sabel,...  PLoS One  \n",
      "title:  Higher viral load drives infrequent SARS-CoV-2 transmission between asymptomatic residence hall roommates\n",
      "abstract:  In 2019-2020, the COVID-19 pandemic spread to over 200 countries in less than six months. To understand the basis of this aggressive spread, it is essential to determine the transmission rate and define the factors that increase the risk of transmission. One complication is the large fraction of asymptomatic cases, particularly in young populations: these individuals have viral loads indistinguishable from symptomatic people and do transmit the SARS-CoV-2 virus, but they often go undetected. As university students living in residence halls commonly share a small living space with roommates, some schools established regular, high density testing programs to mitigate on-campus spread. In this study, we analyzed longitudinal testing data of residence hall students at the University of Colorado Boulder. We observed that students in single rooms were infected at a lower rate than students in multiple occupancy rooms. However, this was not due to high rates of transmission between roommates, which only occurred approximately 20% of the time. Since these cases were usually asymptomatic at the time of diagnosis, this provides further evidence for asymptomatic transmission. Notably, individuals who likely transmitted to their roommates had an average viral load [~]6.5 times higher than individuals who did not. Although students were moved to separate isolation rooms after diagnosis, there was no difference in time to isolation between these cases with or without transmission. This analysis argues that inter-roommate transmission occurs in a minority of cases in university residence halls and provides strong correlative evidence that viral load can be proportional to the probability of transmission.\n",
      "authors:  Bjorkman, Kristen K; Saldi, Tassa K; Lasda, Erika; Bauer, Leisha Conners; Kovarik, Jennifer; Gonzales, Patrick K; Fink, Morgan R; Tat, Kimngang L; Hager, Cole R; Davis, Jack C; Ozeroff, Christopher D; Brisson, Gloria R; Larremore, Daniel B; Leinwand, Leslie A; McQueen, Matthew B; Parker, Roy\n",
      "journal:  nan\n"
     ]
    }
   ],
   "source": [
    "paper_info = df_collection.set_index('cord_uid')[['title', 'abstract', 'authors', 'journal']]\n",
    "print(paper_info[:1])\n",
    "\n",
    "cord_uid = 'xd5uhah4'\n",
    "\n",
    "assert cord_uid in paper_info['title']\n",
    "\n",
    "title = paper_info['title'][cord_uid]\n",
    "print('title: ', title)\n",
    "\n",
    "abstract = paper_info['abstract'][cord_uid]\n",
    "print('abstract: ', abstract)\n",
    "\n",
    "authors = paper_info['authors'][cord_uid]\n",
    "print('authors: ', authors)\n",
    "\n",
    "journal = paper_info['journal'][cord_uid]\n",
    "print('journal: ', journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "955580d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12853 training examples as dictionaries.\n",
      "Converted to Hugging Face Dataset with 12853 rows and columns: ['query', 'positive', 'label']\n",
      "{'query': 'Oral care in rehabilitation medicine: oral vulnerability, oral muscle wasting, and hospital-associated oral issues', 'positive': '[TITLE]: Oral Management in Rehabilitation Medicine: Oral Frailty, Oral Sarcopenia, and Hospital-Associated Oral Problems [AUTHORS]: Shiraishi, A.; Wakabayashi, Hidetaka; Yoshimura, Y. [JOURNAL]: J Nutr Health Aging [ABSTRACT]: Oral health is a crucial but often neglected aspect of rehabilitation medicine. Approximately 71% of hospitalized rehabilitation patients and 91% of hospitalized acute care patients have impaired oral health. Poor oral condition in hospitalized patients can be attributed to factors such as age, physical dependency, cognitive decline, malnutrition, low skeletal muscle mass and strength, and multimorbidity. Another major factor is a lack of knowledge and interest in oral problems among health care workers. Recently, new concepts have been proposed, such as oral frailty, oral sarcopenia, and hospital-associated oral problems. Oral frailty, the accumulation of a slightly poor status of oral conditions and function, strongly predicts physical frailty, dysphagia, malnutrition, need for long-term care, and mortality in community-dwelling older adults. Oral sarcopenia refers to sarcopenia associated with oral conditions and function, although its definition has not yet been fully discussed. Hospital-associated oral problems are caused by disease, disease treatment, surgery, endotracheal intubation, poor self-care abilities, lack of care by medical staff, drugs, and iatrogenic factors during hospitalization. Furthermore, oral problems have negative impacts on rehabilitation outcomes, which include functional recovery, length of hospital stay, discharge home, and in-hospital mortality. Oral health management provided by dental hygienists improves not only oral status and function, swallowing function, and nutritional status but also activities of daily living, discharge home, and in-hospital mortality in post-acute rehabilitation. Oral rehabilitation, promotion, education, and medical-dental collaboration can be effective interventions for oral problems and therefore are necessary to improve rehabilitation outcomes.', 'label': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_list_of_dicts = []\n",
    "\n",
    "# Iterate through the rows of the tweet_info_train DataFrame\n",
    "for index, row in tweet_info_train.iterrows():\n",
    "    tweet_text = row[\"tweet_text\"]\n",
    "    correct_cord_uid = row[\"cord_uid\"]\n",
    "\n",
    "    # Get the title, abstract, authors and journal of the correct paper\n",
    "    if correct_cord_uid in paper_info['title']:\n",
    "        correct_title = paper_info['title'][correct_cord_uid]\n",
    "        correct_abstract = paper_info['abstract'][correct_cord_uid]\n",
    "        correct_authors = paper_info['authors'][correct_cord_uid]\n",
    "        correct_journal = paper_info['journal'][correct_cord_uid]\n",
    "\n",
    "        # Ensure that the abstract  is a string and not empty\n",
    "        if isinstance(correct_abstract, str) and correct_abstract.strip():\n",
    "\n",
    "            if not (isinstance(correct_title, str) and correct_title.strip()):\n",
    "                # Title is missing, leave it blank\n",
    "                correct_title = ''\n",
    "\n",
    "            if not (isinstance(correct_authors, str) and correct_authors.strip()):\n",
    "                correct_authors = ''\n",
    "\n",
    "            if not (isinstance(correct_journal, str) and correct_journal.strip()):\n",
    "                correct_journal = ''\n",
    "\n",
    "            correct_data = '[TITLE]: ' + correct_title + ' [AUTHORS]: ' + correct_authors + ' [JOURNAL]: ' + correct_journal + ' [ABSTRACT]: ' + correct_abstract\n",
    "\n",
    "            # Create a dictionary for this training example (a positive pair)\n",
    "            example_dict = {\n",
    "                'query': tweet_text,\n",
    "                'positive': correct_data,\n",
    "                'label': 1.0 # Label indicating it's a positive pair\n",
    "            }\n",
    "            train_data_list_of_dicts.append(example_dict)\n",
    "\n",
    "        else:\n",
    "            # Handle cases where the some of the data is missing, not a string, or empty after stripping whitespace\n",
    "            print(f\"Warning: Data at {correct_cord_uid} is not a string or empty, skipping tweet at index {index}.\")\n",
    "\n",
    "    else:\n",
    "        # Handle cases where the correct paper's abstract is not found in your paper_info data\n",
    "        print(f\"Warning: Correct data not found for {correct_cord_uid} for tweet at index {index}, skipping tweet.\")\n",
    "\n",
    "print(f\"Created {len(train_data_list_of_dicts)} training examples as dictionaries.\")\n",
    "\n",
    "# Convert the list of dictionaries into a datasets.Dataset\n",
    "train_dataset = Dataset.from_list(train_data_list_of_dicts)\n",
    "\n",
    "print(f\"Converted to Hugging Face Dataset with {len(train_dataset)} rows and columns: {train_dataset.column_names}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3193b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pritamdeka/S-PubMedBert-MS-MARCO\n"
     ]
    }
   ],
   "source": [
    "model = load_sentence_transformer_model('pritamdeka/S-PubMedBert-MS-MARCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "224471c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1430' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1430/1430 23:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Reranking Map</th>\n",
       "      <th>Validation Reranking Mrr@10</th>\n",
       "      <th>Validation Reranking Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679951</td>\n",
       "      <td>0.673822</td>\n",
       "      <td>0.723977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706449</td>\n",
       "      <td>0.701215</td>\n",
       "      <td>0.750021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.718107</td>\n",
       "      <td>0.713613</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.724878</td>\n",
       "      <td>0.770975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723374</td>\n",
       "      <td>0.718540</td>\n",
       "      <td>0.765454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726988</td>\n",
       "      <td>0.722315</td>\n",
       "      <td>0.769576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726423</td>\n",
       "      <td>0.721610</td>\n",
       "      <td>0.768463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727392</td>\n",
       "      <td>0.722546</td>\n",
       "      <td>0.769244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727287</td>\n",
       "      <td>0.722586</td>\n",
       "      <td>0.769644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726611</td>\n",
       "      <td>0.721696</td>\n",
       "      <td>0.768215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1430, training_loss=0.3121883808941274, metrics={'train_runtime': 1386.0572, 'train_samples_per_second': 92.731, 'train_steps_per_second': 1.032, 'total_flos': 0.0, 'train_loss': 0.3121883808941274, 'epoch': 10.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 90\n",
    "num_epochs = 10\n",
    "\n",
    "# Define train loss\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# (Optional) Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir='output/bi-encoder-S-PubMedBert-MS-MARCO-title-abstract-author-journal',\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    run_name=\"bi-encoder-S-PubMedBert-MS-MARCO-tweet-paper-reranking-title-abstract-author-journal\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# (Optional) Create an evaluator & evaluate the base model\n",
    "# !!! WARNING !!! This evaluator EXCLUDES cases where the true paper IS NOT in the top k !!\n",
    "dev_evaluator = RerankingEvaluator(eval_examples, batch_size=BATCH_SIZE, name='validation_reranking')\n",
    "dev_evaluator(model)\n",
    "\n",
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbb737",
   "metadata": {},
   "source": [
    "Using another loss function and hard negatives\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d409489",
   "metadata": {},
   "source": [
    "You can start the script from here - don't have to execute anything before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6ed1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/AIR/CLEF2025-4b/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 186 correct cord_uid's are not in top-k for validation tweets, cannot evaluate re-ranking for them.\n",
      "Created 1214 evaluation examples for RerankingEvaluator.\n"
     ]
    }
   ],
   "source": [
    "# ----- Do Everything needed -----\n",
    "import pandas as pd\n",
    "def load_series_from_json(filename):\n",
    "    loaded_series = pd.read_json(filename)\n",
    "    return loaded_series\n",
    "\n",
    "filename = 'ranked_train'\n",
    "ranked_train = load_series_from_json(filename)\n",
    "\n",
    "filename = 'ranked_dev'\n",
    "ranked_dev = load_series_from_json(filename)\n",
    "\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "\n",
    "paper_info = df_collection.set_index('cord_uid')['abstract']\n",
    "\n",
    "tweet_info_train = ranked_train[[\"tweet_text\", \"cord_uid\", \"tfidf_topk\"]]\n",
    "\n",
    "tweet_info_dev = ranked_dev[[\"post_id\", \"tweet_text\", \"cord_uid\", \"tfidf_topk\"]]\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_sentence_transformer_model(name):\n",
    "    try:\n",
    "        model = SentenceTransformer(name)\n",
    "        print(f\"Successfully loaded {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the model. Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback if it fails\n",
    "    return model\n",
    "\n",
    "eval_examples = []\n",
    "n_correct_cord_uid_not_in_top_k = 0\n",
    "\n",
    "# Iterate through the rows of the validation tweet_info DataFrame\n",
    "for index, row in tweet_info_dev.iterrows():\n",
    "    query_text = row['tweet_text']\n",
    "    correct_cord_uid = row['cord_uid'] # This is the ID of the correct paper\n",
    "    top_k_candidate_uids = row['tfidf_topk'] # This is the list of UIDs from the first stage ranker\n",
    "\n",
    "    # We need the abstract for the correct paper and for all candidate papers\n",
    "    positive_abstract = None\n",
    "    negative_abstracts_map = {} # Map UID to abstract text for candidates\n",
    "\n",
    "    # Get the positive abstract\n",
    "    if correct_cord_uid in paper_info and isinstance(paper_info[correct_cord_uid], str) and paper_info[correct_cord_uid].strip():\n",
    "        positive_abstract = paper_info[correct_cord_uid]\n",
    "\n",
    "    # Get abstracts for all negative candidates (all in the top-k list except the positive abstract)\n",
    "    for uid in top_k_candidate_uids:\n",
    "        if uid in paper_info and uid != correct_cord_uid:\n",
    "            negative_abstracts_map[uid] = paper_info[uid]\n",
    "\n",
    "    if positive_abstract and negative_abstracts_map: # Ensure we have the positive and at least one negative abstract\n",
    "        negative_uids_list = list(negative_abstracts_map.keys())\n",
    "        negative_abstracts_list = list(negative_abstracts_map.values())\n",
    "\n",
    "        if correct_cord_uid in top_k_candidate_uids:\n",
    "            eval_examples.append({\n",
    "                \"query\": query_text,\n",
    "                \"positive\": [positive_abstract],\n",
    "                \"negative\": negative_abstracts_list\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            # This case means the correct paper was not found in the top-k list from the first stage ranker.\n",
    "            n_correct_cord_uid_not_in_top_k += 1\n",
    "\n",
    "if n_correct_cord_uid_not_in_top_k > 0:\n",
    "    print(f\"Warning: {n_correct_cord_uid_not_in_top_k} correct cord_uid's are not in top-k for validation tweets, cannot evaluate re-ranking for them.\")\n",
    "\n",
    "print(f\"Created {len(eval_examples)} evaluation examples for RerankingEvaluator.\")\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import RerankingEvaluator\n",
    "\n",
    "paper_info = df_collection.set_index('cord_uid')[['title', 'abstract', 'authors', 'journal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1db0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating the document string\n",
    "def get_document_string(paper_info, cord_uid):\n",
    "    \n",
    "    title = paper_info['title'][cord_uid]\n",
    "    abstract = paper_info['abstract'][cord_uid]\n",
    "    authors = paper_info['authors'][cord_uid]\n",
    "    journal = paper_info['journal'][cord_uid]\n",
    "\n",
    "    if not (isinstance(abstract, str) and abstract.strip()):\n",
    "        return 'Abstract missing!'\n",
    "\n",
    "    if not (isinstance(title, str) and title.strip()):\n",
    "        # Title is missing, leave it blank\n",
    "        title = ''\n",
    "\n",
    "    if not (isinstance(authors, str) and authors.strip()):\n",
    "        authors = ''\n",
    "\n",
    "    if not (isinstance(journal, str) and journal.strip()):\n",
    "        journal = ''\n",
    "\n",
    "    document_string = '[TITLE]: ' + title + ' [AUTHORS]: ' + authors + ' [JOURNAL]: ' + journal + ' [ABSTRACT]: ' + abstract\n",
    "    return document_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f94cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 952838 training examples as dictionaries.\n",
      "Converted to Hugging Face Triplet Dataset with 952838 rows and columns: ['anchor', 'positive', 'negative']\n",
      "{'anchor': 'Oral care in rehabilitation medicine: oral vulnerability, oral muscle wasting, and hospital-associated oral issues', 'positive': '[TITLE]: Oral Management in Rehabilitation Medicine: Oral Frailty, Oral Sarcopenia, and Hospital-Associated Oral Problems [AUTHORS]: Shiraishi, A.; Wakabayashi, Hidetaka; Yoshimura, Y. [JOURNAL]: J Nutr Health Aging [ABSTRACT]: Oral health is a crucial but often neglected aspect of rehabilitation medicine. Approximately 71% of hospitalized rehabilitation patients and 91% of hospitalized acute care patients have impaired oral health. Poor oral condition in hospitalized patients can be attributed to factors such as age, physical dependency, cognitive decline, malnutrition, low skeletal muscle mass and strength, and multimorbidity. Another major factor is a lack of knowledge and interest in oral problems among health care workers. Recently, new concepts have been proposed, such as oral frailty, oral sarcopenia, and hospital-associated oral problems. Oral frailty, the accumulation of a slightly poor status of oral conditions and function, strongly predicts physical frailty, dysphagia, malnutrition, need for long-term care, and mortality in community-dwelling older adults. Oral sarcopenia refers to sarcopenia associated with oral conditions and function, although its definition has not yet been fully discussed. Hospital-associated oral problems are caused by disease, disease treatment, surgery, endotracheal intubation, poor self-care abilities, lack of care by medical staff, drugs, and iatrogenic factors during hospitalization. Furthermore, oral problems have negative impacts on rehabilitation outcomes, which include functional recovery, length of hospital stay, discharge home, and in-hospital mortality. Oral health management provided by dental hygienists improves not only oral status and function, swallowing function, and nutritional status but also activities of daily living, discharge home, and in-hospital mortality in post-acute rehabilitation. Oral rehabilitation, promotion, education, and medical-dental collaboration can be effective interventions for oral problems and therefore are necessary to improve rehabilitation outcomes.', 'negative': '[TITLE]: High expression of ACE2 receptor of 2019-nCoV on the epithelial cells of oral mucosa [AUTHORS]: Xu, Hao; Zhong, Liang; Deng, Jiaxin; Peng, Jiakuan; Dan, Hongxia; Zeng, Xin; Li, Taiwen; Chen, Qianming [JOURNAL]: Int J Oral Sci [ABSTRACT]: It has been reported that ACE2 is the main host cell receptor of 2019-nCoV and plays a crucial role in the entry of virus into the cell to cause the final infection. To investigate the potential route of 2019-nCov infection on the mucosa of oral cavity, bulk RNA-seq profiles from two public databases including The Cancer Genome Atlas (TCGA) and Functional Annotation of The Mammalian Genome Cap Analysis of Gene Expression (FANTOM5 CAGE) dataset were collected. RNA-seq profiling data of 13 organ types with para-carcinoma normal tissues from TCGA and 14 organ types with normal tissues from FANTOM5 CAGE were analyzed in order to explore and validate the expression of ACE2 on the mucosa of oral cavity. Further, single-cell transcriptomes from an independent data generated in-house were used to identify and confirm the ACE2-expressing cell composition and proportion in oral cavity. The results demonstrated that the ACE2 expressed on the mucosa of oral cavity. Interestingly, this receptor was highly enriched in epithelial cells of tongue. Preliminarily, those findings have explained the basic mechanism that the oral cavity is a potentially high risk for 2019-nCoV infectious susceptibility and provided a piece of evidence for the future prevention strategy in dental clinical practice as well as daily life.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data_list_of_dicts = []\n",
    "\n",
    "\n",
    "perc_top_k = 0.75  # How many of the top k-1 should be taken, i.e. 1 means the documents of all other 99 cord_uids will be taken. \n",
    "\n",
    "# For hyperparameter tuning, we reduced the number of tweets to have faster learning:\n",
    "#perc_n_tweets = 0.1\n",
    "#tweet_info_train = tweet_info_train.sample(frac=perc_n_tweets, random_state=42)\n",
    "\n",
    "# Iterate through the rows of the tweet_info_train DataFrame\n",
    "for index, row in tweet_info_train.iterrows():\n",
    "    tweet_text = row[\"tweet_text\"]\n",
    "    correct_cord_uid = row[\"cord_uid\"]\n",
    "    negative_cord_uids = [elem for elem in row['tfidf_topk'] if elem != correct_cord_uid]\n",
    "\n",
    "    # Get the title, abstract, authors and journal of the correct paper\n",
    "    if correct_cord_uid in paper_info['title']:\n",
    "        \n",
    "        document_string_pos = get_document_string(paper_info, correct_cord_uid)\n",
    "        \n",
    "        if document_string_pos == 'Abstract missing!':\n",
    "            print(f\"Warning: Abstract missing for {correct_cord_uid} in paper_info, skipping tweet.\")\n",
    "            continue\n",
    "\n",
    "        last_indx = int(perc_top_k * (len(negative_cord_uids) - 1))\n",
    "        for i in range(0, last_indx + 1):\n",
    "            negative_cord_uid = negative_cord_uids[i]\n",
    "            \n",
    "            document_string_neg = get_document_string(paper_info, negative_cord_uid)\n",
    "\n",
    "            # Create a dictionary for this training example\n",
    "            example_dict = {\n",
    "                'anchor': tweet_text,\n",
    "                'positive': document_string_pos,\n",
    "                'negative': document_string_neg\n",
    "            }\n",
    "            train_data_list_of_dicts.append(example_dict)\n",
    "\n",
    "    else:\n",
    "        # Handle cases where the correct paper's abstract is not found in your paper_info data\n",
    "        print(f\"Warning: Correct data not found for {correct_cord_uid} for tweet at index {index}, skipping tweet.\")\n",
    "\n",
    "print(f\"Created {len(train_data_list_of_dicts)} training examples as dictionaries.\")\n",
    "\n",
    "# Convert the list of dictionaries into a datasets.Dataset\n",
    "train_dataset = Dataset.from_list(train_data_list_of_dicts)\n",
    "\n",
    "print(f\"Converted to Hugging Face Triplet Dataset with {len(train_dataset)} rows and columns: {train_dataset.column_names}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14d1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pritamdeka/S-PubMedBert-MS-MARCO\n"
     ]
    }
   ],
   "source": [
    "model = load_sentence_transformer_model('pritamdeka/S-PubMedBert-MS-MARCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5314c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import TripletLoss\n",
    "\n",
    "margin = 0.5    # 0.5 workes quite well\n",
    "loss = TripletLoss(model=model, triplet_margin=margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 52\n",
    "num_epochs = 5\n",
    "\n",
    "# Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir='output/bi-encoder-S-PubMedBert-MS-MARCO-title-abstract-author-journal-triplet-loss',\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True,  # Set to True if you have a GPU that supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=None,\n",
    "    save_total_limit=5,\n",
    "    logging_steps=10,\n",
    "    run_name=\"bi-encoder-S-PubMedBert-MS-MARCO-tweet-paper-reranking-title-abstract-author-journal-triplet-loss\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# Create an evaluator & evaluate the base model\n",
    "# !!! WARNING !!! This evaluator EXCLUDES cases where the true paper IS NOT in the top k !!\n",
    "dev_evaluator = RerankingEvaluator(eval_examples, batch_size=BATCH_SIZE, name='validation_reranking')\n",
    "dev_evaluator(model)\n",
    "\n",
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    loss=loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
